{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mvsnet_Run3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMqFFu7wzy6iq2QjRuShMUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "648e92fdd1e1459f8f50e4b2101f36c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99bcfab1b0fb43459ec0d529ca40d6bc",
              "IPY_MODEL_838008484b1645b8b1f945706f28027b",
              "IPY_MODEL_3d5706456c15480aac6da4815b4d51bd"
            ],
            "layout": "IPY_MODEL_ee7d17d91ca54953afa78209ea770bc2"
          }
        },
        "99bcfab1b0fb43459ec0d529ca40d6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f25d9f0dbb3493e877e3600b31ae1bd",
            "placeholder": "​",
            "style": "IPY_MODEL_fdcc42ca4d18481bbf1a020fcbeb67d8",
            "value": "100%"
          }
        },
        "838008484b1645b8b1f945706f28027b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa3592aeb1284c1b902b24f620823ccb",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_635706280b5b421a92225bb6b4cb41b0",
            "value": 10
          }
        },
        "3d5706456c15480aac6da4815b4d51bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077e10a54e5340ef887db17e4cdd8241",
            "placeholder": "​",
            "style": "IPY_MODEL_b9c859c6960745d08137daca14c0013b",
            "value": " 10/10 [14:40:12&lt;00:00, 5279.14s/it]"
          }
        },
        "ee7d17d91ca54953afa78209ea770bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f25d9f0dbb3493e877e3600b31ae1bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdcc42ca4d18481bbf1a020fcbeb67d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa3592aeb1284c1b902b24f620823ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "635706280b5b421a92225bb6b4cb41b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "077e10a54e5340ef887db17e4cdd8241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c859c6960745d08137daca14c0013b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0b0shubham96/Google_colab/blob/main/Mvsnet_Run3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript(''' \n",
        "function Clickconnect() {\n",
        "  btn = document.querySelector(\"colab-connect-button\")\n",
        "  if (btn != null){\n",
        "    console.log(\"Click colab-connect-button\");\n",
        "    btn.click()\n",
        "  }\n",
        "\n",
        "  btn = document.getElementbyID('ok')\n",
        "  if (btn != null){\n",
        "    console.log(\"Click reconnect\");\n",
        "    btn.click()\n",
        "  }\n",
        "}\n",
        "setInterval(Clickconnect,50000)\n",
        "'''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LkXJ6D3CXGYX",
        "outputId": "9d547b55-93a0-47dc-e9fa-923d334cdf7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              " \n",
              "function Clickconnect() {\n",
              "  btn = document.querySelector(\"colab-connect-button\")\n",
              "  if (btn != null){\n",
              "    console.log(\"Click colab-connect-button\");\n",
              "    btn.click()\n",
              "  }\n",
              "\n",
              "  btn = document.getElementbyID('ok')\n",
              "  if (btn != null){\n",
              "    console.log(\"Click reconnect\");\n",
              "    btn.click()\n",
              "  }\n",
              "}\n",
              "setInterval(Clickconnect,50000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9BcI3Ko0R0i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875e93ed-4072-4ca3-8feb-09574ad45d8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/Dataset/dtu_training.rar\" \"/media/\""
      ],
      "metadata": {
        "id": "cNr3i7Z2R2WD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba26599-860a-41f9-c71c-43c5e2190295"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/Dataset/dtu_training.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /media/mvs_training/dtu/Cameras/00000000_cam.txt\n",
            "   200 bytes, modified on 2018-01-09 20:56\n",
            "with a new one\n",
            "   200 bytes, modified on 2018-01-09 20:56\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit Q\n",
            "\n",
            "Program aborted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/home/mvsnet/')"
      ],
      "metadata": {
        "id": "klOvC602SySA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PhI6_lLMMXoD"
      },
      "outputs": [],
      "source": [
        "import preprocess\n",
        "import visualize\n",
        "import torch\n",
        "import model\n",
        "import loss as l\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import constants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper parameters \n",
        "BATCH_SIZE = 5\n",
        "WEIGHT_DECAY = 1e-3\n",
        "LEARNING_RATE = 1e-4\n",
        "TRANSFORMS = preprocess.ToTensor()      #dataset_transforms = transforms.Compose([preprocess.SampleWiseStandardization(), preprocess.ToTensor()])\n",
        "\n",
        "#Dataloader\n",
        "NUM_WORKERS = 3\n",
        "\n",
        "#Training\n",
        "MAX_EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "hgjnvkBPMd-x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list_train = preprocess.gen_dtu_path(\"/media/mvs_training/dtu/\")  #Change path to your own directory of the DTU dataset\n",
        "sample_list_val = preprocess.gen_dtu_path(\"/media/mvs_training/dtu/\", mode=\"validation\")\n",
        "\n",
        "#dataset_transforms = transforms.Compose([preprocess.SampleWiseStandardization(), preprocess.ToTensor()])\n",
        "data_train = preprocess.DTUDatset(sample_list_train, transform=TRANSFORMS)\n",
        "data_val = preprocess.DTUDatset(sample_list_val, transform=TRANSFORMS)"
      ],
      "metadata": {
        "id": "vTJTJd6IMh_e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "loader_val = torch.utils.data.DataLoader(data_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "_c_nnGLKMkS8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MVSModel = model.NeuralNet()\n",
        "MVSModel = MVSModel.float()\n",
        "\n",
        "\n",
        "def one_epoch(model, data_loader, optimizer=None, best_val_acc = 0):\n",
        "    \"\"\"\n",
        "    Outputs the mean loss and acc for every 1000 iterations / ref images.\n",
        "    The returned output is a vector of length 27,\n",
        "    that is 27 mean loss and acc values. \n",
        "    The last index of the list contains a mean over 1097 iterations due to the \n",
        "    total lenght of the data in the dataloader is 27097 \n",
        "    \"\"\"\n",
        "    device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    train = False if optimizer is None else True\n",
        "    model.train() if train else model.eval()    #When model is set to train it knows to enclude dropout, batchnorm, etc in the training\n",
        "    mean_losses, mean_acc, accuracies, losses, acc, correct, total = [], [], 0, [], 0, 0, 0\n",
        "    log_iter = 100 * BATCH_SIZE #Number of iterations before we log loss and acc\n",
        "    max_iter = 27097 \n",
        "    iteration = 0\n",
        "    depth_maps = []\n",
        "    for ref, s1, s2, gt_depth, r_cam, s1_cam, s2_cam in data_loader:\n",
        "        start_time = time.time()\n",
        "        iteration += len(ref)\n",
        "        ref, s1, s2, gt_depth = ref.to(device), s1.to(device), s2.to(device), gt_depth.to(device)\n",
        "        with torch.set_grad_enabled(train): #Enable gradient calculation if train = True\n",
        "            logits = model(ref.float(), s1.float(), s2.float(), r_cam, s1_cam, s2_cam)\n",
        "        #ATM the loss can not be computed since the model is not finished, therefore the loop stops when trying to implement the backward pass\n",
        "        loss, acc = l.mean_abs_diff_loss(logits, gt_depth)\n",
        "        if train: \n",
        "            optimizer.zero_grad()   #Removing accumulated gradients \n",
        "            loss.backward()         #Calculating new gradients\n",
        "            optimizer.step()        #Updating weights\n",
        "        losses.append(loss.item())\n",
        "        accuracies += acc\n",
        "        total += len(ref)   \n",
        "        if iteration % log_iter == 0 and iteration < 27000:\n",
        "            mean_losses.append(np.mean(losses))\n",
        "            mean_acc.append(accuracies/log_iter)\n",
        "            if not train:\n",
        "                #Saving est_depth_map, gt_depth_map and ref image in the validation loop\n",
        "                est_depth_map = logits\n",
        "                est_depth_map = est_depth_map.detach().cpu().numpy()\n",
        "                gt = gt_depth\n",
        "                gt = gt.detach().cpu().numpy()\n",
        "                ref_img = ref\n",
        "                ref_img = ref_img.detach().cpu().numpy()\n",
        "                est_and_gt = [est_depth_map, gt, ref_img]\n",
        "                depth_maps.append(est_and_gt)\n",
        "                #Checking if current acc is better than current best acc\n",
        "                if acc > best_val_acc:\n",
        "                    best_val_acc = acc\n",
        "                    print(\"New best validation acc: \", best_val_acc)\n",
        "                    torch.save(model.state_dict(), \"/home/mvsnet/best_model_param.pt\") #saving best model parameters\n",
        "            losses = []\n",
        "            accuracies = 0\n",
        "        elif iteration == max_iter:   #End of the epoch\n",
        "            print(\"THIS IS THE END OF THE EPOCH\")\n",
        "            mean_losses.append(np.mean(losses))\n",
        "            mean_acc.append(accuracies/log_iter)\n",
        "            if not train:\n",
        "                if acc > best_val_acc:\n",
        "                    best_val_acc = acc\n",
        "                    print(\"New best validation acc: \", best_val_acc)\n",
        "                    torch.save(model.state_dict(), \"/home/mvsnet/best_model_param.pt\") #saving best model parameters\n",
        "        print(\"Iteration: \", iteration, \"     Current loss: \", loss.item(), \"     Current accuracy: \", np.mean(accuracies),  \"     Comp_time of one batch: \", start_time - time.time())\n",
        " \n",
        "    if train:\n",
        "        return mean_losses, mean_acc\n",
        "    else:\n",
        "        return mean_losses, mean_acc, best_val_acc, depth_maps\n",
        "\n",
        "\n",
        "\n",
        "def train(model, loader_train, loader_val, lr=1e-3, max_epochs = 6, weight_decay=1e-2, patience=10):\n",
        "    train_losses, train_accuracies = [], []\n",
        "    val_losses, val_accuracies = [], []\n",
        "    depth_maps_val = []\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_val_epochs = []\n",
        "    best_val_acc = 0\n",
        "    t = tqdm(range(max_epochs)) #Using tqdm to show progress of epochs\n",
        "    for epoch in t:\n",
        "        train_loss, train_acc = one_epoch(model, loader_train, opt) #One epoch returns 27 mean values. \n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc, best_val_acc, depth_map_val = one_epoch(model, loader_val, best_val_acc=best_val_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        best_val_epochs.append(best_val_acc)\n",
        "        depth_maps_val.append(depth_map_val)\n",
        "        \n",
        "\n",
        "    train_losses_flat = [item for sublist in train_losses for item in sublist]\n",
        "    val_losses_flat = [item for sublist in val_losses for item in sublist]\n",
        "    train_accuracies = [item for sublist in val_losses for item in sublist]\n",
        "    val_accuracies = [item for sublist in val_losses for item in sublist]\n",
        "\n",
        "        #t.set_description(f'train_acc: {train_acc:.2f}, valid_acc: {val_acc:.2f}, best_val_acc: {best_val_acc:.2f}')\n",
        "    return train_losses_flat, train_accuracies, val_losses_flat, val_accuracies, depth_maps_val\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
        "    fig = plt.figure(figsize=(7, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    plt.plot(val_losses, label=\"valid\")\n",
        "    #plt.ylim()    \n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.plot(train_accuracies, label=\"train\")\n",
        "    plt.plot(val_accuracies, label=\"valid\")\n",
        "    #plt.ylim()\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"/home/train_results.png\")\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "H8O5fL8aMmdb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train, acc_train, loss_val, acc_val, depth_maps = train(MVSModel, loader_train, loader_val, lr=LEARNING_RATE, max_epochs=MAX_EPOCHS, weight_decay=WEIGHT_DECAY)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_P4j8arMsMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "648e92fdd1e1459f8f50e4b2101f36c5",
            "99bcfab1b0fb43459ec0d529ca40d6bc",
            "838008484b1645b8b1f945706f28027b",
            "3d5706456c15480aac6da4815b4d51bd",
            "ee7d17d91ca54953afa78209ea770bc2",
            "9f25d9f0dbb3493e877e3600b31ae1bd",
            "fdcc42ca4d18481bbf1a020fcbeb67d8",
            "aa3592aeb1284c1b902b24f620823ccb",
            "635706280b5b421a92225bb6b4cb41b0",
            "077e10a54e5340ef887db17e4cdd8241",
            "b9c859c6960745d08137daca14c0013b"
          ]
        },
        "outputId": "e1f87bc3-0aba-4091-c887-35c8cb94f939"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "648e92fdd1e1459f8f50e4b2101f36c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration:  8285      Current loss:  35.280086517333984      Current accuracy:  18.0      Comp_time of one batch:  -0.8837466239929199\n",
            "Iteration:  8290      Current loss:  16.867923736572266      Current accuracy:  19.0      Comp_time of one batch:  -0.9035298824310303\n",
            "Iteration:  8295      Current loss:  22.85218620300293      Current accuracy:  20.0      Comp_time of one batch:  -0.8791368007659912\n",
            "Iteration:  8300      Current loss:  21.243061065673828      Current accuracy:  21.0      Comp_time of one batch:  -0.8807404041290283\n",
            "Iteration:  8305      Current loss:  29.7011661529541      Current accuracy:  21.0      Comp_time of one batch:  -0.8910517692565918\n",
            "Iteration:  8310      Current loss:  23.572891235351562      Current accuracy:  21.0      Comp_time of one batch:  -0.8833727836608887\n",
            "Iteration:  8315      Current loss:  24.48434066772461      Current accuracy:  22.0      Comp_time of one batch:  -0.9024112224578857\n",
            "Iteration:  8320      Current loss:  28.67753028869629      Current accuracy:  22.0      Comp_time of one batch:  -0.9059514999389648\n",
            "Iteration:  8325      Current loss:  28.23505973815918      Current accuracy:  22.0      Comp_time of one batch:  -0.8772010803222656\n",
            "Iteration:  8330      Current loss:  19.52231788635254      Current accuracy:  23.0      Comp_time of one batch:  -0.8961360454559326\n",
            "Iteration:  8335      Current loss:  27.801176071166992      Current accuracy:  23.0      Comp_time of one batch:  -0.8997664451599121\n",
            "Iteration:  8340      Current loss:  26.571170806884766      Current accuracy:  23.0      Comp_time of one batch:  -0.9072237014770508\n",
            "Iteration:  8345      Current loss:  25.785367965698242      Current accuracy:  23.0      Comp_time of one batch:  -0.8926825523376465\n",
            "Iteration:  8350      Current loss:  18.7835636138916      Current accuracy:  24.0      Comp_time of one batch:  -0.8721733093261719\n",
            "Iteration:  8355      Current loss:  26.319656372070312      Current accuracy:  24.0      Comp_time of one batch:  -0.9216344356536865\n",
            "Iteration:  8360      Current loss:  20.353546142578125      Current accuracy:  25.0      Comp_time of one batch:  -0.9192216396331787\n",
            "Iteration:  8365      Current loss:  20.522279739379883      Current accuracy:  26.0      Comp_time of one batch:  -0.8896677494049072\n",
            "Iteration:  8370      Current loss:  21.722827911376953      Current accuracy:  27.0      Comp_time of one batch:  -0.8739683628082275\n",
            "Iteration:  8375      Current loss:  25.44865608215332      Current accuracy:  27.0      Comp_time of one batch:  -0.8819100856781006\n",
            "Iteration:  8380      Current loss:  32.25410842895508      Current accuracy:  27.0      Comp_time of one batch:  -0.9172420501708984\n",
            "Iteration:  8385      Current loss:  20.390430450439453      Current accuracy:  28.0      Comp_time of one batch:  -0.8858721256256104\n",
            "Iteration:  8390      Current loss:  19.93136215209961      Current accuracy:  29.0      Comp_time of one batch:  -0.8784191608428955\n",
            "Iteration:  8395      Current loss:  25.275754928588867      Current accuracy:  29.0      Comp_time of one batch:  -0.8766322135925293\n",
            "Iteration:  8400      Current loss:  23.3677921295166      Current accuracy:  29.0      Comp_time of one batch:  -0.8942348957061768\n",
            "Iteration:  8405      Current loss:  31.501859664916992      Current accuracy:  29.0      Comp_time of one batch:  -0.8831195831298828\n",
            "Iteration:  8410      Current loss:  19.663742065429688      Current accuracy:  30.0      Comp_time of one batch:  -0.8717644214630127\n",
            "Iteration:  8415      Current loss:  25.64336395263672      Current accuracy:  30.0      Comp_time of one batch:  -0.9125463962554932\n",
            "Iteration:  8420      Current loss:  23.705791473388672      Current accuracy:  30.0      Comp_time of one batch:  -0.8816578388214111\n",
            "Iteration:  8425      Current loss:  20.19121742248535      Current accuracy:  31.0      Comp_time of one batch:  -0.8749284744262695\n",
            "Iteration:  8430      Current loss:  22.95233726501465      Current accuracy:  31.0      Comp_time of one batch:  -0.8836398124694824\n",
            "Iteration:  8435      Current loss:  21.02152442932129      Current accuracy:  32.0      Comp_time of one batch:  -0.8967149257659912\n",
            "Iteration:  8440      Current loss:  32.43085479736328      Current accuracy:  32.0      Comp_time of one batch:  -0.8953509330749512\n",
            "Iteration:  8445      Current loss:  19.56035804748535      Current accuracy:  33.0      Comp_time of one batch:  -0.8911197185516357\n",
            "Iteration:  8450      Current loss:  23.742807388305664      Current accuracy:  33.0      Comp_time of one batch:  -0.8824429512023926\n",
            "Iteration:  8455      Current loss:  24.572002410888672      Current accuracy:  34.0      Comp_time of one batch:  -0.9033126831054688\n",
            "Iteration:  8460      Current loss:  21.134048461914062      Current accuracy:  35.0      Comp_time of one batch:  -0.8916659355163574\n",
            "Iteration:  8465      Current loss:  20.775997161865234      Current accuracy:  36.0      Comp_time of one batch:  -0.8802330493927002\n",
            "Iteration:  8470      Current loss:  19.66669273376465      Current accuracy:  37.0      Comp_time of one batch:  -0.8841776847839355\n",
            "Iteration:  8475      Current loss:  27.055639266967773      Current accuracy:  37.0      Comp_time of one batch:  -0.8931727409362793\n",
            "Iteration:  8480      Current loss:  23.69933319091797      Current accuracy:  38.0      Comp_time of one batch:  -0.8779687881469727\n",
            "Iteration:  8485      Current loss:  22.959327697753906      Current accuracy:  38.0      Comp_time of one batch:  -0.8757209777832031\n",
            "Iteration:  8490      Current loss:  22.039260864257812      Current accuracy:  39.0      Comp_time of one batch:  -0.8977406024932861\n",
            "Iteration:  8495      Current loss:  26.620391845703125      Current accuracy:  39.0      Comp_time of one batch:  -0.897942066192627\n",
            "Iteration:  8500      Current loss:  29.200366973876953      Current accuracy:  0.0      Comp_time of one batch:  -0.8822836875915527\n",
            "Iteration:  8505      Current loss:  17.894437789916992      Current accuracy:  1.0      Comp_time of one batch:  -0.8896782398223877\n",
            "Iteration:  8510      Current loss:  22.7567195892334      Current accuracy:  2.0      Comp_time of one batch:  -0.8685014247894287\n",
            "Iteration:  8515      Current loss:  23.4736385345459      Current accuracy:  2.0      Comp_time of one batch:  -0.8856034278869629\n",
            "Iteration:  8520      Current loss:  24.30742835998535      Current accuracy:  2.0      Comp_time of one batch:  -0.8865442276000977\n",
            "Iteration:  8525      Current loss:  20.04322052001953      Current accuracy:  3.0      Comp_time of one batch:  -0.8802051544189453\n",
            "Iteration:  8530      Current loss:  27.080570220947266      Current accuracy:  3.0      Comp_time of one batch:  -0.9120335578918457\n",
            "Iteration:  8535      Current loss:  16.42530059814453      Current accuracy:  4.0      Comp_time of one batch:  -0.8730034828186035\n",
            "Iteration:  8540      Current loss:  27.353153228759766      Current accuracy:  4.0      Comp_time of one batch:  -0.8771300315856934\n",
            "Iteration:  8545      Current loss:  21.99535369873047      Current accuracy:  5.0      Comp_time of one batch:  -0.8792505264282227\n",
            "Iteration:  8550      Current loss:  20.56644630432129      Current accuracy:  6.0      Comp_time of one batch:  -0.8718929290771484\n",
            "Iteration:  8555      Current loss:  25.136842727661133      Current accuracy:  6.0      Comp_time of one batch:  -0.8950557708740234\n",
            "Iteration:  8560      Current loss:  22.157379150390625      Current accuracy:  7.0      Comp_time of one batch:  -0.9043784141540527\n",
            "Iteration:  8565      Current loss:  21.372560501098633      Current accuracy:  8.0      Comp_time of one batch:  -0.9191408157348633\n",
            "Iteration:  8570      Current loss:  28.273225784301758      Current accuracy:  8.0      Comp_time of one batch:  -0.9002726078033447\n",
            "Iteration:  8575      Current loss:  23.822513580322266      Current accuracy:  9.0      Comp_time of one batch:  -0.8762023448944092\n",
            "Iteration:  8580      Current loss:  18.271392822265625      Current accuracy:  10.0      Comp_time of one batch:  -0.9359226226806641\n",
            "Iteration:  8585      Current loss:  23.573877334594727      Current accuracy:  10.0      Comp_time of one batch:  -0.902759313583374\n",
            "Iteration:  8590      Current loss:  20.581071853637695      Current accuracy:  11.0      Comp_time of one batch:  -0.904987096786499\n",
            "Iteration:  8595      Current loss:  20.179964065551758      Current accuracy:  12.0      Comp_time of one batch:  -0.8849887847900391\n",
            "Iteration:  8600      Current loss:  25.190244674682617      Current accuracy:  12.0      Comp_time of one batch:  -0.8749399185180664\n",
            "Iteration:  8605      Current loss:  23.877784729003906      Current accuracy:  12.0      Comp_time of one batch:  -0.9084136486053467\n",
            "Iteration:  8610      Current loss:  25.39702606201172      Current accuracy:  12.0      Comp_time of one batch:  -0.936213493347168\n",
            "Iteration:  8615      Current loss:  23.685420989990234      Current accuracy:  12.0      Comp_time of one batch:  -0.9129693508148193\n",
            "Iteration:  8620      Current loss:  29.551668167114258      Current accuracy:  12.0      Comp_time of one batch:  -0.9055414199829102\n",
            "Iteration:  8625      Current loss:  21.192352294921875      Current accuracy:  13.0      Comp_time of one batch:  -0.8762128353118896\n",
            "Iteration:  8630      Current loss:  22.11475944519043      Current accuracy:  14.0      Comp_time of one batch:  -0.8765101432800293\n",
            "Iteration:  8635      Current loss:  19.517087936401367      Current accuracy:  15.0      Comp_time of one batch:  -0.9061017036437988\n",
            "Iteration:  8640      Current loss:  29.408370971679688      Current accuracy:  15.0      Comp_time of one batch:  -0.8861415386199951\n",
            "Iteration:  8645      Current loss:  24.260210037231445      Current accuracy:  15.0      Comp_time of one batch:  -0.8895528316497803\n",
            "Iteration:  8650      Current loss:  30.147541046142578      Current accuracy:  15.0      Comp_time of one batch:  -0.9227280616760254\n",
            "Iteration:  8655      Current loss:  20.23703956604004      Current accuracy:  16.0      Comp_time of one batch:  -0.907761812210083\n",
            "Iteration:  8660      Current loss:  25.063087463378906      Current accuracy:  16.0      Comp_time of one batch:  -0.9083051681518555\n",
            "Iteration:  8665      Current loss:  28.382413864135742      Current accuracy:  16.0      Comp_time of one batch:  -0.8958032131195068\n",
            "Iteration:  8670      Current loss:  20.211217880249023      Current accuracy:  17.0      Comp_time of one batch:  -0.9058356285095215\n",
            "Iteration:  8675      Current loss:  29.57062339782715      Current accuracy:  17.0      Comp_time of one batch:  -0.910574197769165\n",
            "Iteration:  8680      Current loss:  22.31371307373047      Current accuracy:  18.0      Comp_time of one batch:  -0.8876731395721436\n",
            "Iteration:  8685      Current loss:  23.485708236694336      Current accuracy:  18.0      Comp_time of one batch:  -0.8828170299530029\n",
            "Iteration:  8690      Current loss:  28.141342163085938      Current accuracy:  18.0      Comp_time of one batch:  -0.88297438621521\n",
            "Iteration:  8695      Current loss:  21.384275436401367      Current accuracy:  19.0      Comp_time of one batch:  -0.8846626281738281\n",
            "Iteration:  8700      Current loss:  21.830425262451172      Current accuracy:  20.0      Comp_time of one batch:  -0.9075479507446289\n",
            "Iteration:  8705      Current loss:  21.916887283325195      Current accuracy:  21.0      Comp_time of one batch:  -0.8808016777038574\n",
            "Iteration:  8710      Current loss:  17.432775497436523      Current accuracy:  22.0      Comp_time of one batch:  -0.8800396919250488\n",
            "Iteration:  8715      Current loss:  20.49869155883789      Current accuracy:  23.0      Comp_time of one batch:  -0.8783419132232666\n",
            "Iteration:  8720      Current loss:  22.626636505126953      Current accuracy:  24.0      Comp_time of one batch:  -0.8731698989868164\n",
            "Iteration:  8725      Current loss:  25.51865577697754      Current accuracy:  24.0      Comp_time of one batch:  -0.8775601387023926\n",
            "Iteration:  8730      Current loss:  24.94894027709961      Current accuracy:  24.0      Comp_time of one batch:  -0.8868639469146729\n",
            "Iteration:  8735      Current loss:  23.304353713989258      Current accuracy:  24.0      Comp_time of one batch:  -0.8817367553710938\n",
            "Iteration:  8740      Current loss:  20.898786544799805      Current accuracy:  25.0      Comp_time of one batch:  -0.9099676609039307\n",
            "Iteration:  8745      Current loss:  23.41796112060547      Current accuracy:  26.0      Comp_time of one batch:  -0.9016351699829102\n",
            "Iteration:  8750      Current loss:  21.659687042236328      Current accuracy:  27.0      Comp_time of one batch:  -0.877307653427124\n",
            "Iteration:  8755      Current loss:  20.693628311157227      Current accuracy:  28.0      Comp_time of one batch:  -0.8983328342437744\n",
            "Iteration:  8760      Current loss:  26.63311004638672      Current accuracy:  28.0      Comp_time of one batch:  -0.9052290916442871\n",
            "Iteration:  8765      Current loss:  32.01823043823242      Current accuracy:  28.0      Comp_time of one batch:  -0.8843414783477783\n",
            "Iteration:  8770      Current loss:  33.23766326904297      Current accuracy:  28.0      Comp_time of one batch:  -0.886073112487793\n",
            "Iteration:  8775      Current loss:  26.07014274597168      Current accuracy:  28.0      Comp_time of one batch:  -0.887444257736206\n",
            "Iteration:  8780      Current loss:  27.795637130737305      Current accuracy:  28.0      Comp_time of one batch:  -0.889540433883667\n",
            "Iteration:  8785      Current loss:  25.4495906829834      Current accuracy:  28.0      Comp_time of one batch:  -0.8864185810089111\n",
            "Iteration:  8790      Current loss:  19.540327072143555      Current accuracy:  29.0      Comp_time of one batch:  -0.9050116539001465\n",
            "Iteration:  8795      Current loss:  24.751951217651367      Current accuracy:  29.0      Comp_time of one batch:  -0.8853638172149658\n",
            "Iteration:  8800      Current loss:  33.27932357788086      Current accuracy:  29.0      Comp_time of one batch:  -0.9103143215179443\n",
            "Iteration:  8805      Current loss:  26.1251220703125      Current accuracy:  29.0      Comp_time of one batch:  -0.8759870529174805\n",
            "Iteration:  8810      Current loss:  20.646329879760742      Current accuracy:  30.0      Comp_time of one batch:  -0.8873944282531738\n",
            "Iteration:  8815      Current loss:  19.928625106811523      Current accuracy:  31.0      Comp_time of one batch:  -0.8806416988372803\n",
            "Iteration:  8820      Current loss:  22.67277717590332      Current accuracy:  31.0      Comp_time of one batch:  -0.8931210041046143\n",
            "Iteration:  8825      Current loss:  24.118297576904297      Current accuracy:  32.0      Comp_time of one batch:  -0.9059116840362549\n",
            "Iteration:  8830      Current loss:  25.927616119384766      Current accuracy:  32.0      Comp_time of one batch:  -0.9096171855926514\n",
            "Iteration:  8835      Current loss:  24.802154541015625      Current accuracy:  32.0      Comp_time of one batch:  -0.8676397800445557\n",
            "Iteration:  8840      Current loss:  26.179224014282227      Current accuracy:  32.0      Comp_time of one batch:  -0.8739185333251953\n",
            "Iteration:  8845      Current loss:  20.070402145385742      Current accuracy:  33.0      Comp_time of one batch:  -0.8758041858673096\n",
            "Iteration:  8850      Current loss:  19.184295654296875      Current accuracy:  34.0      Comp_time of one batch:  -0.8637120723724365\n",
            "Iteration:  8855      Current loss:  32.34523391723633      Current accuracy:  34.0      Comp_time of one batch:  -0.9021968841552734\n",
            "Iteration:  8860      Current loss:  22.862993240356445      Current accuracy:  35.0      Comp_time of one batch:  -0.9154658317565918\n",
            "Iteration:  8865      Current loss:  21.425460815429688      Current accuracy:  36.0      Comp_time of one batch:  -0.8900282382965088\n",
            "Iteration:  8870      Current loss:  23.002458572387695      Current accuracy:  37.0      Comp_time of one batch:  -0.8844568729400635\n",
            "Iteration:  8875      Current loss:  21.69147491455078      Current accuracy:  38.0      Comp_time of one batch:  -0.9034571647644043\n",
            "Iteration:  8880      Current loss:  36.77915954589844      Current accuracy:  38.0      Comp_time of one batch:  -0.9009180068969727\n",
            "Iteration:  8885      Current loss:  22.436138153076172      Current accuracy:  39.0      Comp_time of one batch:  -0.8825485706329346\n",
            "Iteration:  8890      Current loss:  31.444599151611328      Current accuracy:  39.0      Comp_time of one batch:  -0.8780324459075928\n",
            "Iteration:  8895      Current loss:  27.061128616333008      Current accuracy:  39.0      Comp_time of one batch:  -0.8882310390472412\n",
            "Iteration:  8900      Current loss:  27.76896095275879      Current accuracy:  39.0      Comp_time of one batch:  -0.8758819103240967\n",
            "Iteration:  8905      Current loss:  20.54067611694336      Current accuracy:  40.0      Comp_time of one batch:  -0.8880891799926758\n",
            "Iteration:  8910      Current loss:  24.54631805419922      Current accuracy:  40.0      Comp_time of one batch:  -0.8892648220062256\n",
            "Iteration:  8915      Current loss:  21.785375595092773      Current accuracy:  41.0      Comp_time of one batch:  -0.8965458869934082\n",
            "Iteration:  8920      Current loss:  27.48128890991211      Current accuracy:  42.0      Comp_time of one batch:  -0.880112886428833\n",
            "Iteration:  8925      Current loss:  25.417173385620117      Current accuracy:  43.0      Comp_time of one batch:  -0.8836383819580078\n",
            "Iteration:  8930      Current loss:  22.779329299926758      Current accuracy:  44.0      Comp_time of one batch:  -0.8862755298614502\n",
            "Iteration:  8935      Current loss:  16.502092361450195      Current accuracy:  45.0      Comp_time of one batch:  -0.8788697719573975\n",
            "Iteration:  8940      Current loss:  21.307519912719727      Current accuracy:  46.0      Comp_time of one batch:  -0.8809387683868408\n",
            "Iteration:  8945      Current loss:  22.85175323486328      Current accuracy:  46.0      Comp_time of one batch:  -0.905449628829956\n",
            "Iteration:  8950      Current loss:  25.96483039855957      Current accuracy:  46.0      Comp_time of one batch:  -0.9033496379852295\n",
            "Iteration:  8955      Current loss:  21.597990036010742      Current accuracy:  47.0      Comp_time of one batch:  -0.874197244644165\n",
            "Iteration:  8960      Current loss:  24.324546813964844      Current accuracy:  47.0      Comp_time of one batch:  -0.8753397464752197\n",
            "Iteration:  8965      Current loss:  25.43022918701172      Current accuracy:  47.0      Comp_time of one batch:  -0.8882474899291992\n",
            "Iteration:  8970      Current loss:  23.7418155670166      Current accuracy:  48.0      Comp_time of one batch:  -0.8792572021484375\n",
            "Iteration:  8975      Current loss:  22.85317039489746      Current accuracy:  49.0      Comp_time of one batch:  -0.8801195621490479\n",
            "Iteration:  8980      Current loss:  23.266191482543945      Current accuracy:  49.0      Comp_time of one batch:  -0.8784525394439697\n",
            "Iteration:  8985      Current loss:  19.624406814575195      Current accuracy:  50.0      Comp_time of one batch:  -0.887331485748291\n",
            "Iteration:  8990      Current loss:  20.223224639892578      Current accuracy:  51.0      Comp_time of one batch:  -0.9050500392913818\n",
            "Iteration:  8995      Current loss:  27.14859962463379      Current accuracy:  51.0      Comp_time of one batch:  -0.8822135925292969\n",
            "Iteration:  9000      Current loss:  25.885601043701172      Current accuracy:  0.0      Comp_time of one batch:  -0.8798651695251465\n",
            "Iteration:  9005      Current loss:  23.342098236083984      Current accuracy:  1.0      Comp_time of one batch:  -0.8834691047668457\n",
            "Iteration:  9010      Current loss:  22.516063690185547      Current accuracy:  1.0      Comp_time of one batch:  -0.8836667537689209\n",
            "Iteration:  9015      Current loss:  26.50238800048828      Current accuracy:  1.0      Comp_time of one batch:  -0.9281830787658691\n",
            "Iteration:  9020      Current loss:  24.977842330932617      Current accuracy:  1.0      Comp_time of one batch:  -0.8840997219085693\n",
            "Iteration:  9025      Current loss:  33.27175521850586      Current accuracy:  1.0      Comp_time of one batch:  -0.9107840061187744\n",
            "Iteration:  9030      Current loss:  23.053508758544922      Current accuracy:  1.0      Comp_time of one batch:  -0.9142787456512451\n",
            "Iteration:  9035      Current loss:  22.300994873046875      Current accuracy:  1.0      Comp_time of one batch:  -0.9055252075195312\n",
            "Iteration:  9040      Current loss:  23.598800659179688      Current accuracy:  1.0      Comp_time of one batch:  -0.9009618759155273\n",
            "Iteration:  9045      Current loss:  20.920106887817383      Current accuracy:  2.0      Comp_time of one batch:  -0.8867371082305908\n",
            "Iteration:  9050      Current loss:  34.88840866088867      Current accuracy:  2.0      Comp_time of one batch:  -0.8726551532745361\n",
            "Iteration:  9055      Current loss:  22.51647186279297      Current accuracy:  2.0      Comp_time of one batch:  -0.8936703205108643\n",
            "Iteration:  9060      Current loss:  23.377954483032227      Current accuracy:  3.0      Comp_time of one batch:  -0.8761217594146729\n",
            "Iteration:  9065      Current loss:  21.167633056640625      Current accuracy:  4.0      Comp_time of one batch:  -0.8568017482757568\n",
            "Iteration:  9070      Current loss:  20.489286422729492      Current accuracy:  5.0      Comp_time of one batch:  -0.8789441585540771\n",
            "Iteration:  9075      Current loss:  17.457048416137695      Current accuracy:  6.0      Comp_time of one batch:  -0.8826708793640137\n",
            "Iteration:  9080      Current loss:  15.618871688842773      Current accuracy:  7.0      Comp_time of one batch:  -0.8877012729644775\n",
            "Iteration:  9085      Current loss:  22.11341094970703      Current accuracy:  8.0      Comp_time of one batch:  -0.8745498657226562\n",
            "Iteration:  9090      Current loss:  31.409912109375      Current accuracy:  8.0      Comp_time of one batch:  -0.8996703624725342\n",
            "Iteration:  9095      Current loss:  22.82373809814453      Current accuracy:  9.0      Comp_time of one batch:  -0.8985903263092041\n",
            "Iteration:  9100      Current loss:  18.739118576049805      Current accuracy:  10.0      Comp_time of one batch:  -0.9050145149230957\n",
            "Iteration:  9105      Current loss:  19.883079528808594      Current accuracy:  11.0      Comp_time of one batch:  -0.8957738876342773\n",
            "Iteration:  9110      Current loss:  20.580120086669922      Current accuracy:  12.0      Comp_time of one batch:  -0.8972232341766357\n",
            "Iteration:  9115      Current loss:  29.179306030273438      Current accuracy:  12.0      Comp_time of one batch:  -0.9045169353485107\n",
            "Iteration:  9120      Current loss:  24.36084747314453      Current accuracy:  12.0      Comp_time of one batch:  -0.891261100769043\n",
            "Iteration:  9125      Current loss:  22.7951602935791      Current accuracy:  12.0      Comp_time of one batch:  -0.893052339553833\n",
            "Iteration:  9130      Current loss:  27.905696868896484      Current accuracy:  12.0      Comp_time of one batch:  -0.9034631252288818\n",
            "Iteration:  9135      Current loss:  33.94377517700195      Current accuracy:  12.0      Comp_time of one batch:  -0.9267585277557373\n",
            "Iteration:  9140      Current loss:  23.76831817626953      Current accuracy:  12.0      Comp_time of one batch:  -0.9284622669219971\n",
            "Iteration:  9145      Current loss:  19.684528350830078      Current accuracy:  13.0      Comp_time of one batch:  -0.9344959259033203\n",
            "Iteration:  9150      Current loss:  22.073713302612305      Current accuracy:  14.0      Comp_time of one batch:  -0.9000663757324219\n",
            "Iteration:  9155      Current loss:  26.12717628479004      Current accuracy:  14.0      Comp_time of one batch:  -0.8798201084136963\n",
            "Iteration:  9160      Current loss:  21.310890197753906      Current accuracy:  15.0      Comp_time of one batch:  -0.8689515590667725\n",
            "Iteration:  9165      Current loss:  22.665666580200195      Current accuracy:  16.0      Comp_time of one batch:  -0.8913309574127197\n",
            "Iteration:  9170      Current loss:  32.28935241699219      Current accuracy:  16.0      Comp_time of one batch:  -0.8903622627258301\n",
            "Iteration:  9175      Current loss:  23.114913940429688      Current accuracy:  17.0      Comp_time of one batch:  -0.8784687519073486\n",
            "Iteration:  9180      Current loss:  24.315393447875977      Current accuracy:  17.0      Comp_time of one batch:  -0.879986047744751\n",
            "Iteration:  9185      Current loss:  24.671188354492188      Current accuracy:  18.0      Comp_time of one batch:  -0.868335485458374\n",
            "Iteration:  9190      Current loss:  23.716859817504883      Current accuracy:  18.0      Comp_time of one batch:  -0.8833301067352295\n",
            "Iteration:  9195      Current loss:  16.347026824951172      Current accuracy:  19.0      Comp_time of one batch:  -0.8682651519775391\n",
            "Iteration:  9200      Current loss:  16.214111328125      Current accuracy:  20.0      Comp_time of one batch:  -0.8697912693023682\n",
            "Iteration:  9205      Current loss:  26.061338424682617      Current accuracy:  20.0      Comp_time of one batch:  -0.8890130519866943\n",
            "Iteration:  9210      Current loss:  17.470136642456055      Current accuracy:  21.0      Comp_time of one batch:  -0.8771214485168457\n",
            "Iteration:  9215      Current loss:  30.884946823120117      Current accuracy:  21.0      Comp_time of one batch:  -0.882190465927124\n",
            "Iteration:  9220      Current loss:  27.70299530029297      Current accuracy:  21.0      Comp_time of one batch:  -0.9081227779388428\n",
            "Iteration:  9225      Current loss:  19.897777557373047      Current accuracy:  22.0      Comp_time of one batch:  -0.8768575191497803\n",
            "Iteration:  9230      Current loss:  23.324010848999023      Current accuracy:  23.0      Comp_time of one batch:  -0.9057676792144775\n",
            "Iteration:  9235      Current loss:  32.6156005859375      Current accuracy:  23.0      Comp_time of one batch:  -0.9079303741455078\n",
            "Iteration:  9240      Current loss:  25.895570755004883      Current accuracy:  23.0      Comp_time of one batch:  -0.8844568729400635\n",
            "Iteration:  9245      Current loss:  24.139028549194336      Current accuracy:  23.0      Comp_time of one batch:  -0.8935062885284424\n",
            "Iteration:  9250      Current loss:  20.37677001953125      Current accuracy:  24.0      Comp_time of one batch:  -0.901287317276001\n",
            "Iteration:  9255      Current loss:  21.83816909790039      Current accuracy:  25.0      Comp_time of one batch:  -0.8863890171051025\n",
            "Iteration:  9260      Current loss:  30.93414878845215      Current accuracy:  25.0      Comp_time of one batch:  -0.8859753608703613\n",
            "Iteration:  9265      Current loss:  17.214637756347656      Current accuracy:  26.0      Comp_time of one batch:  -0.8989059925079346\n",
            "Iteration:  9270      Current loss:  18.92376136779785      Current accuracy:  27.0      Comp_time of one batch:  -0.8987264633178711\n",
            "Iteration:  9275      Current loss:  26.308208465576172      Current accuracy:  27.0      Comp_time of one batch:  -0.8920009136199951\n",
            "Iteration:  9280      Current loss:  23.705615997314453      Current accuracy:  27.0      Comp_time of one batch:  -0.9085667133331299\n",
            "Iteration:  9285      Current loss:  24.44403839111328      Current accuracy:  27.0      Comp_time of one batch:  -0.8816273212432861\n",
            "Iteration:  9290      Current loss:  20.91242027282715      Current accuracy:  28.0      Comp_time of one batch:  -0.8886373043060303\n",
            "Iteration:  9295      Current loss:  38.54280471801758      Current accuracy:  28.0      Comp_time of one batch:  -0.9014239311218262\n",
            "Iteration:  9300      Current loss:  19.622743606567383      Current accuracy:  29.0      Comp_time of one batch:  -0.8981704711914062\n",
            "Iteration:  9305      Current loss:  31.01863670349121      Current accuracy:  29.0      Comp_time of one batch:  -0.8924210071563721\n",
            "Iteration:  9310      Current loss:  24.18216896057129      Current accuracy:  30.0      Comp_time of one batch:  -0.8734688758850098\n",
            "Iteration:  9315      Current loss:  21.894187927246094      Current accuracy:  31.0      Comp_time of one batch:  -0.8728475570678711\n",
            "Iteration:  9320      Current loss:  15.8323392868042      Current accuracy:  32.0      Comp_time of one batch:  -0.8968698978424072\n",
            "Iteration:  9325      Current loss:  41.713008880615234      Current accuracy:  32.0      Comp_time of one batch:  -0.8997683525085449\n",
            "Iteration:  9330      Current loss:  26.482215881347656      Current accuracy:  32.0      Comp_time of one batch:  -0.8802945613861084\n",
            "Iteration:  9335      Current loss:  17.32460594177246      Current accuracy:  33.0      Comp_time of one batch:  -0.8896503448486328\n",
            "Iteration:  9340      Current loss:  21.52292251586914      Current accuracy:  34.0      Comp_time of one batch:  -0.8735935688018799\n",
            "Iteration:  9345      Current loss:  19.21445083618164      Current accuracy:  35.0      Comp_time of one batch:  -0.8747265338897705\n",
            "Iteration:  9350      Current loss:  25.12784767150879      Current accuracy:  35.0      Comp_time of one batch:  -0.8961400985717773\n",
            "Iteration:  9355      Current loss:  20.156587600708008      Current accuracy:  36.0      Comp_time of one batch:  -0.8817200660705566\n",
            "Iteration:  9360      Current loss:  19.797340393066406      Current accuracy:  37.0      Comp_time of one batch:  -0.8826603889465332\n",
            "Iteration:  9365      Current loss:  24.338577270507812      Current accuracy:  37.0      Comp_time of one batch:  -0.8971419334411621\n",
            "Iteration:  9370      Current loss:  35.00736618041992      Current accuracy:  37.0      Comp_time of one batch:  -0.8789699077606201\n",
            "Iteration:  9375      Current loss:  18.26934242248535      Current accuracy:  38.0      Comp_time of one batch:  -0.8976309299468994\n",
            "Iteration:  9380      Current loss:  19.553298950195312      Current accuracy:  39.0      Comp_time of one batch:  -0.8782453536987305\n",
            "Iteration:  9385      Current loss:  21.567808151245117      Current accuracy:  40.0      Comp_time of one batch:  -0.8863844871520996\n",
            "Iteration:  9390      Current loss:  24.290523529052734      Current accuracy:  40.0      Comp_time of one batch:  -0.8989272117614746\n",
            "Iteration:  9395      Current loss:  37.84724807739258      Current accuracy:  40.0      Comp_time of one batch:  -0.8788957595825195\n",
            "Iteration:  9400      Current loss:  25.500728607177734      Current accuracy:  40.0      Comp_time of one batch:  -0.8833246231079102\n",
            "Iteration:  9405      Current loss:  19.874839782714844      Current accuracy:  41.0      Comp_time of one batch:  -0.9057257175445557\n",
            "Iteration:  9410      Current loss:  32.03260803222656      Current accuracy:  41.0      Comp_time of one batch:  -0.8800098896026611\n",
            "Iteration:  9415      Current loss:  30.786714553833008      Current accuracy:  41.0      Comp_time of one batch:  -0.8995451927185059\n",
            "Iteration:  9420      Current loss:  20.50193214416504      Current accuracy:  42.0      Comp_time of one batch:  -0.896172046661377\n",
            "Iteration:  9425      Current loss:  23.416921615600586      Current accuracy:  43.0      Comp_time of one batch:  -0.892601490020752\n",
            "Iteration:  9430      Current loss:  21.846481323242188      Current accuracy:  44.0      Comp_time of one batch:  -0.8699381351470947\n",
            "Iteration:  9435      Current loss:  29.312213897705078      Current accuracy:  44.0      Comp_time of one batch:  -0.8760416507720947\n",
            "Iteration:  9440      Current loss:  25.976852416992188      Current accuracy:  44.0      Comp_time of one batch:  -0.8837864398956299\n",
            "Iteration:  9445      Current loss:  32.881168365478516      Current accuracy:  44.0      Comp_time of one batch:  -0.882504940032959\n",
            "Iteration:  9450      Current loss:  23.710739135742188      Current accuracy:  45.0      Comp_time of one batch:  -0.9299583435058594\n",
            "Iteration:  9455      Current loss:  30.29647445678711      Current accuracy:  45.0      Comp_time of one batch:  -0.8772151470184326\n",
            "Iteration:  9460      Current loss:  26.11292266845703      Current accuracy:  45.0      Comp_time of one batch:  -0.9000897407531738\n",
            "Iteration:  9465      Current loss:  25.171588897705078      Current accuracy:  45.0      Comp_time of one batch:  -0.8931670188903809\n",
            "Iteration:  9470      Current loss:  30.537565231323242      Current accuracy:  45.0      Comp_time of one batch:  -0.9161601066589355\n",
            "Iteration:  9475      Current loss:  32.3508415222168      Current accuracy:  45.0      Comp_time of one batch:  -0.8864583969116211\n",
            "Iteration:  9480      Current loss:  31.610149383544922      Current accuracy:  45.0      Comp_time of one batch:  -0.9147405624389648\n",
            "Iteration:  9485      Current loss:  22.061717987060547      Current accuracy:  46.0      Comp_time of one batch:  -0.8869223594665527\n",
            "Iteration:  9490      Current loss:  25.667022705078125      Current accuracy:  46.0      Comp_time of one batch:  -0.9046907424926758\n",
            "Iteration:  9495      Current loss:  25.53714942932129      Current accuracy:  46.0      Comp_time of one batch:  -0.89794921875\n",
            "Iteration:  9500      Current loss:  24.409740447998047      Current accuracy:  0.0      Comp_time of one batch:  -0.8886463642120361\n",
            "Iteration:  9505      Current loss:  18.032211303710938      Current accuracy:  1.0      Comp_time of one batch:  -0.9058089256286621\n",
            "Iteration:  9510      Current loss:  27.285505294799805      Current accuracy:  1.0      Comp_time of one batch:  -0.8834261894226074\n",
            "Iteration:  9515      Current loss:  26.009113311767578      Current accuracy:  1.0      Comp_time of one batch:  -0.9007868766784668\n",
            "Iteration:  9520      Current loss:  18.89980697631836      Current accuracy:  2.0      Comp_time of one batch:  -0.8781559467315674\n",
            "Iteration:  9525      Current loss:  29.163415908813477      Current accuracy:  2.0      Comp_time of one batch:  -0.8814706802368164\n",
            "Iteration:  9530      Current loss:  23.236286163330078      Current accuracy:  2.0      Comp_time of one batch:  -0.8868563175201416\n",
            "Iteration:  9535      Current loss:  18.257389068603516      Current accuracy:  3.0      Comp_time of one batch:  -0.8723409175872803\n",
            "Iteration:  9540      Current loss:  20.335268020629883      Current accuracy:  4.0      Comp_time of one batch:  -0.8986244201660156\n",
            "Iteration:  9545      Current loss:  28.920494079589844      Current accuracy:  4.0      Comp_time of one batch:  -0.8753077983856201\n",
            "Iteration:  9550      Current loss:  26.57185935974121      Current accuracy:  4.0      Comp_time of one batch:  -0.8920061588287354\n",
            "Iteration:  9555      Current loss:  14.622673034667969      Current accuracy:  5.0      Comp_time of one batch:  -0.8806226253509521\n",
            "Iteration:  9560      Current loss:  21.53740119934082      Current accuracy:  6.0      Comp_time of one batch:  -0.8749411106109619\n",
            "Iteration:  9565      Current loss:  21.38553810119629      Current accuracy:  7.0      Comp_time of one batch:  -0.8747234344482422\n",
            "Iteration:  9570      Current loss:  21.239768981933594      Current accuracy:  8.0      Comp_time of one batch:  -0.8738133907318115\n",
            "Iteration:  9575      Current loss:  34.60434341430664      Current accuracy:  8.0      Comp_time of one batch:  -0.8778548240661621\n",
            "Iteration:  9580      Current loss:  16.14568328857422      Current accuracy:  9.0      Comp_time of one batch:  -0.8784496784210205\n",
            "Iteration:  9585      Current loss:  30.017309188842773      Current accuracy:  9.0      Comp_time of one batch:  -0.9105832576751709\n",
            "Iteration:  9590      Current loss:  19.549060821533203      Current accuracy:  10.0      Comp_time of one batch:  -0.8750131130218506\n",
            "Iteration:  9595      Current loss:  22.56145668029785      Current accuracy:  11.0      Comp_time of one batch:  -0.9171392917633057\n",
            "Iteration:  9600      Current loss:  23.623332977294922      Current accuracy:  11.0      Comp_time of one batch:  -0.8811273574829102\n",
            "Iteration:  9605      Current loss:  27.686655044555664      Current accuracy:  11.0      Comp_time of one batch:  -0.8723611831665039\n",
            "Iteration:  9610      Current loss:  24.71600341796875      Current accuracy:  11.0      Comp_time of one batch:  -0.8890278339385986\n",
            "Iteration:  9615      Current loss:  27.013063430786133      Current accuracy:  11.0      Comp_time of one batch:  -0.8927474021911621\n",
            "Iteration:  9620      Current loss:  23.132144927978516      Current accuracy:  11.0      Comp_time of one batch:  -0.8801138401031494\n",
            "Iteration:  9625      Current loss:  21.46644401550293      Current accuracy:  12.0      Comp_time of one batch:  -0.8700006008148193\n",
            "Iteration:  9630      Current loss:  30.973352432250977      Current accuracy:  12.0      Comp_time of one batch:  -0.8871421813964844\n",
            "Iteration:  9635      Current loss:  20.494571685791016      Current accuracy:  13.0      Comp_time of one batch:  -0.8889977931976318\n",
            "Iteration:  9640      Current loss:  20.87374496459961      Current accuracy:  14.0      Comp_time of one batch:  -0.8845374584197998\n",
            "Iteration:  9645      Current loss:  18.318819046020508      Current accuracy:  15.0      Comp_time of one batch:  -0.8758690357208252\n",
            "Iteration:  9650      Current loss:  23.091175079345703      Current accuracy:  15.0      Comp_time of one batch:  -0.8831744194030762\n",
            "Iteration:  9655      Current loss:  24.657241821289062      Current accuracy:  15.0      Comp_time of one batch:  -0.8817236423492432\n",
            "Iteration:  9660      Current loss:  25.246437072753906      Current accuracy:  15.0      Comp_time of one batch:  -0.9019906520843506\n",
            "Iteration:  9665      Current loss:  34.821842193603516      Current accuracy:  15.0      Comp_time of one batch:  -0.8947293758392334\n",
            "Iteration:  9670      Current loss:  21.728208541870117      Current accuracy:  16.0      Comp_time of one batch:  -0.8874924182891846\n",
            "Iteration:  9675      Current loss:  28.718402862548828      Current accuracy:  16.0      Comp_time of one batch:  -0.8718323707580566\n",
            "Iteration:  9680      Current loss:  22.982139587402344      Current accuracy:  17.0      Comp_time of one batch:  -0.89715576171875\n",
            "Iteration:  9685      Current loss:  24.254440307617188      Current accuracy:  17.0      Comp_time of one batch:  -0.877342939376831\n",
            "Iteration:  9690      Current loss:  25.97627830505371      Current accuracy:  17.0      Comp_time of one batch:  -0.889824628829956\n",
            "Iteration:  9695      Current loss:  26.484615325927734      Current accuracy:  17.0      Comp_time of one batch:  -0.9043641090393066\n",
            "Iteration:  9700      Current loss:  28.717924118041992      Current accuracy:  17.0      Comp_time of one batch:  -0.8898298740386963\n",
            "Iteration:  9705      Current loss:  22.731937408447266      Current accuracy:  17.0      Comp_time of one batch:  -0.8985652923583984\n",
            "Iteration:  9710      Current loss:  24.589218139648438      Current accuracy:  18.0      Comp_time of one batch:  -0.9035689830780029\n",
            "Iteration:  9715      Current loss:  29.36028289794922      Current accuracy:  18.0      Comp_time of one batch:  -0.8844683170318604\n",
            "Iteration:  9720      Current loss:  28.449810028076172      Current accuracy:  18.0      Comp_time of one batch:  -0.9105315208435059\n",
            "Iteration:  9725      Current loss:  30.913841247558594      Current accuracy:  18.0      Comp_time of one batch:  -0.8923215866088867\n",
            "Iteration:  9730      Current loss:  32.375484466552734      Current accuracy:  18.0      Comp_time of one batch:  -0.9152383804321289\n",
            "Iteration:  9735      Current loss:  27.212488174438477      Current accuracy:  19.0      Comp_time of one batch:  -0.8736250400543213\n",
            "Iteration:  9740      Current loss:  21.129470825195312      Current accuracy:  20.0      Comp_time of one batch:  -0.8853967189788818\n",
            "Iteration:  9745      Current loss:  21.99053192138672      Current accuracy:  21.0      Comp_time of one batch:  -0.889918327331543\n",
            "Iteration:  9750      Current loss:  18.662628173828125      Current accuracy:  22.0      Comp_time of one batch:  -0.8896386623382568\n",
            "Iteration:  9755      Current loss:  31.141643524169922      Current accuracy:  22.0      Comp_time of one batch:  -0.8854842185974121\n",
            "Iteration:  9760      Current loss:  20.164527893066406      Current accuracy:  23.0      Comp_time of one batch:  -0.9048383235931396\n",
            "Iteration:  9765      Current loss:  26.360506057739258      Current accuracy:  23.0      Comp_time of one batch:  -0.909005880355835\n",
            "Iteration:  9770      Current loss:  24.725929260253906      Current accuracy:  23.0      Comp_time of one batch:  -0.8965239524841309\n",
            "Iteration:  9775      Current loss:  24.433019638061523      Current accuracy:  23.0      Comp_time of one batch:  -0.8880553245544434\n",
            "Iteration:  9780      Current loss:  27.2818660736084      Current accuracy:  23.0      Comp_time of one batch:  -0.8787479400634766\n",
            "Iteration:  9785      Current loss:  22.24254608154297      Current accuracy:  24.0      Comp_time of one batch:  -0.8698880672454834\n",
            "Iteration:  9790      Current loss:  26.74607276916504      Current accuracy:  24.0      Comp_time of one batch:  -0.9037339687347412\n",
            "Iteration:  9795      Current loss:  12.106822967529297      Current accuracy:  25.0      Comp_time of one batch:  -0.8925964832305908\n",
            "Iteration:  9800      Current loss:  21.8836612701416      Current accuracy:  26.0      Comp_time of one batch:  -0.8811869621276855\n",
            "Iteration:  9805      Current loss:  22.6192626953125      Current accuracy:  27.0      Comp_time of one batch:  -0.8973410129547119\n",
            "Iteration:  9810      Current loss:  19.433271408081055      Current accuracy:  28.0      Comp_time of one batch:  -0.9022698402404785\n",
            "Iteration:  9815      Current loss:  25.644994735717773      Current accuracy:  28.0      Comp_time of one batch:  -0.8742053508758545\n",
            "Iteration:  9820      Current loss:  25.97184944152832      Current accuracy:  28.0      Comp_time of one batch:  -0.875922441482544\n",
            "Iteration:  9825      Current loss:  32.15232849121094      Current accuracy:  28.0      Comp_time of one batch:  -0.8755459785461426\n",
            "Iteration:  9830      Current loss:  30.225561141967773      Current accuracy:  28.0      Comp_time of one batch:  -0.8908445835113525\n",
            "Iteration:  9835      Current loss:  30.927980422973633      Current accuracy:  28.0      Comp_time of one batch:  -0.8792724609375\n",
            "Iteration:  9840      Current loss:  26.974685668945312      Current accuracy:  28.0      Comp_time of one batch:  -0.9188971519470215\n",
            "Iteration:  9845      Current loss:  26.34981918334961      Current accuracy:  28.0      Comp_time of one batch:  -0.8738536834716797\n",
            "Iteration:  9850      Current loss:  31.532684326171875      Current accuracy:  28.0      Comp_time of one batch:  -0.909630298614502\n",
            "Iteration:  9855      Current loss:  17.693191528320312      Current accuracy:  29.0      Comp_time of one batch:  -0.8689875602722168\n",
            "Iteration:  9860      Current loss:  23.323410034179688      Current accuracy:  30.0      Comp_time of one batch:  -0.8803327083587646\n",
            "Iteration:  9865      Current loss:  22.537704467773438      Current accuracy:  31.0      Comp_time of one batch:  -0.8808701038360596\n",
            "Iteration:  9870      Current loss:  18.08658790588379      Current accuracy:  32.0      Comp_time of one batch:  -0.871121883392334\n",
            "Iteration:  9875      Current loss:  28.092601776123047      Current accuracy:  32.0      Comp_time of one batch:  -0.8739724159240723\n",
            "Iteration:  9880      Current loss:  22.549781799316406      Current accuracy:  33.0      Comp_time of one batch:  -0.8895986080169678\n",
            "Iteration:  9885      Current loss:  26.941221237182617      Current accuracy:  33.0      Comp_time of one batch:  -0.8761143684387207\n",
            "Iteration:  9890      Current loss:  22.968643188476562      Current accuracy:  34.0      Comp_time of one batch:  -0.87520432472229\n",
            "Iteration:  9895      Current loss:  21.238723754882812      Current accuracy:  35.0      Comp_time of one batch:  -0.8974316120147705\n",
            "Iteration:  9900      Current loss:  21.429990768432617      Current accuracy:  36.0      Comp_time of one batch:  -0.8896751403808594\n",
            "Iteration:  9905      Current loss:  22.931320190429688      Current accuracy:  36.0      Comp_time of one batch:  -0.882927417755127\n",
            "Iteration:  9910      Current loss:  22.5814266204834      Current accuracy:  37.0      Comp_time of one batch:  -0.9164674282073975\n",
            "Iteration:  9915      Current loss:  19.87510871887207      Current accuracy:  38.0      Comp_time of one batch:  -0.8830749988555908\n",
            "Iteration:  9920      Current loss:  18.08539581298828      Current accuracy:  39.0      Comp_time of one batch:  -0.8993144035339355\n",
            "Iteration:  9925      Current loss:  24.724422454833984      Current accuracy:  39.0      Comp_time of one batch:  -0.8940646648406982\n",
            "Iteration:  9930      Current loss:  25.028789520263672      Current accuracy:  39.0      Comp_time of one batch:  -0.8806428909301758\n",
            "Iteration:  9935      Current loss:  27.739973068237305      Current accuracy:  39.0      Comp_time of one batch:  -0.8629992008209229\n",
            "Iteration:  9940      Current loss:  18.56447982788086      Current accuracy:  40.0      Comp_time of one batch:  -0.9065992832183838\n",
            "Iteration:  9945      Current loss:  28.532499313354492      Current accuracy:  40.0      Comp_time of one batch:  -0.8829264640808105\n",
            "Iteration:  9950      Current loss:  26.351634979248047      Current accuracy:  40.0      Comp_time of one batch:  -0.8764245510101318\n",
            "Iteration:  9955      Current loss:  31.50922393798828      Current accuracy:  40.0      Comp_time of one batch:  -0.8693933486938477\n",
            "Iteration:  9960      Current loss:  22.29033851623535      Current accuracy:  41.0      Comp_time of one batch:  -0.8805382251739502\n",
            "Iteration:  9965      Current loss:  26.373374938964844      Current accuracy:  41.0      Comp_time of one batch:  -0.9089586734771729\n",
            "Iteration:  9970      Current loss:  28.246278762817383      Current accuracy:  41.0      Comp_time of one batch:  -0.8814616203308105\n",
            "Iteration:  9975      Current loss:  31.21061897277832      Current accuracy:  41.0      Comp_time of one batch:  -0.8786654472351074\n",
            "Iteration:  9980      Current loss:  30.242767333984375      Current accuracy:  41.0      Comp_time of one batch:  -0.890601634979248\n",
            "Iteration:  9985      Current loss:  28.25164222717285      Current accuracy:  41.0      Comp_time of one batch:  -0.8945801258087158\n",
            "Iteration:  9990      Current loss:  29.55626678466797      Current accuracy:  41.0      Comp_time of one batch:  -0.8897261619567871\n",
            "Iteration:  9995      Current loss:  24.9327335357666      Current accuracy:  41.0      Comp_time of one batch:  -0.8672966957092285\n",
            "Iteration:  10000      Current loss:  29.879810333251953      Current accuracy:  0.0      Comp_time of one batch:  -0.9102339744567871\n",
            "Iteration:  10005      Current loss:  32.09181213378906      Current accuracy:  0.0      Comp_time of one batch:  -0.8625743389129639\n",
            "Iteration:  10010      Current loss:  19.799348831176758      Current accuracy:  1.0      Comp_time of one batch:  -0.880929708480835\n",
            "Iteration:  10015      Current loss:  35.03071594238281      Current accuracy:  1.0      Comp_time of one batch:  -0.8783714771270752\n",
            "Iteration:  10020      Current loss:  19.642555236816406      Current accuracy:  2.0      Comp_time of one batch:  -0.9014983177185059\n",
            "Iteration:  10025      Current loss:  20.135719299316406      Current accuracy:  3.0      Comp_time of one batch:  -0.9041404724121094\n",
            "Iteration:  10030      Current loss:  20.524978637695312      Current accuracy:  4.0      Comp_time of one batch:  -0.9237620830535889\n",
            "Iteration:  10035      Current loss:  28.46944808959961      Current accuracy:  4.0      Comp_time of one batch:  -0.882080078125\n",
            "Iteration:  10040      Current loss:  32.42448425292969      Current accuracy:  4.0      Comp_time of one batch:  -0.8956100940704346\n",
            "Iteration:  10045      Current loss:  24.799062728881836      Current accuracy:  4.0      Comp_time of one batch:  -0.8745479583740234\n",
            "Iteration:  10050      Current loss:  40.58620071411133      Current accuracy:  4.0      Comp_time of one batch:  -0.9010570049285889\n",
            "Iteration:  10055      Current loss:  16.627796173095703      Current accuracy:  5.0      Comp_time of one batch:  -0.9143242835998535\n",
            "Iteration:  10060      Current loss:  16.05196189880371      Current accuracy:  6.0      Comp_time of one batch:  -0.8798589706420898\n",
            "Iteration:  10065      Current loss:  24.666643142700195      Current accuracy:  6.0      Comp_time of one batch:  -0.8976078033447266\n",
            "Iteration:  10070      Current loss:  44.362056732177734      Current accuracy:  6.0      Comp_time of one batch:  -0.8752083778381348\n",
            "Iteration:  10075      Current loss:  21.471572875976562      Current accuracy:  7.0      Comp_time of one batch:  -0.8783745765686035\n",
            "Iteration:  10080      Current loss:  25.084335327148438      Current accuracy:  7.0      Comp_time of one batch:  -0.8979706764221191\n",
            "Iteration:  10085      Current loss:  23.82868766784668      Current accuracy:  8.0      Comp_time of one batch:  -0.918769359588623\n",
            "Iteration:  10090      Current loss:  23.122446060180664      Current accuracy:  8.0      Comp_time of one batch:  -0.8980512619018555\n",
            "Iteration:  10095      Current loss:  21.314722061157227      Current accuracy:  9.0      Comp_time of one batch:  -0.8787944316864014\n",
            "Iteration:  10100      Current loss:  18.855241775512695      Current accuracy:  10.0      Comp_time of one batch:  -0.8716127872467041\n",
            "Iteration:  10105      Current loss:  24.412996292114258      Current accuracy:  10.0      Comp_time of one batch:  -0.872957706451416\n",
            "Iteration:  10110      Current loss:  21.555545806884766      Current accuracy:  11.0      Comp_time of one batch:  -0.8776280879974365\n",
            "Iteration:  10115      Current loss:  22.842391967773438      Current accuracy:  11.0      Comp_time of one batch:  -0.8798246383666992\n",
            "Iteration:  10120      Current loss:  22.776500701904297      Current accuracy:  12.0      Comp_time of one batch:  -0.879596471786499\n",
            "Iteration:  10125      Current loss:  28.148977279663086      Current accuracy:  12.0      Comp_time of one batch:  -0.8726725578308105\n",
            "Iteration:  10130      Current loss:  30.399093627929688      Current accuracy:  12.0      Comp_time of one batch:  -0.9165949821472168\n",
            "Iteration:  10135      Current loss:  23.825469970703125      Current accuracy:  12.0      Comp_time of one batch:  -0.8747336864471436\n",
            "Iteration:  10140      Current loss:  28.286094665527344      Current accuracy:  12.0      Comp_time of one batch:  -0.8832840919494629\n",
            "Iteration:  10145      Current loss:  22.942365646362305      Current accuracy:  13.0      Comp_time of one batch:  -0.8844103813171387\n",
            "Iteration:  10150      Current loss:  23.029001235961914      Current accuracy:  14.0      Comp_time of one batch:  -0.8814444541931152\n",
            "Iteration:  10155      Current loss:  25.618568420410156      Current accuracy:  14.0      Comp_time of one batch:  -0.9009909629821777\n",
            "Iteration:  10160      Current loss:  19.854171752929688      Current accuracy:  15.0      Comp_time of one batch:  -0.8718400001525879\n",
            "Iteration:  10165      Current loss:  26.93182945251465      Current accuracy:  15.0      Comp_time of one batch:  -0.8809287548065186\n",
            "Iteration:  10170      Current loss:  19.074636459350586      Current accuracy:  16.0      Comp_time of one batch:  -0.9006984233856201\n",
            "Iteration:  10175      Current loss:  24.359851837158203      Current accuracy:  16.0      Comp_time of one batch:  -0.8800468444824219\n",
            "Iteration:  10180      Current loss:  28.144641876220703      Current accuracy:  16.0      Comp_time of one batch:  -0.8840148448944092\n",
            "Iteration:  10185      Current loss:  22.14706802368164      Current accuracy:  17.0      Comp_time of one batch:  -0.8887186050415039\n",
            "Iteration:  10190      Current loss:  20.965486526489258      Current accuracy:  18.0      Comp_time of one batch:  -0.9071083068847656\n",
            "Iteration:  10195      Current loss:  31.295713424682617      Current accuracy:  18.0      Comp_time of one batch:  -0.892585277557373\n",
            "Iteration:  10200      Current loss:  27.712190628051758      Current accuracy:  18.0      Comp_time of one batch:  -0.8823196887969971\n",
            "Iteration:  10205      Current loss:  24.629884719848633      Current accuracy:  18.0      Comp_time of one batch:  -0.882427453994751\n",
            "Iteration:  10210      Current loss:  21.837053298950195      Current accuracy:  19.0      Comp_time of one batch:  -0.8817086219787598\n",
            "Iteration:  10215      Current loss:  27.962520599365234      Current accuracy:  19.0      Comp_time of one batch:  -0.8834383487701416\n",
            "Iteration:  10220      Current loss:  27.75019645690918      Current accuracy:  19.0      Comp_time of one batch:  -0.8779845237731934\n",
            "Iteration:  10225      Current loss:  25.960689544677734      Current accuracy:  19.0      Comp_time of one batch:  -0.8942208290100098\n",
            "Iteration:  10230      Current loss:  25.359066009521484      Current accuracy:  19.0      Comp_time of one batch:  -0.8816781044006348\n",
            "Iteration:  10235      Current loss:  19.041942596435547      Current accuracy:  20.0      Comp_time of one batch:  -0.8809914588928223\n",
            "Iteration:  10240      Current loss:  22.238401412963867      Current accuracy:  20.0      Comp_time of one batch:  -0.8740561008453369\n",
            "Iteration:  10245      Current loss:  21.433452606201172      Current accuracy:  21.0      Comp_time of one batch:  -0.8757169246673584\n",
            "Iteration:  10250      Current loss:  27.1446590423584      Current accuracy:  21.0      Comp_time of one batch:  -0.892573356628418\n",
            "Iteration:  10255      Current loss:  26.555400848388672      Current accuracy:  21.0      Comp_time of one batch:  -0.901216983795166\n",
            "Iteration:  10260      Current loss:  17.159011840820312      Current accuracy:  22.0      Comp_time of one batch:  -0.9164113998413086\n",
            "Iteration:  10265      Current loss:  19.045434951782227      Current accuracy:  23.0      Comp_time of one batch:  -0.8854448795318604\n",
            "Iteration:  10270      Current loss:  24.86708641052246      Current accuracy:  23.0      Comp_time of one batch:  -0.8854224681854248\n",
            "Iteration:  10275      Current loss:  33.35214614868164      Current accuracy:  23.0      Comp_time of one batch:  -0.9135844707489014\n",
            "Iteration:  10280      Current loss:  20.69809341430664      Current accuracy:  24.0      Comp_time of one batch:  -0.9060711860656738\n",
            "Iteration:  10285      Current loss:  27.026933670043945      Current accuracy:  24.0      Comp_time of one batch:  -0.8802118301391602\n",
            "Iteration:  10290      Current loss:  27.329925537109375      Current accuracy:  24.0      Comp_time of one batch:  -0.8842976093292236\n",
            "Iteration:  10295      Current loss:  20.032215118408203      Current accuracy:  25.0      Comp_time of one batch:  -0.8850984573364258\n",
            "Iteration:  10300      Current loss:  19.524003982543945      Current accuracy:  26.0      Comp_time of one batch:  -0.903430700302124\n",
            "Iteration:  10305      Current loss:  23.43355369567871      Current accuracy:  26.0      Comp_time of one batch:  -0.8861615657806396\n",
            "Iteration:  10310      Current loss:  19.44816780090332      Current accuracy:  27.0      Comp_time of one batch:  -0.8767340183258057\n",
            "Iteration:  10315      Current loss:  30.71339988708496      Current accuracy:  27.0      Comp_time of one batch:  -0.8822493553161621\n",
            "Iteration:  10320      Current loss:  28.65877342224121      Current accuracy:  27.0      Comp_time of one batch:  -0.8803310394287109\n",
            "Iteration:  10325      Current loss:  25.9143123626709      Current accuracy:  27.0      Comp_time of one batch:  -0.8710637092590332\n",
            "Iteration:  10330      Current loss:  30.612592697143555      Current accuracy:  27.0      Comp_time of one batch:  -0.8786654472351074\n",
            "Iteration:  10335      Current loss:  25.040388107299805      Current accuracy:  27.0      Comp_time of one batch:  -0.8941073417663574\n",
            "Iteration:  10340      Current loss:  22.961698532104492      Current accuracy:  28.0      Comp_time of one batch:  -0.8921923637390137\n",
            "Iteration:  10345      Current loss:  29.18781089782715      Current accuracy:  28.0      Comp_time of one batch:  -0.9012773036956787\n",
            "Iteration:  10350      Current loss:  22.017589569091797      Current accuracy:  29.0      Comp_time of one batch:  -0.8812258243560791\n",
            "Iteration:  10355      Current loss:  21.893918991088867      Current accuracy:  30.0      Comp_time of one batch:  -0.8924810886383057\n",
            "Iteration:  10360      Current loss:  24.818622589111328      Current accuracy:  30.0      Comp_time of one batch:  -0.8836166858673096\n",
            "Iteration:  10365      Current loss:  18.422945022583008      Current accuracy:  31.0      Comp_time of one batch:  -0.9035954475402832\n",
            "Iteration:  10370      Current loss:  35.092018127441406      Current accuracy:  31.0      Comp_time of one batch:  -0.9068114757537842\n",
            "Iteration:  10375      Current loss:  23.97517967224121      Current accuracy:  32.0      Comp_time of one batch:  -0.8781647682189941\n",
            "Iteration:  10380      Current loss:  25.01679039001465      Current accuracy:  32.0      Comp_time of one batch:  -0.8915367126464844\n",
            "Iteration:  10385      Current loss:  33.24642562866211      Current accuracy:  32.0      Comp_time of one batch:  -0.884941577911377\n",
            "Iteration:  10390      Current loss:  26.844816207885742      Current accuracy:  32.0      Comp_time of one batch:  -0.8865926265716553\n",
            "Iteration:  10395      Current loss:  37.21013641357422      Current accuracy:  32.0      Comp_time of one batch:  -0.8836956024169922\n",
            "Iteration:  10400      Current loss:  26.282787322998047      Current accuracy:  32.0      Comp_time of one batch:  -0.9260401725769043\n",
            "Iteration:  10405      Current loss:  19.128768920898438      Current accuracy:  33.0      Comp_time of one batch:  -0.9008941650390625\n",
            "Iteration:  10410      Current loss:  24.033477783203125      Current accuracy:  33.0      Comp_time of one batch:  -0.8729219436645508\n",
            "Iteration:  10415      Current loss:  26.512733459472656      Current accuracy:  33.0      Comp_time of one batch:  -0.8808383941650391\n",
            "Iteration:  10420      Current loss:  21.098243713378906      Current accuracy:  34.0      Comp_time of one batch:  -0.9094290733337402\n",
            "Iteration:  10425      Current loss:  24.149517059326172      Current accuracy:  34.0      Comp_time of one batch:  -0.9002721309661865\n",
            "Iteration:  10430      Current loss:  32.80345916748047      Current accuracy:  34.0      Comp_time of one batch:  -0.8758053779602051\n",
            "Iteration:  10435      Current loss:  23.798898696899414      Current accuracy:  34.0      Comp_time of one batch:  -0.903080940246582\n",
            "Iteration:  10440      Current loss:  27.983097076416016      Current accuracy:  34.0      Comp_time of one batch:  -0.8790643215179443\n",
            "Iteration:  10445      Current loss:  22.89745330810547      Current accuracy:  35.0      Comp_time of one batch:  -0.8775331974029541\n",
            "Iteration:  10450      Current loss:  26.984994888305664      Current accuracy:  35.0      Comp_time of one batch:  -0.8825407028198242\n",
            "Iteration:  10455      Current loss:  23.791889190673828      Current accuracy:  35.0      Comp_time of one batch:  -0.8806490898132324\n",
            "Iteration:  10460      Current loss:  24.97431182861328      Current accuracy:  35.0      Comp_time of one batch:  -0.8784782886505127\n",
            "Iteration:  10465      Current loss:  24.61126136779785      Current accuracy:  35.0      Comp_time of one batch:  -0.9189267158508301\n",
            "Iteration:  10470      Current loss:  27.854686737060547      Current accuracy:  35.0      Comp_time of one batch:  -0.8679449558258057\n",
            "Iteration:  10475      Current loss:  22.401365280151367      Current accuracy:  36.0      Comp_time of one batch:  -0.867332935333252\n",
            "Iteration:  10480      Current loss:  25.97740936279297      Current accuracy:  36.0      Comp_time of one batch:  -0.8784215450286865\n",
            "Iteration:  10485      Current loss:  20.0501708984375      Current accuracy:  37.0      Comp_time of one batch:  -0.8672235012054443\n",
            "Iteration:  10490      Current loss:  29.40570640563965      Current accuracy:  37.0      Comp_time of one batch:  -0.8890895843505859\n",
            "Iteration:  10495      Current loss:  19.325851440429688      Current accuracy:  38.0      Comp_time of one batch:  -0.8771076202392578\n",
            "Iteration:  10500      Current loss:  19.3782958984375      Current accuracy:  0.0      Comp_time of one batch:  -0.881476640701294\n",
            "Iteration:  10505      Current loss:  24.24319076538086      Current accuracy:  0.0      Comp_time of one batch:  -0.891108512878418\n",
            "Iteration:  10510      Current loss:  19.95697021484375      Current accuracy:  1.0      Comp_time of one batch:  -0.8799448013305664\n",
            "Iteration:  10515      Current loss:  26.548419952392578      Current accuracy:  1.0      Comp_time of one batch:  -0.8799152374267578\n",
            "Iteration:  10520      Current loss:  26.15028190612793      Current accuracy:  1.0      Comp_time of one batch:  -0.8916411399841309\n",
            "Iteration:  10525      Current loss:  23.070133209228516      Current accuracy:  1.0      Comp_time of one batch:  -0.8830180168151855\n",
            "Iteration:  10530      Current loss:  17.459667205810547      Current accuracy:  2.0      Comp_time of one batch:  -0.9015047550201416\n",
            "Iteration:  10535      Current loss:  20.41298484802246      Current accuracy:  3.0      Comp_time of one batch:  -0.8934295177459717\n",
            "Iteration:  10540      Current loss:  23.49039649963379      Current accuracy:  3.0      Comp_time of one batch:  -0.8856196403503418\n",
            "Iteration:  10545      Current loss:  29.81637954711914      Current accuracy:  3.0      Comp_time of one batch:  -0.8900885581970215\n",
            "Iteration:  10550      Current loss:  19.23366928100586      Current accuracy:  4.0      Comp_time of one batch:  -0.880622386932373\n",
            "Iteration:  10555      Current loss:  30.76295280456543      Current accuracy:  4.0      Comp_time of one batch:  -0.8715317249298096\n",
            "Iteration:  10560      Current loss:  21.852596282958984      Current accuracy:  4.0      Comp_time of one batch:  -0.8784153461456299\n",
            "Iteration:  10565      Current loss:  23.650541305541992      Current accuracy:  4.0      Comp_time of one batch:  -0.8767271041870117\n",
            "Iteration:  10570      Current loss:  19.953474044799805      Current accuracy:  5.0      Comp_time of one batch:  -0.8657712936401367\n",
            "Iteration:  10575      Current loss:  17.893131256103516      Current accuracy:  6.0      Comp_time of one batch:  -0.8817954063415527\n",
            "Iteration:  10580      Current loss:  31.817846298217773      Current accuracy:  6.0      Comp_time of one batch:  -0.8855602741241455\n",
            "Iteration:  10585      Current loss:  19.21115493774414      Current accuracy:  7.0      Comp_time of one batch:  -0.8798348903656006\n",
            "Iteration:  10590      Current loss:  28.994428634643555      Current accuracy:  7.0      Comp_time of one batch:  -0.8957178592681885\n",
            "Iteration:  10595      Current loss:  18.554677963256836      Current accuracy:  8.0      Comp_time of one batch:  -0.8827028274536133\n",
            "Iteration:  10600      Current loss:  17.230928421020508      Current accuracy:  9.0      Comp_time of one batch:  -0.8851208686828613\n",
            "Iteration:  10605      Current loss:  23.318845748901367      Current accuracy:  9.0      Comp_time of one batch:  -0.8840265274047852\n",
            "Iteration:  10610      Current loss:  18.441370010375977      Current accuracy:  10.0      Comp_time of one batch:  -0.892047643661499\n",
            "Iteration:  10615      Current loss:  26.592180252075195      Current accuracy:  11.0      Comp_time of one batch:  -0.910858154296875\n",
            "Iteration:  10620      Current loss:  15.929780006408691      Current accuracy:  12.0      Comp_time of one batch:  -0.8852176666259766\n",
            "Iteration:  10625      Current loss:  24.076322555541992      Current accuracy:  12.0      Comp_time of one batch:  -0.9048385620117188\n",
            "Iteration:  10630      Current loss:  16.75552749633789      Current accuracy:  13.0      Comp_time of one batch:  -0.8823885917663574\n",
            "Iteration:  10635      Current loss:  34.15134048461914      Current accuracy:  13.0      Comp_time of one batch:  -0.8876206874847412\n",
            "Iteration:  10640      Current loss:  21.753454208374023      Current accuracy:  14.0      Comp_time of one batch:  -0.8836338520050049\n",
            "Iteration:  10645      Current loss:  30.915363311767578      Current accuracy:  14.0      Comp_time of one batch:  -0.8759779930114746\n",
            "Iteration:  10650      Current loss:  27.34934425354004      Current accuracy:  14.0      Comp_time of one batch:  -0.9043068885803223\n",
            "Iteration:  10655      Current loss:  37.38484573364258      Current accuracy:  14.0      Comp_time of one batch:  -0.8882153034210205\n",
            "Iteration:  10660      Current loss:  19.791494369506836      Current accuracy:  15.0      Comp_time of one batch:  -0.8862674236297607\n",
            "Iteration:  10665      Current loss:  28.402568817138672      Current accuracy:  15.0      Comp_time of one batch:  -0.8854284286499023\n",
            "Iteration:  10670      Current loss:  19.42851448059082      Current accuracy:  16.0      Comp_time of one batch:  -0.9180502891540527\n",
            "Iteration:  10675      Current loss:  25.137277603149414      Current accuracy:  16.0      Comp_time of one batch:  -0.8932280540466309\n",
            "Iteration:  10680      Current loss:  26.144020080566406      Current accuracy:  16.0      Comp_time of one batch:  -0.8791444301605225\n",
            "Iteration:  10685      Current loss:  25.04151725769043      Current accuracy:  16.0      Comp_time of one batch:  -0.8960905075073242\n",
            "Iteration:  10690      Current loss:  25.47562026977539      Current accuracy:  16.0      Comp_time of one batch:  -0.9043087959289551\n",
            "Iteration:  10695      Current loss:  28.396774291992188      Current accuracy:  16.0      Comp_time of one batch:  -0.8909399509429932\n",
            "Iteration:  10700      Current loss:  35.634315490722656      Current accuracy:  16.0      Comp_time of one batch:  -0.9042365550994873\n",
            "Iteration:  10705      Current loss:  32.408241271972656      Current accuracy:  16.0      Comp_time of one batch:  -0.9015593528747559\n",
            "Iteration:  10710      Current loss:  20.931989669799805      Current accuracy:  17.0      Comp_time of one batch:  -0.9070732593536377\n",
            "Iteration:  10715      Current loss:  28.513839721679688      Current accuracy:  17.0      Comp_time of one batch:  -0.9003164768218994\n",
            "Iteration:  10720      Current loss:  25.0910701751709      Current accuracy:  17.0      Comp_time of one batch:  -0.8813321590423584\n",
            "Iteration:  10725      Current loss:  22.09444808959961      Current accuracy:  18.0      Comp_time of one batch:  -0.8803868293762207\n",
            "Iteration:  10730      Current loss:  24.18961524963379      Current accuracy:  18.0      Comp_time of one batch:  -0.9042253494262695\n",
            "Iteration:  10735      Current loss:  26.557279586791992      Current accuracy:  18.0      Comp_time of one batch:  -0.8966548442840576\n",
            "Iteration:  10740      Current loss:  23.41950798034668      Current accuracy:  18.0      Comp_time of one batch:  -0.8828742504119873\n",
            "Iteration:  10745      Current loss:  25.947046279907227      Current accuracy:  18.0      Comp_time of one batch:  -0.8943557739257812\n",
            "Iteration:  10750      Current loss:  31.483997344970703      Current accuracy:  18.0      Comp_time of one batch:  -0.908714771270752\n",
            "Iteration:  10755      Current loss:  17.027128219604492      Current accuracy:  19.0      Comp_time of one batch:  -0.8911628723144531\n",
            "Iteration:  10760      Current loss:  18.991371154785156      Current accuracy:  20.0      Comp_time of one batch:  -0.8830595016479492\n",
            "Iteration:  10765      Current loss:  18.796945571899414      Current accuracy:  21.0      Comp_time of one batch:  -0.8793458938598633\n",
            "Iteration:  10770      Current loss:  22.745576858520508      Current accuracy:  21.0      Comp_time of one batch:  -0.8767104148864746\n",
            "Iteration:  10775      Current loss:  30.125511169433594      Current accuracy:  21.0      Comp_time of one batch:  -0.8809401988983154\n",
            "Iteration:  10780      Current loss:  20.125526428222656      Current accuracy:  22.0      Comp_time of one batch:  -0.8782386779785156\n",
            "Iteration:  10785      Current loss:  23.354177474975586      Current accuracy:  23.0      Comp_time of one batch:  -0.8840713500976562\n",
            "Iteration:  10790      Current loss:  18.96922492980957      Current accuracy:  24.0      Comp_time of one batch:  -0.8984279632568359\n",
            "Iteration:  10795      Current loss:  41.912715911865234      Current accuracy:  24.0      Comp_time of one batch:  -0.8916089534759521\n",
            "Iteration:  10800      Current loss:  18.110536575317383      Current accuracy:  25.0      Comp_time of one batch:  -0.8836057186126709\n",
            "Iteration:  10805      Current loss:  23.928020477294922      Current accuracy:  25.0      Comp_time of one batch:  -0.8779289722442627\n",
            "Iteration:  10810      Current loss:  18.09796142578125      Current accuracy:  26.0      Comp_time of one batch:  -0.8867943286895752\n",
            "Iteration:  10815      Current loss:  22.545869827270508      Current accuracy:  26.0      Comp_time of one batch:  -0.8749377727508545\n",
            "Iteration:  10820      Current loss:  21.827091217041016      Current accuracy:  27.0      Comp_time of one batch:  -0.8714523315429688\n",
            "Iteration:  10825      Current loss:  18.798015594482422      Current accuracy:  28.0      Comp_time of one batch:  -0.8878884315490723\n",
            "Iteration:  10830      Current loss:  23.23821258544922      Current accuracy:  28.0      Comp_time of one batch:  -0.888453483581543\n",
            "Iteration:  10835      Current loss:  20.187562942504883      Current accuracy:  29.0      Comp_time of one batch:  -0.8955516815185547\n",
            "Iteration:  10840      Current loss:  21.158658981323242      Current accuracy:  30.0      Comp_time of one batch:  -0.8802700042724609\n",
            "Iteration:  10845      Current loss:  26.072729110717773      Current accuracy:  30.0      Comp_time of one batch:  -0.89737868309021\n",
            "Iteration:  10850      Current loss:  20.263357162475586      Current accuracy:  31.0      Comp_time of one batch:  -0.8921399116516113\n",
            "Iteration:  10855      Current loss:  24.99454116821289      Current accuracy:  31.0      Comp_time of one batch:  -0.8752121925354004\n",
            "Iteration:  10860      Current loss:  18.988245010375977      Current accuracy:  32.0      Comp_time of one batch:  -0.9198920726776123\n",
            "Iteration:  10865      Current loss:  18.464765548706055      Current accuracy:  33.0      Comp_time of one batch:  -0.8804199695587158\n",
            "Iteration:  10870      Current loss:  25.24650764465332      Current accuracy:  34.0      Comp_time of one batch:  -0.8814148902893066\n",
            "Iteration:  10875      Current loss:  25.700956344604492      Current accuracy:  34.0      Comp_time of one batch:  -0.8747589588165283\n",
            "Iteration:  10880      Current loss:  16.06511878967285      Current accuracy:  35.0      Comp_time of one batch:  -0.8891961574554443\n",
            "Iteration:  10885      Current loss:  23.837560653686523      Current accuracy:  36.0      Comp_time of one batch:  -0.8795897960662842\n",
            "Iteration:  10890      Current loss:  20.156490325927734      Current accuracy:  37.0      Comp_time of one batch:  -0.8840560913085938\n",
            "Iteration:  10895      Current loss:  28.421730041503906      Current accuracy:  37.0      Comp_time of one batch:  -0.8808951377868652\n",
            "Iteration:  10900      Current loss:  25.706134796142578      Current accuracy:  37.0      Comp_time of one batch:  -0.9009823799133301\n",
            "Iteration:  10905      Current loss:  21.892139434814453      Current accuracy:  38.0      Comp_time of one batch:  -0.8991987705230713\n",
            "Iteration:  10910      Current loss:  25.507564544677734      Current accuracy:  38.0      Comp_time of one batch:  -0.8876662254333496\n",
            "Iteration:  10915      Current loss:  28.830595016479492      Current accuracy:  38.0      Comp_time of one batch:  -0.8941152095794678\n",
            "Iteration:  10920      Current loss:  26.419666290283203      Current accuracy:  38.0      Comp_time of one batch:  -0.8887016773223877\n",
            "Iteration:  10925      Current loss:  20.793928146362305      Current accuracy:  39.0      Comp_time of one batch:  -0.8881688117980957\n",
            "Iteration:  10930      Current loss:  19.896272659301758      Current accuracy:  40.0      Comp_time of one batch:  -0.9084205627441406\n",
            "Iteration:  10935      Current loss:  26.333450317382812      Current accuracy:  40.0      Comp_time of one batch:  -0.8669543266296387\n",
            "Iteration:  10940      Current loss:  34.29296112060547      Current accuracy:  40.0      Comp_time of one batch:  -0.8797042369842529\n",
            "Iteration:  10945      Current loss:  23.685806274414062      Current accuracy:  40.0      Comp_time of one batch:  -0.9037351608276367\n",
            "Iteration:  10950      Current loss:  20.70427703857422      Current accuracy:  41.0      Comp_time of one batch:  -0.8867852687835693\n",
            "Iteration:  10955      Current loss:  29.595678329467773      Current accuracy:  41.0      Comp_time of one batch:  -0.8780953884124756\n",
            "Iteration:  10960      Current loss:  23.292253494262695      Current accuracy:  41.0      Comp_time of one batch:  -0.8720319271087646\n",
            "Iteration:  10965      Current loss:  18.302509307861328      Current accuracy:  42.0      Comp_time of one batch:  -0.8943274021148682\n",
            "Iteration:  10970      Current loss:  24.240894317626953      Current accuracy:  42.0      Comp_time of one batch:  -0.8844521045684814\n",
            "Iteration:  10975      Current loss:  25.127958297729492      Current accuracy:  42.0      Comp_time of one batch:  -0.8802049160003662\n",
            "Iteration:  10980      Current loss:  23.068981170654297      Current accuracy:  42.0      Comp_time of one batch:  -0.9031710624694824\n",
            "Iteration:  10985      Current loss:  25.819971084594727      Current accuracy:  42.0      Comp_time of one batch:  -0.9014477729797363\n",
            "Iteration:  10990      Current loss:  30.803485870361328      Current accuracy:  42.0      Comp_time of one batch:  -0.8746788501739502\n",
            "Iteration:  10995      Current loss:  27.16266441345215      Current accuracy:  42.0      Comp_time of one batch:  -0.8959574699401855\n",
            "Iteration:  11000      Current loss:  22.016050338745117      Current accuracy:  0.0      Comp_time of one batch:  -0.8942720890045166\n",
            "Iteration:  11005      Current loss:  21.131938934326172      Current accuracy:  1.0      Comp_time of one batch:  -0.8686733245849609\n",
            "Iteration:  11010      Current loss:  23.972118377685547      Current accuracy:  1.0      Comp_time of one batch:  -0.8997993469238281\n",
            "Iteration:  11015      Current loss:  26.55971908569336      Current accuracy:  1.0      Comp_time of one batch:  -0.8946924209594727\n",
            "Iteration:  11020      Current loss:  17.62397003173828      Current accuracy:  2.0      Comp_time of one batch:  -0.8952503204345703\n",
            "Iteration:  11025      Current loss:  25.168455123901367      Current accuracy:  2.0      Comp_time of one batch:  -0.8733768463134766\n",
            "Iteration:  11030      Current loss:  21.781539916992188      Current accuracy:  3.0      Comp_time of one batch:  -0.8848063945770264\n",
            "Iteration:  11035      Current loss:  21.2626953125      Current accuracy:  4.0      Comp_time of one batch:  -0.8807101249694824\n",
            "Iteration:  11040      Current loss:  21.72277069091797      Current accuracy:  5.0      Comp_time of one batch:  -0.8953144550323486\n",
            "Iteration:  11045      Current loss:  36.00889205932617      Current accuracy:  5.0      Comp_time of one batch:  -0.8667991161346436\n",
            "Iteration:  11050      Current loss:  19.75712013244629      Current accuracy:  6.0      Comp_time of one batch:  -0.8741099834442139\n",
            "Iteration:  11055      Current loss:  22.52549934387207      Current accuracy:  7.0      Comp_time of one batch:  -0.8827159404754639\n",
            "Iteration:  11060      Current loss:  19.37779426574707      Current accuracy:  8.0      Comp_time of one batch:  -0.886812686920166\n",
            "Iteration:  11065      Current loss:  25.78515625      Current accuracy:  8.0      Comp_time of one batch:  -0.8783853054046631\n",
            "Iteration:  11070      Current loss:  22.183311462402344      Current accuracy:  9.0      Comp_time of one batch:  -0.920403242111206\n",
            "Iteration:  11075      Current loss:  24.198657989501953      Current accuracy:  9.0      Comp_time of one batch:  -0.9025170803070068\n",
            "Iteration:  11080      Current loss:  21.740596771240234      Current accuracy:  10.0      Comp_time of one batch:  -0.8953089714050293\n",
            "Iteration:  11085      Current loss:  19.754047393798828      Current accuracy:  11.0      Comp_time of one batch:  -0.8846447467803955\n",
            "Iteration:  11090      Current loss:  22.01293182373047      Current accuracy:  12.0      Comp_time of one batch:  -0.8896536827087402\n",
            "Iteration:  11095      Current loss:  21.829959869384766      Current accuracy:  13.0      Comp_time of one batch:  -0.8819339275360107\n",
            "Iteration:  11100      Current loss:  26.593984603881836      Current accuracy:  13.0      Comp_time of one batch:  -0.8772275447845459\n",
            "Iteration:  11105      Current loss:  27.066679000854492      Current accuracy:  13.0      Comp_time of one batch:  -0.8982200622558594\n",
            "Iteration:  11110      Current loss:  16.459142684936523      Current accuracy:  14.0      Comp_time of one batch:  -0.8822066783905029\n",
            "Iteration:  11115      Current loss:  20.32950210571289      Current accuracy:  15.0      Comp_time of one batch:  -0.9008772373199463\n",
            "Iteration:  11120      Current loss:  18.439247131347656      Current accuracy:  16.0      Comp_time of one batch:  -0.9032230377197266\n",
            "Iteration:  11125      Current loss:  27.205636978149414      Current accuracy:  16.0      Comp_time of one batch:  -0.893545389175415\n",
            "Iteration:  11130      Current loss:  19.616641998291016      Current accuracy:  17.0      Comp_time of one batch:  -0.888256311416626\n",
            "Iteration:  11135      Current loss:  22.560108184814453      Current accuracy:  17.0      Comp_time of one batch:  -0.886742353439331\n",
            "Iteration:  11140      Current loss:  23.24089813232422      Current accuracy:  18.0      Comp_time of one batch:  -0.8910417556762695\n",
            "Iteration:  11145      Current loss:  17.520687103271484      Current accuracy:  19.0      Comp_time of one batch:  -0.8979511260986328\n",
            "Iteration:  11150      Current loss:  23.556612014770508      Current accuracy:  19.0      Comp_time of one batch:  -0.9137537479400635\n",
            "Iteration:  11155      Current loss:  19.454307556152344      Current accuracy:  20.0      Comp_time of one batch:  -0.8883914947509766\n",
            "Iteration:  11160      Current loss:  32.01459503173828      Current accuracy:  20.0      Comp_time of one batch:  -0.9070408344268799\n",
            "Iteration:  11165      Current loss:  27.027597427368164      Current accuracy:  20.0      Comp_time of one batch:  -0.9169285297393799\n",
            "Iteration:  11170      Current loss:  30.115346908569336      Current accuracy:  20.0      Comp_time of one batch:  -0.9144985675811768\n",
            "Iteration:  11175      Current loss:  24.436687469482422      Current accuracy:  20.0      Comp_time of one batch:  -0.8752748966217041\n",
            "Iteration:  11180      Current loss:  31.97595977783203      Current accuracy:  20.0      Comp_time of one batch:  -0.9255564212799072\n",
            "Iteration:  11185      Current loss:  17.93589973449707      Current accuracy:  21.0      Comp_time of one batch:  -0.9282698631286621\n",
            "Iteration:  11190      Current loss:  22.74155044555664      Current accuracy:  22.0      Comp_time of one batch:  -0.8755927085876465\n",
            "Iteration:  11195      Current loss:  23.635557174682617      Current accuracy:  23.0      Comp_time of one batch:  -0.8798575401306152\n",
            "Iteration:  11200      Current loss:  18.093168258666992      Current accuracy:  24.0      Comp_time of one batch:  -0.8876125812530518\n",
            "Iteration:  11205      Current loss:  20.56928253173828      Current accuracy:  25.0      Comp_time of one batch:  -0.8911831378936768\n",
            "Iteration:  11210      Current loss:  27.384986877441406      Current accuracy:  25.0      Comp_time of one batch:  -0.8763165473937988\n",
            "Iteration:  11215      Current loss:  27.271821975708008      Current accuracy:  25.0      Comp_time of one batch:  -0.8905296325683594\n",
            "Iteration:  11220      Current loss:  24.101612091064453      Current accuracy:  25.0      Comp_time of one batch:  -0.8794069290161133\n",
            "Iteration:  11225      Current loss:  14.200540542602539      Current accuracy:  26.0      Comp_time of one batch:  -0.8778440952301025\n",
            "Iteration:  11230      Current loss:  30.393774032592773      Current accuracy:  26.0      Comp_time of one batch:  -0.8812010288238525\n",
            "Iteration:  11235      Current loss:  18.57413101196289      Current accuracy:  27.0      Comp_time of one batch:  -0.8798441886901855\n",
            "Iteration:  11240      Current loss:  34.89486312866211      Current accuracy:  27.0      Comp_time of one batch:  -0.8814277648925781\n",
            "Iteration:  11245      Current loss:  25.724660873413086      Current accuracy:  27.0      Comp_time of one batch:  -0.8672363758087158\n",
            "Iteration:  11250      Current loss:  18.035140991210938      Current accuracy:  28.0      Comp_time of one batch:  -0.895226240158081\n",
            "Iteration:  11255      Current loss:  23.182960510253906      Current accuracy:  29.0      Comp_time of one batch:  -0.8762879371643066\n",
            "Iteration:  11260      Current loss:  22.186933517456055      Current accuracy:  30.0      Comp_time of one batch:  -0.8812620639801025\n",
            "Iteration:  11265      Current loss:  21.955045700073242      Current accuracy:  31.0      Comp_time of one batch:  -0.9056901931762695\n",
            "Iteration:  11270      Current loss:  34.3067741394043      Current accuracy:  31.0      Comp_time of one batch:  -0.910944938659668\n",
            "Iteration:  11275      Current loss:  24.714309692382812      Current accuracy:  31.0      Comp_time of one batch:  -0.8937649726867676\n",
            "Iteration:  11280      Current loss:  17.744335174560547      Current accuracy:  32.0      Comp_time of one batch:  -0.8794968128204346\n",
            "Iteration:  11285      Current loss:  23.028942108154297      Current accuracy:  33.0      Comp_time of one batch:  -0.9108929634094238\n",
            "Iteration:  11290      Current loss:  26.554452896118164      Current accuracy:  33.0      Comp_time of one batch:  -0.8781678676605225\n",
            "Iteration:  11295      Current loss:  21.06904411315918      Current accuracy:  34.0      Comp_time of one batch:  -0.8810062408447266\n",
            "Iteration:  11300      Current loss:  18.68199348449707      Current accuracy:  35.0      Comp_time of one batch:  -0.8922364711761475\n",
            "Iteration:  11305      Current loss:  16.9414005279541      Current accuracy:  36.0      Comp_time of one batch:  -0.8968167304992676\n",
            "Iteration:  11310      Current loss:  21.319618225097656      Current accuracy:  37.0      Comp_time of one batch:  -0.8784782886505127\n",
            "Iteration:  11315      Current loss:  17.838640213012695      Current accuracy:  38.0      Comp_time of one batch:  -0.8791868686676025\n",
            "Iteration:  11320      Current loss:  22.513858795166016      Current accuracy:  38.0      Comp_time of one batch:  -0.9117624759674072\n",
            "Iteration:  11325      Current loss:  16.299367904663086      Current accuracy:  39.0      Comp_time of one batch:  -0.8976597785949707\n",
            "Iteration:  11330      Current loss:  27.617521286010742      Current accuracy:  39.0      Comp_time of one batch:  -0.8923909664154053\n",
            "Iteration:  11335      Current loss:  26.376556396484375      Current accuracy:  39.0      Comp_time of one batch:  -0.8884143829345703\n",
            "Iteration:  11340      Current loss:  19.790311813354492      Current accuracy:  40.0      Comp_time of one batch:  -0.8868165016174316\n",
            "Iteration:  11345      Current loss:  20.92942237854004      Current accuracy:  41.0      Comp_time of one batch:  -0.8890149593353271\n",
            "Iteration:  11350      Current loss:  21.32183074951172      Current accuracy:  42.0      Comp_time of one batch:  -0.8859047889709473\n",
            "Iteration:  11355      Current loss:  28.983434677124023      Current accuracy:  43.0      Comp_time of one batch:  -0.8861923217773438\n",
            "Iteration:  11360      Current loss:  35.31222915649414      Current accuracy:  43.0      Comp_time of one batch:  -0.8855767250061035\n",
            "Iteration:  11365      Current loss:  26.56513023376465      Current accuracy:  43.0      Comp_time of one batch:  -0.9088006019592285\n",
            "Iteration:  11370      Current loss:  20.869169235229492      Current accuracy:  44.0      Comp_time of one batch:  -0.8847579956054688\n",
            "Iteration:  11375      Current loss:  29.299020767211914      Current accuracy:  44.0      Comp_time of one batch:  -0.9149594306945801\n",
            "Iteration:  11380      Current loss:  26.546730041503906      Current accuracy:  44.0      Comp_time of one batch:  -0.8838186264038086\n",
            "Iteration:  11385      Current loss:  22.559757232666016      Current accuracy:  44.0      Comp_time of one batch:  -0.8698990345001221\n",
            "Iteration:  11390      Current loss:  26.16080093383789      Current accuracy:  44.0      Comp_time of one batch:  -0.8766019344329834\n",
            "Iteration:  11395      Current loss:  29.195608139038086      Current accuracy:  44.0      Comp_time of one batch:  -0.8933968544006348\n",
            "Iteration:  11400      Current loss:  21.756271362304688      Current accuracy:  45.0      Comp_time of one batch:  -0.8817694187164307\n",
            "Iteration:  11405      Current loss:  24.760875701904297      Current accuracy:  45.0      Comp_time of one batch:  -0.8910095691680908\n",
            "Iteration:  11410      Current loss:  20.91261863708496      Current accuracy:  46.0      Comp_time of one batch:  -0.9049499034881592\n",
            "Iteration:  11415      Current loss:  28.539363861083984      Current accuracy:  46.0      Comp_time of one batch:  -0.8941888809204102\n",
            "Iteration:  11420      Current loss:  16.19535255432129      Current accuracy:  47.0      Comp_time of one batch:  -0.8936409950256348\n",
            "Iteration:  11425      Current loss:  19.377031326293945      Current accuracy:  48.0      Comp_time of one batch:  -0.9024665355682373\n",
            "Iteration:  11430      Current loss:  34.83097457885742      Current accuracy:  48.0      Comp_time of one batch:  -0.8873755931854248\n",
            "Iteration:  11435      Current loss:  21.32677459716797      Current accuracy:  49.0      Comp_time of one batch:  -0.8908870220184326\n",
            "Iteration:  11440      Current loss:  25.343395233154297      Current accuracy:  49.0      Comp_time of one batch:  -0.8819320201873779\n",
            "Iteration:  11445      Current loss:  34.07277297973633      Current accuracy:  49.0      Comp_time of one batch:  -0.8864231109619141\n",
            "Iteration:  11450      Current loss:  22.02342414855957      Current accuracy:  50.0      Comp_time of one batch:  -0.8945906162261963\n",
            "Iteration:  11455      Current loss:  22.266324996948242      Current accuracy:  51.0      Comp_time of one batch:  -0.9014954566955566\n",
            "Iteration:  11460      Current loss:  17.774394989013672      Current accuracy:  52.0      Comp_time of one batch:  -0.8834307193756104\n",
            "Iteration:  11465      Current loss:  24.94099235534668      Current accuracy:  52.0      Comp_time of one batch:  -0.8770008087158203\n",
            "Iteration:  11470      Current loss:  24.986886978149414      Current accuracy:  52.0      Comp_time of one batch:  -0.8875796794891357\n",
            "Iteration:  11475      Current loss:  16.826478958129883      Current accuracy:  53.0      Comp_time of one batch:  -0.8961646556854248\n",
            "Iteration:  11480      Current loss:  23.000032424926758      Current accuracy:  54.0      Comp_time of one batch:  -0.9140570163726807\n",
            "Iteration:  11485      Current loss:  19.844348907470703      Current accuracy:  55.0      Comp_time of one batch:  -0.9030971527099609\n",
            "Iteration:  11490      Current loss:  25.069860458374023      Current accuracy:  55.0      Comp_time of one batch:  -0.8899548053741455\n",
            "Iteration:  11495      Current loss:  24.542831420898438      Current accuracy:  56.0      Comp_time of one batch:  -0.8873038291931152\n",
            "Iteration:  11500      Current loss:  28.212804794311523      Current accuracy:  0.0      Comp_time of one batch:  -0.8705999851226807\n",
            "Iteration:  11505      Current loss:  15.104706764221191      Current accuracy:  1.0      Comp_time of one batch:  -0.8959274291992188\n",
            "Iteration:  11510      Current loss:  19.24582862854004      Current accuracy:  2.0      Comp_time of one batch:  -0.8888978958129883\n",
            "Iteration:  11515      Current loss:  21.566184997558594      Current accuracy:  3.0      Comp_time of one batch:  -0.9051058292388916\n",
            "Iteration:  11520      Current loss:  32.161964416503906      Current accuracy:  3.0      Comp_time of one batch:  -0.8769948482513428\n",
            "Iteration:  11525      Current loss:  18.231613159179688      Current accuracy:  4.0      Comp_time of one batch:  -0.9030241966247559\n",
            "Iteration:  11530      Current loss:  30.66263771057129      Current accuracy:  4.0      Comp_time of one batch:  -0.9027092456817627\n",
            "Iteration:  11535      Current loss:  31.96468734741211      Current accuracy:  4.0      Comp_time of one batch:  -0.8909802436828613\n",
            "Iteration:  11540      Current loss:  16.777450561523438      Current accuracy:  5.0      Comp_time of one batch:  -0.899177074432373\n",
            "Iteration:  11545      Current loss:  30.927766799926758      Current accuracy:  5.0      Comp_time of one batch:  -0.8829030990600586\n",
            "Iteration:  11550      Current loss:  17.8189640045166      Current accuracy:  6.0      Comp_time of one batch:  -0.8981497287750244\n",
            "Iteration:  11555      Current loss:  29.662511825561523      Current accuracy:  6.0      Comp_time of one batch:  -0.8822262287139893\n",
            "Iteration:  11560      Current loss:  20.62665367126465      Current accuracy:  7.0      Comp_time of one batch:  -0.9303433895111084\n",
            "Iteration:  11565      Current loss:  41.00185775756836      Current accuracy:  7.0      Comp_time of one batch:  -0.9135713577270508\n",
            "Iteration:  11570      Current loss:  19.437206268310547      Current accuracy:  8.0      Comp_time of one batch:  -0.9012470245361328\n",
            "Iteration:  11575      Current loss:  38.25291061401367      Current accuracy:  8.0      Comp_time of one batch:  -0.905144214630127\n",
            "Iteration:  11580      Current loss:  24.001440048217773      Current accuracy:  8.0      Comp_time of one batch:  -0.8873374462127686\n",
            "Iteration:  11585      Current loss:  25.618812561035156      Current accuracy:  8.0      Comp_time of one batch:  -0.8845605850219727\n",
            "Iteration:  11590      Current loss:  23.39065170288086      Current accuracy:  9.0      Comp_time of one batch:  -0.8916237354278564\n",
            "Iteration:  11595      Current loss:  20.509639739990234      Current accuracy:  10.0      Comp_time of one batch:  -0.9175209999084473\n",
            "Iteration:  11600      Current loss:  24.657207489013672      Current accuracy:  10.0      Comp_time of one batch:  -0.8847029209136963\n",
            "Iteration:  11605      Current loss:  31.438915252685547      Current accuracy:  10.0      Comp_time of one batch:  -0.9123907089233398\n",
            "Iteration:  11610      Current loss:  28.986713409423828      Current accuracy:  10.0      Comp_time of one batch:  -0.9046363830566406\n",
            "Iteration:  11615      Current loss:  34.03387451171875      Current accuracy:  10.0      Comp_time of one batch:  -0.8912272453308105\n",
            "Iteration:  11620      Current loss:  23.761573791503906      Current accuracy:  10.0      Comp_time of one batch:  -0.9298191070556641\n",
            "Iteration:  11625      Current loss:  32.69700241088867      Current accuracy:  10.0      Comp_time of one batch:  -0.9024927616119385\n",
            "Iteration:  11630      Current loss:  24.929529190063477      Current accuracy:  11.0      Comp_time of one batch:  -0.9016623497009277\n",
            "Iteration:  11635      Current loss:  26.145418167114258      Current accuracy:  11.0      Comp_time of one batch:  -0.8992276191711426\n",
            "Iteration:  11640      Current loss:  22.577194213867188      Current accuracy:  12.0      Comp_time of one batch:  -0.9027833938598633\n",
            "Iteration:  11645      Current loss:  21.21066665649414      Current accuracy:  13.0      Comp_time of one batch:  -0.8987348079681396\n",
            "Iteration:  11650      Current loss:  31.1766357421875      Current accuracy:  13.0      Comp_time of one batch:  -0.9164080619812012\n",
            "Iteration:  11655      Current loss:  20.767805099487305      Current accuracy:  14.0      Comp_time of one batch:  -0.8968369960784912\n",
            "Iteration:  11660      Current loss:  23.750980377197266      Current accuracy:  15.0      Comp_time of one batch:  -0.8962385654449463\n",
            "Iteration:  11665      Current loss:  15.927610397338867      Current accuracy:  16.0      Comp_time of one batch:  -0.9067521095275879\n",
            "Iteration:  11670      Current loss:  19.02630043029785      Current accuracy:  17.0      Comp_time of one batch:  -0.9077224731445312\n",
            "Iteration:  11675      Current loss:  22.084861755371094      Current accuracy:  18.0      Comp_time of one batch:  -0.8915848731994629\n",
            "Iteration:  11680      Current loss:  24.759689331054688      Current accuracy:  19.0      Comp_time of one batch:  -0.8826212882995605\n",
            "Iteration:  11685      Current loss:  21.2439022064209      Current accuracy:  20.0      Comp_time of one batch:  -0.8934154510498047\n",
            "Iteration:  11690      Current loss:  32.30342102050781      Current accuracy:  20.0      Comp_time of one batch:  -0.8807554244995117\n",
            "Iteration:  11695      Current loss:  32.731529235839844      Current accuracy:  20.0      Comp_time of one batch:  -0.8866591453552246\n",
            "Iteration:  11700      Current loss:  21.709442138671875      Current accuracy:  20.0      Comp_time of one batch:  -0.8746287822723389\n",
            "Iteration:  11705      Current loss:  20.007978439331055      Current accuracy:  21.0      Comp_time of one batch:  -0.8775942325592041\n",
            "Iteration:  11710      Current loss:  33.10307312011719      Current accuracy:  21.0      Comp_time of one batch:  -0.8859241008758545\n",
            "Iteration:  11715      Current loss:  17.476825714111328      Current accuracy:  22.0      Comp_time of one batch:  -0.9068236351013184\n",
            "Iteration:  11720      Current loss:  22.671720504760742      Current accuracy:  23.0      Comp_time of one batch:  -0.8968935012817383\n",
            "Iteration:  11725      Current loss:  39.62375259399414      Current accuracy:  23.0      Comp_time of one batch:  -0.8956429958343506\n",
            "Iteration:  11730      Current loss:  23.82314682006836      Current accuracy:  23.0      Comp_time of one batch:  -0.9064216613769531\n",
            "Iteration:  11735      Current loss:  29.981897354125977      Current accuracy:  23.0      Comp_time of one batch:  -0.89341139793396\n",
            "Iteration:  11740      Current loss:  24.623943328857422      Current accuracy:  24.0      Comp_time of one batch:  -0.8756091594696045\n",
            "Iteration:  11745      Current loss:  28.246076583862305      Current accuracy:  24.0      Comp_time of one batch:  -0.8806464672088623\n",
            "Iteration:  11750      Current loss:  26.315372467041016      Current accuracy:  24.0      Comp_time of one batch:  -0.8924281597137451\n",
            "Iteration:  11755      Current loss:  21.98215675354004      Current accuracy:  25.0      Comp_time of one batch:  -0.8948993682861328\n",
            "Iteration:  11760      Current loss:  25.481063842773438      Current accuracy:  25.0      Comp_time of one batch:  -0.8883135318756104\n",
            "Iteration:  11765      Current loss:  31.655460357666016      Current accuracy:  25.0      Comp_time of one batch:  -0.8919062614440918\n",
            "Iteration:  11770      Current loss:  24.663318634033203      Current accuracy:  25.0      Comp_time of one batch:  -0.8755521774291992\n",
            "Iteration:  11775      Current loss:  19.201210021972656      Current accuracy:  26.0      Comp_time of one batch:  -0.8945271968841553\n",
            "Iteration:  11780      Current loss:  22.42522621154785      Current accuracy:  27.0      Comp_time of one batch:  -0.8741614818572998\n",
            "Iteration:  11785      Current loss:  31.507450103759766      Current accuracy:  27.0      Comp_time of one batch:  -0.8854148387908936\n",
            "Iteration:  11790      Current loss:  24.335538864135742      Current accuracy:  27.0      Comp_time of one batch:  -0.887444019317627\n",
            "Iteration:  11795      Current loss:  26.13615608215332      Current accuracy:  27.0      Comp_time of one batch:  -0.8809771537780762\n",
            "Iteration:  11800      Current loss:  30.190793991088867      Current accuracy:  27.0      Comp_time of one batch:  -0.8722763061523438\n",
            "Iteration:  11805      Current loss:  24.306499481201172      Current accuracy:  27.0      Comp_time of one batch:  -0.8955292701721191\n",
            "Iteration:  11810      Current loss:  26.99942398071289      Current accuracy:  27.0      Comp_time of one batch:  -0.9031722545623779\n",
            "Iteration:  11815      Current loss:  21.959253311157227      Current accuracy:  28.0      Comp_time of one batch:  -0.8739044666290283\n",
            "Iteration:  11820      Current loss:  29.964982986450195      Current accuracy:  28.0      Comp_time of one batch:  -0.8929333686828613\n",
            "Iteration:  11825      Current loss:  19.13315200805664      Current accuracy:  29.0      Comp_time of one batch:  -0.8907256126403809\n",
            "Iteration:  11830      Current loss:  22.85317039489746      Current accuracy:  30.0      Comp_time of one batch:  -0.8714327812194824\n",
            "Iteration:  11835      Current loss:  31.409082412719727      Current accuracy:  30.0      Comp_time of one batch:  -0.8869774341583252\n",
            "Iteration:  11840      Current loss:  24.407068252563477      Current accuracy:  30.0      Comp_time of one batch:  -0.9149990081787109\n",
            "Iteration:  11845      Current loss:  32.231300354003906      Current accuracy:  30.0      Comp_time of one batch:  -0.8986632823944092\n",
            "Iteration:  11850      Current loss:  20.69085121154785      Current accuracy:  31.0      Comp_time of one batch:  -0.9035165309906006\n",
            "Iteration:  11855      Current loss:  27.222148895263672      Current accuracy:  31.0      Comp_time of one batch:  -0.8838326930999756\n",
            "Iteration:  11860      Current loss:  19.42181968688965      Current accuracy:  32.0      Comp_time of one batch:  -0.9079556465148926\n",
            "Iteration:  11865      Current loss:  27.396800994873047      Current accuracy:  32.0      Comp_time of one batch:  -0.8813815116882324\n",
            "Iteration:  11870      Current loss:  28.18712043762207      Current accuracy:  32.0      Comp_time of one batch:  -0.906301736831665\n",
            "Iteration:  11875      Current loss:  31.111406326293945      Current accuracy:  32.0      Comp_time of one batch:  -0.8943474292755127\n",
            "Iteration:  11880      Current loss:  25.191516876220703      Current accuracy:  32.0      Comp_time of one batch:  -0.8908991813659668\n",
            "Iteration:  11885      Current loss:  21.731584548950195      Current accuracy:  33.0      Comp_time of one batch:  -0.8870799541473389\n",
            "Iteration:  11890      Current loss:  23.619983673095703      Current accuracy:  33.0      Comp_time of one batch:  -0.8882148265838623\n",
            "Iteration:  11895      Current loss:  27.68434715270996      Current accuracy:  33.0      Comp_time of one batch:  -0.902766227722168\n",
            "Iteration:  11900      Current loss:  27.50931167602539      Current accuracy:  33.0      Comp_time of one batch:  -0.8812234401702881\n",
            "Iteration:  11905      Current loss:  26.849811553955078      Current accuracy:  33.0      Comp_time of one batch:  -0.885540246963501\n",
            "Iteration:  11910      Current loss:  22.049428939819336      Current accuracy:  34.0      Comp_time of one batch:  -0.8779113292694092\n",
            "Iteration:  11915      Current loss:  21.7429141998291      Current accuracy:  35.0      Comp_time of one batch:  -0.8816156387329102\n",
            "Iteration:  11920      Current loss:  23.48423194885254      Current accuracy:  35.0      Comp_time of one batch:  -0.8919193744659424\n",
            "Iteration:  11925      Current loss:  19.79134178161621      Current accuracy:  36.0      Comp_time of one batch:  -0.8868839740753174\n",
            "Iteration:  11930      Current loss:  30.3359317779541      Current accuracy:  36.0      Comp_time of one batch:  -0.8759427070617676\n",
            "Iteration:  11935      Current loss:  18.9427433013916      Current accuracy:  37.0      Comp_time of one batch:  -0.8872513771057129\n",
            "Iteration:  11940      Current loss:  31.127676010131836      Current accuracy:  37.0      Comp_time of one batch:  -0.8804214000701904\n",
            "Iteration:  11945      Current loss:  22.14434242248535      Current accuracy:  38.0      Comp_time of one batch:  -0.8911867141723633\n",
            "Iteration:  11950      Current loss:  24.03714370727539      Current accuracy:  39.0      Comp_time of one batch:  -0.8955960273742676\n",
            "Iteration:  11955      Current loss:  25.05387306213379      Current accuracy:  40.0      Comp_time of one batch:  -0.89613938331604\n",
            "Iteration:  11960      Current loss:  19.779436111450195      Current accuracy:  41.0      Comp_time of one batch:  -0.9118521213531494\n",
            "Iteration:  11965      Current loss:  27.668506622314453      Current accuracy:  41.0      Comp_time of one batch:  -0.9137017726898193\n",
            "Iteration:  11970      Current loss:  19.822595596313477      Current accuracy:  42.0      Comp_time of one batch:  -0.8859958648681641\n",
            "Iteration:  11975      Current loss:  19.964208602905273      Current accuracy:  43.0      Comp_time of one batch:  -0.9028315544128418\n",
            "Iteration:  11980      Current loss:  19.158126831054688      Current accuracy:  44.0      Comp_time of one batch:  -0.8763365745544434\n",
            "Iteration:  11985      Current loss:  33.73624801635742      Current accuracy:  44.0      Comp_time of one batch:  -0.8873858451843262\n",
            "Iteration:  11990      Current loss:  22.714391708374023      Current accuracy:  45.0      Comp_time of one batch:  -0.8769063949584961\n",
            "Iteration:  11995      Current loss:  20.529340744018555      Current accuracy:  46.0      Comp_time of one batch:  -0.879951000213623\n",
            "Iteration:  12000      Current loss:  22.692766189575195      Current accuracy:  0.0      Comp_time of one batch:  -0.8768129348754883\n",
            "Iteration:  12005      Current loss:  23.141260147094727      Current accuracy:  1.0      Comp_time of one batch:  -0.9047603607177734\n",
            "Iteration:  12010      Current loss:  25.884971618652344      Current accuracy:  1.0      Comp_time of one batch:  -0.882946252822876\n",
            "Iteration:  12015      Current loss:  20.39839744567871      Current accuracy:  2.0      Comp_time of one batch:  -0.8843185901641846\n",
            "Iteration:  12020      Current loss:  32.34725570678711      Current accuracy:  2.0      Comp_time of one batch:  -0.8932929039001465\n",
            "Iteration:  12025      Current loss:  21.595439910888672      Current accuracy:  3.0      Comp_time of one batch:  -0.9006140232086182\n",
            "Iteration:  12030      Current loss:  27.846080780029297      Current accuracy:  3.0      Comp_time of one batch:  -0.9031705856323242\n",
            "Iteration:  12035      Current loss:  24.098876953125      Current accuracy:  3.0      Comp_time of one batch:  -0.876481294631958\n",
            "Iteration:  12040      Current loss:  24.346923828125      Current accuracy:  3.0      Comp_time of one batch:  -0.9021923542022705\n",
            "Iteration:  12045      Current loss:  20.505746841430664      Current accuracy:  4.0      Comp_time of one batch:  -0.8888716697692871\n",
            "Iteration:  12050      Current loss:  19.85538673400879      Current accuracy:  5.0      Comp_time of one batch:  -0.8972396850585938\n",
            "Iteration:  12055      Current loss:  27.5399227142334      Current accuracy:  5.0      Comp_time of one batch:  -0.9072892665863037\n",
            "Iteration:  12060      Current loss:  34.402347564697266      Current accuracy:  5.0      Comp_time of one batch:  -0.9052009582519531\n",
            "Iteration:  12065      Current loss:  20.182510375976562      Current accuracy:  6.0      Comp_time of one batch:  -0.8848025798797607\n",
            "Iteration:  12070      Current loss:  26.187759399414062      Current accuracy:  6.0      Comp_time of one batch:  -0.892296314239502\n",
            "Iteration:  12075      Current loss:  19.59333038330078      Current accuracy:  7.0      Comp_time of one batch:  -0.8944816589355469\n",
            "Iteration:  12080      Current loss:  21.81192398071289      Current accuracy:  8.0      Comp_time of one batch:  -0.8833339214324951\n",
            "Iteration:  12085      Current loss:  24.111385345458984      Current accuracy:  8.0      Comp_time of one batch:  -0.8945107460021973\n",
            "Iteration:  12090      Current loss:  18.903932571411133      Current accuracy:  9.0      Comp_time of one batch:  -0.9011991024017334\n",
            "Iteration:  12095      Current loss:  26.251558303833008      Current accuracy:  9.0      Comp_time of one batch:  -0.9069929122924805\n",
            "Iteration:  12100      Current loss:  17.56171989440918      Current accuracy:  10.0      Comp_time of one batch:  -0.8919808864593506\n",
            "Iteration:  12105      Current loss:  25.4161376953125      Current accuracy:  10.0      Comp_time of one batch:  -0.9000892639160156\n",
            "Iteration:  12110      Current loss:  24.65944480895996      Current accuracy:  10.0      Comp_time of one batch:  -0.9008405208587646\n",
            "Iteration:  12115      Current loss:  22.30596160888672      Current accuracy:  11.0      Comp_time of one batch:  -0.8704538345336914\n",
            "Iteration:  12120      Current loss:  20.61513900756836      Current accuracy:  12.0      Comp_time of one batch:  -0.8886592388153076\n",
            "Iteration:  12125      Current loss:  18.547321319580078      Current accuracy:  13.0      Comp_time of one batch:  -0.8778433799743652\n",
            "Iteration:  12130      Current loss:  31.57642936706543      Current accuracy:  13.0      Comp_time of one batch:  -0.9163172245025635\n",
            "Iteration:  12135      Current loss:  17.555702209472656      Current accuracy:  14.0      Comp_time of one batch:  -0.8978621959686279\n",
            "Iteration:  12140      Current loss:  22.581329345703125      Current accuracy:  15.0      Comp_time of one batch:  -0.8869476318359375\n",
            "Iteration:  12145      Current loss:  22.94668197631836      Current accuracy:  15.0      Comp_time of one batch:  -0.8846654891967773\n",
            "Iteration:  12150      Current loss:  19.16581916809082      Current accuracy:  16.0      Comp_time of one batch:  -0.9079587459564209\n",
            "Iteration:  12155      Current loss:  21.419849395751953      Current accuracy:  17.0      Comp_time of one batch:  -0.8914635181427002\n",
            "Iteration:  12160      Current loss:  27.11902618408203      Current accuracy:  17.0      Comp_time of one batch:  -0.8773539066314697\n",
            "Iteration:  12165      Current loss:  16.567726135253906      Current accuracy:  18.0      Comp_time of one batch:  -0.8793308734893799\n",
            "Iteration:  12170      Current loss:  19.89995002746582      Current accuracy:  19.0      Comp_time of one batch:  -0.934424638748169\n",
            "Iteration:  12175      Current loss:  16.68793487548828      Current accuracy:  20.0      Comp_time of one batch:  -0.9011993408203125\n",
            "Iteration:  12180      Current loss:  18.69681739807129      Current accuracy:  21.0      Comp_time of one batch:  -0.8877027034759521\n",
            "Iteration:  12185      Current loss:  21.23276138305664      Current accuracy:  22.0      Comp_time of one batch:  -0.9005472660064697\n",
            "Iteration:  12190      Current loss:  32.899879455566406      Current accuracy:  22.0      Comp_time of one batch:  -0.8810052871704102\n",
            "Iteration:  12195      Current loss:  23.587730407714844      Current accuracy:  22.0      Comp_time of one batch:  -0.8755707740783691\n",
            "Iteration:  12200      Current loss:  41.48509979248047      Current accuracy:  22.0      Comp_time of one batch:  -0.8766937255859375\n",
            "Iteration:  12205      Current loss:  25.017780303955078      Current accuracy:  23.0      Comp_time of one batch:  -0.9037594795227051\n",
            "Iteration:  12210      Current loss:  34.7137451171875      Current accuracy:  23.0      Comp_time of one batch:  -0.9022696018218994\n",
            "Iteration:  12215      Current loss:  28.239978790283203      Current accuracy:  23.0      Comp_time of one batch:  -0.8875765800476074\n",
            "Iteration:  12220      Current loss:  38.335487365722656      Current accuracy:  23.0      Comp_time of one batch:  -0.903205394744873\n",
            "Iteration:  12225      Current loss:  18.32029151916504      Current accuracy:  24.0      Comp_time of one batch:  -0.8932557106018066\n",
            "Iteration:  12230      Current loss:  29.48150062561035      Current accuracy:  24.0      Comp_time of one batch:  -0.9148895740509033\n",
            "Iteration:  12235      Current loss:  23.43718719482422      Current accuracy:  24.0      Comp_time of one batch:  -0.8952093124389648\n",
            "Iteration:  12240      Current loss:  19.94749641418457      Current accuracy:  25.0      Comp_time of one batch:  -0.8875560760498047\n",
            "Iteration:  12245      Current loss:  25.56338882446289      Current accuracy:  26.0      Comp_time of one batch:  -0.9173660278320312\n",
            "Iteration:  12250      Current loss:  18.887805938720703      Current accuracy:  27.0      Comp_time of one batch:  -0.897144079208374\n",
            "Iteration:  12255      Current loss:  16.099531173706055      Current accuracy:  28.0      Comp_time of one batch:  -0.8814013004302979\n",
            "Iteration:  12260      Current loss:  26.395370483398438      Current accuracy:  28.0      Comp_time of one batch:  -0.8998551368713379\n",
            "Iteration:  12265      Current loss:  21.330129623413086      Current accuracy:  29.0      Comp_time of one batch:  -0.8785452842712402\n",
            "Iteration:  12270      Current loss:  21.78537368774414      Current accuracy:  30.0      Comp_time of one batch:  -0.8819341659545898\n",
            "Iteration:  12275      Current loss:  17.374431610107422      Current accuracy:  31.0      Comp_time of one batch:  -0.9159657955169678\n",
            "Iteration:  12280      Current loss:  24.585102081298828      Current accuracy:  32.0      Comp_time of one batch:  -0.8782863616943359\n",
            "Iteration:  12285      Current loss:  17.942886352539062      Current accuracy:  33.0      Comp_time of one batch:  -0.8815317153930664\n",
            "Iteration:  12290      Current loss:  20.93987464904785      Current accuracy:  34.0      Comp_time of one batch:  -0.9065747261047363\n",
            "Iteration:  12295      Current loss:  29.727140426635742      Current accuracy:  34.0      Comp_time of one batch:  -0.8909492492675781\n",
            "Iteration:  12300      Current loss:  24.653276443481445      Current accuracy:  34.0      Comp_time of one batch:  -0.8856244087219238\n",
            "Iteration:  12305      Current loss:  27.712753295898438      Current accuracy:  34.0      Comp_time of one batch:  -0.8927755355834961\n",
            "Iteration:  12310      Current loss:  26.645660400390625      Current accuracy:  34.0      Comp_time of one batch:  -0.8800704479217529\n",
            "Iteration:  12315      Current loss:  23.16633415222168      Current accuracy:  34.0      Comp_time of one batch:  -0.8812735080718994\n",
            "Iteration:  12320      Current loss:  21.177345275878906      Current accuracy:  35.0      Comp_time of one batch:  -0.8777637481689453\n",
            "Iteration:  12325      Current loss:  22.07893943786621      Current accuracy:  36.0      Comp_time of one batch:  -0.8946905136108398\n",
            "Iteration:  12330      Current loss:  25.235618591308594      Current accuracy:  36.0      Comp_time of one batch:  -0.8865456581115723\n",
            "Iteration:  12335      Current loss:  23.836151123046875      Current accuracy:  36.0      Comp_time of one batch:  -0.8812272548675537\n",
            "Iteration:  12340      Current loss:  20.286561965942383      Current accuracy:  37.0      Comp_time of one batch:  -0.9081308841705322\n",
            "Iteration:  12345      Current loss:  29.93236541748047      Current accuracy:  37.0      Comp_time of one batch:  -0.8863976001739502\n",
            "Iteration:  12350      Current loss:  18.975576400756836      Current accuracy:  38.0      Comp_time of one batch:  -0.9135706424713135\n",
            "Iteration:  12355      Current loss:  35.16197204589844      Current accuracy:  38.0      Comp_time of one batch:  -0.8934612274169922\n",
            "Iteration:  12360      Current loss:  17.60560417175293      Current accuracy:  39.0      Comp_time of one batch:  -0.905308723449707\n",
            "Iteration:  12365      Current loss:  24.9183349609375      Current accuracy:  39.0      Comp_time of one batch:  -0.9023828506469727\n",
            "Iteration:  12370      Current loss:  20.765478134155273      Current accuracy:  40.0      Comp_time of one batch:  -0.8838107585906982\n",
            "Iteration:  12375      Current loss:  24.77667236328125      Current accuracy:  40.0      Comp_time of one batch:  -0.9016578197479248\n",
            "Iteration:  12380      Current loss:  21.651288986206055      Current accuracy:  41.0      Comp_time of one batch:  -0.886289119720459\n",
            "Iteration:  12385      Current loss:  30.441740036010742      Current accuracy:  41.0      Comp_time of one batch:  -0.8897290229797363\n",
            "Iteration:  12390      Current loss:  24.88890266418457      Current accuracy:  41.0      Comp_time of one batch:  -0.8945620059967041\n",
            "Iteration:  12395      Current loss:  17.54350471496582      Current accuracy:  42.0      Comp_time of one batch:  -0.9075419902801514\n",
            "Iteration:  12400      Current loss:  21.184207916259766      Current accuracy:  43.0      Comp_time of one batch:  -0.8911557197570801\n",
            "Iteration:  12405      Current loss:  22.81793212890625      Current accuracy:  44.0      Comp_time of one batch:  -0.9123258590698242\n",
            "Iteration:  12410      Current loss:  30.5349063873291      Current accuracy:  44.0      Comp_time of one batch:  -0.8802204132080078\n",
            "Iteration:  12415      Current loss:  24.081037521362305      Current accuracy:  44.0      Comp_time of one batch:  -0.8696088790893555\n",
            "Iteration:  12420      Current loss:  21.466459274291992      Current accuracy:  45.0      Comp_time of one batch:  -0.882575273513794\n",
            "Iteration:  12425      Current loss:  22.17925262451172      Current accuracy:  46.0      Comp_time of one batch:  -0.8950724601745605\n",
            "Iteration:  12430      Current loss:  26.426267623901367      Current accuracy:  46.0      Comp_time of one batch:  -0.9042661190032959\n",
            "Iteration:  12435      Current loss:  17.740936279296875      Current accuracy:  47.0      Comp_time of one batch:  -0.8999650478363037\n",
            "Iteration:  12440      Current loss:  21.93508529663086      Current accuracy:  48.0      Comp_time of one batch:  -0.9027683734893799\n",
            "Iteration:  12445      Current loss:  24.6444034576416      Current accuracy:  48.0      Comp_time of one batch:  -0.8959903717041016\n",
            "Iteration:  12450      Current loss:  23.584003448486328      Current accuracy:  48.0      Comp_time of one batch:  -0.884519100189209\n",
            "Iteration:  12455      Current loss:  30.743173599243164      Current accuracy:  48.0      Comp_time of one batch:  -0.8815789222717285\n",
            "Iteration:  12460      Current loss:  17.655914306640625      Current accuracy:  49.0      Comp_time of one batch:  -0.8744289875030518\n",
            "Iteration:  12465      Current loss:  20.62769889831543      Current accuracy:  50.0      Comp_time of one batch:  -0.8901503086090088\n",
            "Iteration:  12470      Current loss:  21.283416748046875      Current accuracy:  51.0      Comp_time of one batch:  -0.8793730735778809\n",
            "Iteration:  12475      Current loss:  24.960857391357422      Current accuracy:  51.0      Comp_time of one batch:  -0.8797194957733154\n",
            "Iteration:  12480      Current loss:  25.381799697875977      Current accuracy:  51.0      Comp_time of one batch:  -0.8985216617584229\n",
            "Iteration:  12485      Current loss:  26.854825973510742      Current accuracy:  51.0      Comp_time of one batch:  -0.8877472877502441\n",
            "Iteration:  12490      Current loss:  23.413150787353516      Current accuracy:  51.0      Comp_time of one batch:  -0.9027202129364014\n",
            "Iteration:  12495      Current loss:  19.451122283935547      Current accuracy:  52.0      Comp_time of one batch:  -0.8884661197662354\n",
            "Iteration:  12500      Current loss:  22.13193130493164      Current accuracy:  0.0      Comp_time of one batch:  -0.8851299285888672\n",
            "Iteration:  12505      Current loss:  20.583778381347656      Current accuracy:  1.0      Comp_time of one batch:  -0.8873462677001953\n",
            "Iteration:  12510      Current loss:  19.079330444335938      Current accuracy:  2.0      Comp_time of one batch:  -0.8861167430877686\n",
            "Iteration:  12515      Current loss:  18.788537979125977      Current accuracy:  3.0      Comp_time of one batch:  -0.9017722606658936\n",
            "Iteration:  12520      Current loss:  21.279138565063477      Current accuracy:  4.0      Comp_time of one batch:  -0.8874127864837646\n",
            "Iteration:  12525      Current loss:  18.705703735351562      Current accuracy:  5.0      Comp_time of one batch:  -0.8824887275695801\n",
            "Iteration:  12530      Current loss:  27.85116958618164      Current accuracy:  5.0      Comp_time of one batch:  -0.9034068584442139\n",
            "Iteration:  12535      Current loss:  22.513193130493164      Current accuracy:  5.0      Comp_time of one batch:  -0.8838756084442139\n",
            "Iteration:  12540      Current loss:  23.798812866210938      Current accuracy:  5.0      Comp_time of one batch:  -0.8835570812225342\n",
            "Iteration:  12545      Current loss:  22.335737228393555      Current accuracy:  6.0      Comp_time of one batch:  -0.9276080131530762\n",
            "Iteration:  12550      Current loss:  32.04964828491211      Current accuracy:  6.0      Comp_time of one batch:  -0.8868095874786377\n",
            "Iteration:  12555      Current loss:  20.969951629638672      Current accuracy:  7.0      Comp_time of one batch:  -0.8861441612243652\n",
            "Iteration:  12560      Current loss:  22.7388916015625      Current accuracy:  8.0      Comp_time of one batch:  -0.8968710899353027\n",
            "Iteration:  12565      Current loss:  37.94640350341797      Current accuracy:  8.0      Comp_time of one batch:  -0.9156715869903564\n",
            "Iteration:  12570      Current loss:  17.075817108154297      Current accuracy:  9.0      Comp_time of one batch:  -0.8823966979980469\n",
            "Iteration:  12575      Current loss:  19.845428466796875      Current accuracy:  10.0      Comp_time of one batch:  -0.9060049057006836\n",
            "Iteration:  12580      Current loss:  25.68497085571289      Current accuracy:  10.0      Comp_time of one batch:  -0.8783156871795654\n",
            "Iteration:  12585      Current loss:  25.07769012451172      Current accuracy:  10.0      Comp_time of one batch:  -0.8815319538116455\n",
            "Iteration:  12590      Current loss:  23.783016204833984      Current accuracy:  11.0      Comp_time of one batch:  -0.8921597003936768\n",
            "Iteration:  12595      Current loss:  23.362226486206055      Current accuracy:  12.0      Comp_time of one batch:  -0.9072017669677734\n",
            "Iteration:  12600      Current loss:  27.597475051879883      Current accuracy:  12.0      Comp_time of one batch:  -0.8808729648590088\n",
            "Iteration:  12605      Current loss:  16.978614807128906      Current accuracy:  13.0      Comp_time of one batch:  -0.9056620597839355\n",
            "Iteration:  12610      Current loss:  28.89985466003418      Current accuracy:  13.0      Comp_time of one batch:  -0.8975427150726318\n",
            "Iteration:  12615      Current loss:  23.541824340820312      Current accuracy:  13.0      Comp_time of one batch:  -0.8731257915496826\n",
            "Iteration:  12620      Current loss:  17.754487991333008      Current accuracy:  14.0      Comp_time of one batch:  -0.9051401615142822\n",
            "Iteration:  12625      Current loss:  23.633024215698242      Current accuracy:  14.0      Comp_time of one batch:  -0.8790619373321533\n",
            "Iteration:  12630      Current loss:  22.550872802734375      Current accuracy:  15.0      Comp_time of one batch:  -0.8766765594482422\n",
            "Iteration:  12635      Current loss:  27.65528106689453      Current accuracy:  15.0      Comp_time of one batch:  -0.9195747375488281\n",
            "Iteration:  12640      Current loss:  23.438596725463867      Current accuracy:  15.0      Comp_time of one batch:  -0.8854947090148926\n",
            "Iteration:  12645      Current loss:  27.47701072692871      Current accuracy:  15.0      Comp_time of one batch:  -0.8742425441741943\n",
            "Iteration:  12650      Current loss:  18.481712341308594      Current accuracy:  16.0      Comp_time of one batch:  -0.917595624923706\n",
            "Iteration:  12655      Current loss:  31.187904357910156      Current accuracy:  16.0      Comp_time of one batch:  -0.8819258213043213\n",
            "Iteration:  12660      Current loss:  29.4024600982666      Current accuracy:  16.0      Comp_time of one batch:  -0.8799886703491211\n",
            "Iteration:  12665      Current loss:  22.081592559814453      Current accuracy:  17.0      Comp_time of one batch:  -0.8817636966705322\n",
            "Iteration:  12670      Current loss:  21.3605899810791      Current accuracy:  18.0      Comp_time of one batch:  -0.8943424224853516\n",
            "Iteration:  12675      Current loss:  30.27985954284668      Current accuracy:  18.0      Comp_time of one batch:  -0.9075956344604492\n",
            "Iteration:  12680      Current loss:  25.103120803833008      Current accuracy:  18.0      Comp_time of one batch:  -0.8675334453582764\n",
            "Iteration:  12685      Current loss:  25.268447875976562      Current accuracy:  18.0      Comp_time of one batch:  -0.8966507911682129\n",
            "Iteration:  12690      Current loss:  24.038394927978516      Current accuracy:  19.0      Comp_time of one batch:  -0.8909695148468018\n",
            "Iteration:  12695      Current loss:  19.18009376525879      Current accuracy:  20.0      Comp_time of one batch:  -0.8722643852233887\n",
            "Iteration:  12700      Current loss:  25.167987823486328      Current accuracy:  20.0      Comp_time of one batch:  -0.8847439289093018\n",
            "Iteration:  12705      Current loss:  25.349285125732422      Current accuracy:  20.0      Comp_time of one batch:  -0.8800666332244873\n",
            "Iteration:  12710      Current loss:  21.3106746673584      Current accuracy:  21.0      Comp_time of one batch:  -0.885298490524292\n",
            "Iteration:  12715      Current loss:  32.16295623779297      Current accuracy:  21.0      Comp_time of one batch:  -0.8746721744537354\n",
            "Iteration:  12720      Current loss:  27.28525733947754      Current accuracy:  22.0      Comp_time of one batch:  -0.8873302936553955\n",
            "Iteration:  12725      Current loss:  25.372562408447266      Current accuracy:  22.0      Comp_time of one batch:  -0.8768901824951172\n",
            "Iteration:  12730      Current loss:  23.60218620300293      Current accuracy:  23.0      Comp_time of one batch:  -0.9021017551422119\n",
            "Iteration:  12735      Current loss:  17.42021369934082      Current accuracy:  24.0      Comp_time of one batch:  -0.8865108489990234\n",
            "Iteration:  12740      Current loss:  29.017292022705078      Current accuracy:  24.0      Comp_time of one batch:  -0.9179129600524902\n",
            "Iteration:  12745      Current loss:  22.27119255065918      Current accuracy:  25.0      Comp_time of one batch:  -0.9076569080352783\n",
            "Iteration:  12750      Current loss:  28.27381134033203      Current accuracy:  25.0      Comp_time of one batch:  -0.8776850700378418\n",
            "Iteration:  12755      Current loss:  16.920509338378906      Current accuracy:  26.0      Comp_time of one batch:  -0.8834779262542725\n",
            "Iteration:  12760      Current loss:  23.147991180419922      Current accuracy:  26.0      Comp_time of one batch:  -0.8885796070098877\n",
            "Iteration:  12765      Current loss:  25.12999153137207      Current accuracy:  26.0      Comp_time of one batch:  -0.8812582492828369\n",
            "Iteration:  12770      Current loss:  25.21734619140625      Current accuracy:  26.0      Comp_time of one batch:  -0.8867130279541016\n",
            "Iteration:  12775      Current loss:  19.358102798461914      Current accuracy:  27.0      Comp_time of one batch:  -0.8868489265441895\n",
            "Iteration:  12780      Current loss:  22.086381912231445      Current accuracy:  28.0      Comp_time of one batch:  -0.8877007961273193\n",
            "Iteration:  12785      Current loss:  31.053491592407227      Current accuracy:  28.0      Comp_time of one batch:  -0.8841142654418945\n",
            "Iteration:  12790      Current loss:  23.395463943481445      Current accuracy:  29.0      Comp_time of one batch:  -0.8844969272613525\n",
            "Iteration:  12795      Current loss:  25.521059036254883      Current accuracy:  29.0      Comp_time of one batch:  -0.8756601810455322\n",
            "Iteration:  12800      Current loss:  18.54288673400879      Current accuracy:  30.0      Comp_time of one batch:  -0.8808004856109619\n",
            "Iteration:  12805      Current loss:  22.68273162841797      Current accuracy:  30.0      Comp_time of one batch:  -0.884204626083374\n",
            "Iteration:  12810      Current loss:  40.33402633666992      Current accuracy:  30.0      Comp_time of one batch:  -0.8896462917327881\n",
            "Iteration:  12815      Current loss:  31.79872703552246      Current accuracy:  30.0      Comp_time of one batch:  -0.8890900611877441\n",
            "Iteration:  12820      Current loss:  24.133800506591797      Current accuracy:  30.0      Comp_time of one batch:  -0.8974132537841797\n",
            "Iteration:  12825      Current loss:  28.97039794921875      Current accuracy:  30.0      Comp_time of one batch:  -0.9084219932556152\n",
            "Iteration:  12830      Current loss:  30.343332290649414      Current accuracy:  30.0      Comp_time of one batch:  -0.9176235198974609\n",
            "Iteration:  12835      Current loss:  39.35990524291992      Current accuracy:  30.0      Comp_time of one batch:  -0.922555685043335\n",
            "Iteration:  12840      Current loss:  18.79388999938965      Current accuracy:  31.0      Comp_time of one batch:  -0.8903524875640869\n",
            "Iteration:  12845      Current loss:  22.260265350341797      Current accuracy:  32.0      Comp_time of one batch:  -0.9159202575683594\n",
            "Iteration:  12850      Current loss:  37.2379264831543      Current accuracy:  32.0      Comp_time of one batch:  -0.8926877975463867\n",
            "Iteration:  12855      Current loss:  16.509353637695312      Current accuracy:  33.0      Comp_time of one batch:  -0.8929662704467773\n",
            "Iteration:  12860      Current loss:  28.575510025024414      Current accuracy:  33.0      Comp_time of one batch:  -0.8826720714569092\n",
            "Iteration:  12865      Current loss:  21.86897850036621      Current accuracy:  34.0      Comp_time of one batch:  -0.9413771629333496\n",
            "Iteration:  12870      Current loss:  24.91925048828125      Current accuracy:  34.0      Comp_time of one batch:  -0.8845772743225098\n",
            "Iteration:  12875      Current loss:  25.09313201904297      Current accuracy:  34.0      Comp_time of one batch:  -0.9222240447998047\n",
            "Iteration:  12880      Current loss:  24.250179290771484      Current accuracy:  34.0      Comp_time of one batch:  -0.8860433101654053\n",
            "Iteration:  12885      Current loss:  27.570716857910156      Current accuracy:  34.0      Comp_time of one batch:  -0.8870840072631836\n",
            "Iteration:  12890      Current loss:  20.83323860168457      Current accuracy:  35.0      Comp_time of one batch:  -0.9063124656677246\n",
            "Iteration:  12895      Current loss:  27.104963302612305      Current accuracy:  35.0      Comp_time of one batch:  -0.8835022449493408\n",
            "Iteration:  12900      Current loss:  19.749637603759766      Current accuracy:  36.0      Comp_time of one batch:  -0.885547399520874\n",
            "Iteration:  12905      Current loss:  29.720447540283203      Current accuracy:  36.0      Comp_time of one batch:  -0.8956928253173828\n",
            "Iteration:  12910      Current loss:  23.694194793701172      Current accuracy:  36.0      Comp_time of one batch:  -0.9023306369781494\n",
            "Iteration:  12915      Current loss:  26.73898696899414      Current accuracy:  36.0      Comp_time of one batch:  -0.8833181858062744\n",
            "Iteration:  12920      Current loss:  18.644132614135742      Current accuracy:  37.0      Comp_time of one batch:  -0.917651891708374\n",
            "Iteration:  12925      Current loss:  28.783002853393555      Current accuracy:  37.0      Comp_time of one batch:  -0.8818035125732422\n",
            "Iteration:  12930      Current loss:  21.357139587402344      Current accuracy:  38.0      Comp_time of one batch:  -0.8860342502593994\n",
            "Iteration:  12935      Current loss:  22.39498519897461      Current accuracy:  39.0      Comp_time of one batch:  -0.8869564533233643\n",
            "Iteration:  12940      Current loss:  28.59088134765625      Current accuracy:  39.0      Comp_time of one batch:  -0.8849709033966064\n",
            "Iteration:  12945      Current loss:  23.335765838623047      Current accuracy:  40.0      Comp_time of one batch:  -0.8889284133911133\n",
            "Iteration:  12950      Current loss:  25.28928565979004      Current accuracy:  40.0      Comp_time of one batch:  -0.8672051429748535\n",
            "Iteration:  12955      Current loss:  20.313282012939453      Current accuracy:  41.0      Comp_time of one batch:  -0.9092545509338379\n",
            "Iteration:  12960      Current loss:  17.53129005432129      Current accuracy:  42.0      Comp_time of one batch:  -0.904984712600708\n",
            "Iteration:  12965      Current loss:  19.901533126831055      Current accuracy:  43.0      Comp_time of one batch:  -0.8856356143951416\n",
            "Iteration:  12970      Current loss:  24.382797241210938      Current accuracy:  43.0      Comp_time of one batch:  -0.8850817680358887\n",
            "Iteration:  12975      Current loss:  39.02815628051758      Current accuracy:  43.0      Comp_time of one batch:  -0.908994197845459\n",
            "Iteration:  12980      Current loss:  22.984020233154297      Current accuracy:  44.0      Comp_time of one batch:  -0.8900680541992188\n",
            "Iteration:  12985      Current loss:  24.21516990661621      Current accuracy:  44.0      Comp_time of one batch:  -0.8949313163757324\n",
            "Iteration:  12990      Current loss:  20.497657775878906      Current accuracy:  45.0      Comp_time of one batch:  -0.9044148921966553\n",
            "Iteration:  12995      Current loss:  16.11701202392578      Current accuracy:  46.0      Comp_time of one batch:  -0.8899481296539307\n",
            "Iteration:  13000      Current loss:  26.60605812072754      Current accuracy:  0.0      Comp_time of one batch:  -0.8812317848205566\n",
            "Iteration:  13005      Current loss:  37.23920440673828      Current accuracy:  0.0      Comp_time of one batch:  -0.8823513984680176\n",
            "Iteration:  13010      Current loss:  22.20371437072754      Current accuracy:  1.0      Comp_time of one batch:  -0.9145338535308838\n",
            "Iteration:  13015      Current loss:  23.273780822753906      Current accuracy:  1.0      Comp_time of one batch:  -0.8868231773376465\n",
            "Iteration:  13020      Current loss:  33.046627044677734      Current accuracy:  1.0      Comp_time of one batch:  -0.8883500099182129\n",
            "Iteration:  13025      Current loss:  30.937917709350586      Current accuracy:  1.0      Comp_time of one batch:  -0.8809328079223633\n",
            "Iteration:  13030      Current loss:  24.020830154418945      Current accuracy:  1.0      Comp_time of one batch:  -0.8810899257659912\n",
            "Iteration:  13035      Current loss:  32.77323532104492      Current accuracy:  1.0      Comp_time of one batch:  -0.8918678760528564\n",
            "Iteration:  13040      Current loss:  20.753541946411133      Current accuracy:  2.0      Comp_time of one batch:  -0.8950028419494629\n",
            "Iteration:  13045      Current loss:  33.953834533691406      Current accuracy:  2.0      Comp_time of one batch:  -0.930321216583252\n",
            "Iteration:  13050      Current loss:  22.59718894958496      Current accuracy:  3.0      Comp_time of one batch:  -0.9044604301452637\n",
            "Iteration:  13055      Current loss:  17.725208282470703      Current accuracy:  4.0      Comp_time of one batch:  -0.8930974006652832\n",
            "Iteration:  13060      Current loss:  20.633792877197266      Current accuracy:  5.0      Comp_time of one batch:  -0.9125082492828369\n",
            "Iteration:  13065      Current loss:  26.38144302368164      Current accuracy:  5.0      Comp_time of one batch:  -0.877920389175415\n",
            "Iteration:  13070      Current loss:  21.0335750579834      Current accuracy:  6.0      Comp_time of one batch:  -0.8836665153503418\n",
            "Iteration:  13075      Current loss:  22.83706283569336      Current accuracy:  7.0      Comp_time of one batch:  -0.8883805274963379\n",
            "Iteration:  13080      Current loss:  23.268613815307617      Current accuracy:  8.0      Comp_time of one batch:  -0.888840913772583\n",
            "Iteration:  13085      Current loss:  27.526599884033203      Current accuracy:  8.0      Comp_time of one batch:  -0.8796367645263672\n",
            "Iteration:  13090      Current loss:  31.211977005004883      Current accuracy:  8.0      Comp_time of one batch:  -0.8819987773895264\n",
            "Iteration:  13095      Current loss:  23.570655822753906      Current accuracy:  8.0      Comp_time of one batch:  -0.9106369018554688\n",
            "Iteration:  13100      Current loss:  21.080575942993164      Current accuracy:  9.0      Comp_time of one batch:  -0.9083006381988525\n",
            "Iteration:  13105      Current loss:  24.970848083496094      Current accuracy:  10.0      Comp_time of one batch:  -0.8710019588470459\n",
            "Iteration:  13110      Current loss:  23.48724365234375      Current accuracy:  10.0      Comp_time of one batch:  -0.8798880577087402\n",
            "Iteration:  13115      Current loss:  43.19552993774414      Current accuracy:  10.0      Comp_time of one batch:  -0.8776886463165283\n",
            "Iteration:  13120      Current loss:  20.362932205200195      Current accuracy:  11.0      Comp_time of one batch:  -0.9184117317199707\n",
            "Iteration:  13125      Current loss:  24.060497283935547      Current accuracy:  11.0      Comp_time of one batch:  -0.8904366493225098\n",
            "Iteration:  13130      Current loss:  18.275815963745117      Current accuracy:  12.0      Comp_time of one batch:  -0.9207453727722168\n",
            "Iteration:  13135      Current loss:  26.600765228271484      Current accuracy:  12.0      Comp_time of one batch:  -0.9134006500244141\n",
            "Iteration:  13140      Current loss:  26.086872100830078      Current accuracy:  12.0      Comp_time of one batch:  -0.8879969120025635\n",
            "Iteration:  13145      Current loss:  19.950231552124023      Current accuracy:  13.0      Comp_time of one batch:  -0.9238359928131104\n",
            "Iteration:  13150      Current loss:  21.260967254638672      Current accuracy:  14.0      Comp_time of one batch:  -0.8991954326629639\n",
            "Iteration:  13155      Current loss:  22.1758975982666      Current accuracy:  15.0      Comp_time of one batch:  -0.8826277256011963\n",
            "Iteration:  13160      Current loss:  27.51079559326172      Current accuracy:  15.0      Comp_time of one batch:  -0.8863511085510254\n",
            "Iteration:  13165      Current loss:  16.13187026977539      Current accuracy:  16.0      Comp_time of one batch:  -0.926785945892334\n",
            "Iteration:  13170      Current loss:  24.387496948242188      Current accuracy:  16.0      Comp_time of one batch:  -0.8808755874633789\n",
            "Iteration:  13175      Current loss:  18.795467376708984      Current accuracy:  17.0      Comp_time of one batch:  -0.8809802532196045\n",
            "Iteration:  13180      Current loss:  23.7005558013916      Current accuracy:  17.0      Comp_time of one batch:  -0.9004678726196289\n",
            "Iteration:  13185      Current loss:  28.673070907592773      Current accuracy:  17.0      Comp_time of one batch:  -0.8770649433135986\n",
            "Iteration:  13190      Current loss:  22.537931442260742      Current accuracy:  17.0      Comp_time of one batch:  -0.883037805557251\n",
            "Iteration:  13195      Current loss:  20.710268020629883      Current accuracy:  18.0      Comp_time of one batch:  -0.8883717060089111\n",
            "Iteration:  13200      Current loss:  18.546886444091797      Current accuracy:  19.0      Comp_time of one batch:  -0.8937644958496094\n",
            "Iteration:  13205      Current loss:  22.426315307617188      Current accuracy:  19.0      Comp_time of one batch:  -0.8808739185333252\n",
            "Iteration:  13210      Current loss:  32.02820587158203      Current accuracy:  19.0      Comp_time of one batch:  -0.8921675682067871\n",
            "Iteration:  13215      Current loss:  23.69310760498047      Current accuracy:  19.0      Comp_time of one batch:  -0.888587474822998\n",
            "Iteration:  13220      Current loss:  27.604736328125      Current accuracy:  19.0      Comp_time of one batch:  -0.892841100692749\n",
            "Iteration:  13225      Current loss:  21.746936798095703      Current accuracy:  20.0      Comp_time of one batch:  -0.8967609405517578\n",
            "Iteration:  13230      Current loss:  25.050369262695312      Current accuracy:  20.0      Comp_time of one batch:  -0.8923184871673584\n",
            "Iteration:  13235      Current loss:  23.828636169433594      Current accuracy:  21.0      Comp_time of one batch:  -0.8819780349731445\n",
            "Iteration:  13240      Current loss:  21.627166748046875      Current accuracy:  22.0      Comp_time of one batch:  -0.8814477920532227\n",
            "Iteration:  13245      Current loss:  21.938756942749023      Current accuracy:  23.0      Comp_time of one batch:  -0.8948311805725098\n",
            "Iteration:  13250      Current loss:  25.72523307800293      Current accuracy:  23.0      Comp_time of one batch:  -0.8747100830078125\n",
            "Iteration:  13255      Current loss:  19.504940032958984      Current accuracy:  24.0      Comp_time of one batch:  -0.8815596103668213\n",
            "Iteration:  13260      Current loss:  27.390615463256836      Current accuracy:  24.0      Comp_time of one batch:  -0.9093568325042725\n",
            "Iteration:  13265      Current loss:  25.927480697631836      Current accuracy:  24.0      Comp_time of one batch:  -0.8994598388671875\n",
            "Iteration:  13270      Current loss:  19.031688690185547      Current accuracy:  25.0      Comp_time of one batch:  -0.8872127532958984\n",
            "Iteration:  13275      Current loss:  18.63727569580078      Current accuracy:  26.0      Comp_time of one batch:  -0.907585859298706\n",
            "Iteration:  13280      Current loss:  32.0120964050293      Current accuracy:  26.0      Comp_time of one batch:  -0.8927552700042725\n",
            "Iteration:  13285      Current loss:  20.367115020751953      Current accuracy:  27.0      Comp_time of one batch:  -0.8868215084075928\n",
            "Iteration:  13290      Current loss:  23.794206619262695      Current accuracy:  28.0      Comp_time of one batch:  -0.9149770736694336\n",
            "Iteration:  13295      Current loss:  24.011503219604492      Current accuracy:  28.0      Comp_time of one batch:  -0.8793692588806152\n",
            "Iteration:  13300      Current loss:  24.647151947021484      Current accuracy:  28.0      Comp_time of one batch:  -0.9007024765014648\n",
            "Iteration:  13305      Current loss:  29.837827682495117      Current accuracy:  28.0      Comp_time of one batch:  -0.8918237686157227\n",
            "Iteration:  13310      Current loss:  28.202789306640625      Current accuracy:  28.0      Comp_time of one batch:  -0.8788888454437256\n",
            "Iteration:  13315      Current loss:  27.98927116394043      Current accuracy:  28.0      Comp_time of one batch:  -0.9020669460296631\n",
            "Iteration:  13320      Current loss:  19.162960052490234      Current accuracy:  29.0      Comp_time of one batch:  -0.8772308826446533\n",
            "Iteration:  13325      Current loss:  26.639968872070312      Current accuracy:  29.0      Comp_time of one batch:  -0.8895153999328613\n",
            "Iteration:  13330      Current loss:  23.989147186279297      Current accuracy:  29.0      Comp_time of one batch:  -0.9101309776306152\n",
            "Iteration:  13335      Current loss:  23.786998748779297      Current accuracy:  29.0      Comp_time of one batch:  -0.8809962272644043\n",
            "Iteration:  13340      Current loss:  31.288042068481445      Current accuracy:  29.0      Comp_time of one batch:  -0.8759446144104004\n",
            "Iteration:  13345      Current loss:  31.856998443603516      Current accuracy:  29.0      Comp_time of one batch:  -0.9165365695953369\n",
            "Iteration:  13350      Current loss:  19.027381896972656      Current accuracy:  30.0      Comp_time of one batch:  -0.8830413818359375\n",
            "Iteration:  13355      Current loss:  18.213085174560547      Current accuracy:  31.0      Comp_time of one batch:  -0.8893182277679443\n",
            "Iteration:  13360      Current loss:  30.88518714904785      Current accuracy:  31.0      Comp_time of one batch:  -0.9054164886474609\n",
            "Iteration:  13365      Current loss:  19.90381622314453      Current accuracy:  32.0      Comp_time of one batch:  -0.9066321849822998\n",
            "Iteration:  13370      Current loss:  21.231460571289062      Current accuracy:  33.0      Comp_time of one batch:  -0.8780243396759033\n",
            "Iteration:  13375      Current loss:  25.074663162231445      Current accuracy:  33.0      Comp_time of one batch:  -0.9351363182067871\n",
            "Iteration:  13380      Current loss:  19.431394577026367      Current accuracy:  34.0      Comp_time of one batch:  -0.914780855178833\n",
            "Iteration:  13385      Current loss:  28.815250396728516      Current accuracy:  34.0      Comp_time of one batch:  -0.8759043216705322\n",
            "Iteration:  13390      Current loss:  34.82173156738281      Current accuracy:  34.0      Comp_time of one batch:  -0.892432451248169\n",
            "Iteration:  13395      Current loss:  30.08125877380371      Current accuracy:  34.0      Comp_time of one batch:  -0.9104621410369873\n",
            "Iteration:  13400      Current loss:  21.26023292541504      Current accuracy:  35.0      Comp_time of one batch:  -0.925105094909668\n",
            "Iteration:  13405      Current loss:  21.642213821411133      Current accuracy:  36.0      Comp_time of one batch:  -0.903792142868042\n",
            "Iteration:  13410      Current loss:  21.990123748779297      Current accuracy:  37.0      Comp_time of one batch:  -0.9436924457550049\n",
            "Iteration:  13415      Current loss:  19.23657989501953      Current accuracy:  38.0      Comp_time of one batch:  -0.9055132865905762\n",
            "Iteration:  13420      Current loss:  33.30843734741211      Current accuracy:  38.0      Comp_time of one batch:  -0.9081764221191406\n",
            "Iteration:  13425      Current loss:  24.81338882446289      Current accuracy:  38.0      Comp_time of one batch:  -0.8889579772949219\n",
            "Iteration:  13430      Current loss:  19.708131790161133      Current accuracy:  39.0      Comp_time of one batch:  -0.905181884765625\n",
            "Iteration:  13435      Current loss:  17.749622344970703      Current accuracy:  40.0      Comp_time of one batch:  -0.9056246280670166\n",
            "Iteration:  13440      Current loss:  16.374267578125      Current accuracy:  41.0      Comp_time of one batch:  -0.9308433532714844\n",
            "Iteration:  13445      Current loss:  25.425701141357422      Current accuracy:  41.0      Comp_time of one batch:  -0.8989579677581787\n",
            "Iteration:  13450      Current loss:  20.241077423095703      Current accuracy:  42.0      Comp_time of one batch:  -0.8909928798675537\n",
            "Iteration:  13455      Current loss:  23.36724853515625      Current accuracy:  43.0      Comp_time of one batch:  -0.8813488483428955\n",
            "Iteration:  13460      Current loss:  19.566526412963867      Current accuracy:  44.0      Comp_time of one batch:  -0.8783068656921387\n",
            "Iteration:  13465      Current loss:  23.460071563720703      Current accuracy:  44.0      Comp_time of one batch:  -0.9089579582214355\n",
            "Iteration:  13470      Current loss:  25.96593475341797      Current accuracy:  44.0      Comp_time of one batch:  -0.8846802711486816\n",
            "Iteration:  13475      Current loss:  23.760391235351562      Current accuracy:  45.0      Comp_time of one batch:  -0.8821382522583008\n",
            "Iteration:  13480      Current loss:  21.454641342163086      Current accuracy:  46.0      Comp_time of one batch:  -0.8792047500610352\n",
            "Iteration:  13485      Current loss:  16.443912506103516      Current accuracy:  47.0      Comp_time of one batch:  -0.8771007061004639\n",
            "Iteration:  13490      Current loss:  28.51692771911621      Current accuracy:  47.0      Comp_time of one batch:  -0.8960840702056885\n",
            "Iteration:  13495      Current loss:  19.7501277923584      Current accuracy:  48.0      Comp_time of one batch:  -0.9022409915924072\n",
            "Iteration:  13500      Current loss:  28.967449188232422      Current accuracy:  0.0      Comp_time of one batch:  -0.9203009605407715\n",
            "Iteration:  13505      Current loss:  31.379480361938477      Current accuracy:  0.0      Comp_time of one batch:  -0.8757565021514893\n",
            "Iteration:  13510      Current loss:  21.49875831604004      Current accuracy:  1.0      Comp_time of one batch:  -0.8935906887054443\n",
            "Iteration:  13515      Current loss:  23.009132385253906      Current accuracy:  1.0      Comp_time of one batch:  -0.880509614944458\n",
            "Iteration:  13520      Current loss:  35.275943756103516      Current accuracy:  1.0      Comp_time of one batch:  -0.8907675743103027\n",
            "Iteration:  13525      Current loss:  24.07761001586914      Current accuracy:  1.0      Comp_time of one batch:  -0.8807041645050049\n",
            "Iteration:  13530      Current loss:  29.03849220275879      Current accuracy:  1.0      Comp_time of one batch:  -0.9050695896148682\n",
            "Iteration:  13535      Current loss:  23.949644088745117      Current accuracy:  1.0      Comp_time of one batch:  -0.8866231441497803\n",
            "Iteration:  13540      Current loss:  21.273344039916992      Current accuracy:  2.0      Comp_time of one batch:  -0.9253935813903809\n",
            "Iteration:  13545      Current loss:  25.924644470214844      Current accuracy:  2.0      Comp_time of one batch:  -0.8986866474151611\n",
            "Iteration:  13550      Current loss:  29.80937385559082      Current accuracy:  2.0      Comp_time of one batch:  -0.8892230987548828\n",
            "Iteration:  13555      Current loss:  37.95181655883789      Current accuracy:  2.0      Comp_time of one batch:  -0.902327299118042\n",
            "Iteration:  13560      Current loss:  24.538297653198242      Current accuracy:  2.0      Comp_time of one batch:  -0.8882603645324707\n",
            "Iteration:  13565      Current loss:  23.924463272094727      Current accuracy:  2.0      Comp_time of one batch:  -0.9235177040100098\n",
            "Iteration:  13570      Current loss:  27.111894607543945      Current accuracy:  2.0      Comp_time of one batch:  -0.8758053779602051\n",
            "Iteration:  13575      Current loss:  34.88052749633789      Current accuracy:  2.0      Comp_time of one batch:  -0.9096100330352783\n",
            "Iteration:  13580      Current loss:  41.37945556640625      Current accuracy:  2.0      Comp_time of one batch:  -0.8821277618408203\n",
            "Iteration:  13585      Current loss:  26.083642959594727      Current accuracy:  2.0      Comp_time of one batch:  -0.8934774398803711\n",
            "Iteration:  13590      Current loss:  23.394309997558594      Current accuracy:  2.0      Comp_time of one batch:  -0.9264318943023682\n",
            "Iteration:  13595      Current loss:  22.100683212280273      Current accuracy:  3.0      Comp_time of one batch:  -0.8774693012237549\n",
            "Iteration:  13600      Current loss:  25.34172821044922      Current accuracy:  3.0      Comp_time of one batch:  -0.883746862411499\n",
            "Iteration:  13605      Current loss:  22.561309814453125      Current accuracy:  4.0      Comp_time of one batch:  -0.8715088367462158\n",
            "Iteration:  13610      Current loss:  24.70457649230957      Current accuracy:  4.0      Comp_time of one batch:  -0.8821926116943359\n",
            "Iteration:  13615      Current loss:  20.585220336914062      Current accuracy:  5.0      Comp_time of one batch:  -0.8844945430755615\n",
            "Iteration:  13620      Current loss:  29.225065231323242      Current accuracy:  5.0      Comp_time of one batch:  -0.8998770713806152\n",
            "Iteration:  13625      Current loss:  36.59503173828125      Current accuracy:  5.0      Comp_time of one batch:  -0.8934569358825684\n",
            "Iteration:  13630      Current loss:  28.196269989013672      Current accuracy:  5.0      Comp_time of one batch:  -0.8984248638153076\n",
            "Iteration:  13635      Current loss:  24.32205581665039      Current accuracy:  5.0      Comp_time of one batch:  -0.8878252506256104\n",
            "Iteration:  13640      Current loss:  20.445220947265625      Current accuracy:  6.0      Comp_time of one batch:  -0.8879730701446533\n",
            "Iteration:  13645      Current loss:  24.81863021850586      Current accuracy:  6.0      Comp_time of one batch:  -0.8928487300872803\n",
            "Iteration:  13650      Current loss:  19.43927001953125      Current accuracy:  7.0      Comp_time of one batch:  -0.8907201290130615\n",
            "Iteration:  13655      Current loss:  22.796424865722656      Current accuracy:  8.0      Comp_time of one batch:  -0.8908712863922119\n",
            "Iteration:  13660      Current loss:  19.902809143066406      Current accuracy:  9.0      Comp_time of one batch:  -0.8859400749206543\n",
            "Iteration:  13665      Current loss:  20.674312591552734      Current accuracy:  10.0      Comp_time of one batch:  -0.8749151229858398\n",
            "Iteration:  13670      Current loss:  22.672365188598633      Current accuracy:  10.0      Comp_time of one batch:  -0.8809893131256104\n",
            "Iteration:  13675      Current loss:  21.983848571777344      Current accuracy:  11.0      Comp_time of one batch:  -0.8812308311462402\n",
            "Iteration:  13680      Current loss:  22.210102081298828      Current accuracy:  11.0      Comp_time of one batch:  -0.8761672973632812\n",
            "Iteration:  13685      Current loss:  28.080312728881836      Current accuracy:  11.0      Comp_time of one batch:  -0.8735432624816895\n",
            "Iteration:  13690      Current loss:  19.98455810546875      Current accuracy:  12.0      Comp_time of one batch:  -0.8742859363555908\n",
            "Iteration:  13695      Current loss:  32.34944152832031      Current accuracy:  12.0      Comp_time of one batch:  -0.875018835067749\n",
            "Iteration:  13700      Current loss:  22.728649139404297      Current accuracy:  12.0      Comp_time of one batch:  -0.8781659603118896\n",
            "Iteration:  13705      Current loss:  28.56083106994629      Current accuracy:  12.0      Comp_time of one batch:  -0.8841018676757812\n",
            "Iteration:  13710      Current loss:  21.744123458862305      Current accuracy:  13.0      Comp_time of one batch:  -0.8827526569366455\n",
            "Iteration:  13715      Current loss:  24.021520614624023      Current accuracy:  13.0      Comp_time of one batch:  -0.8668806552886963\n",
            "Iteration:  13720      Current loss:  20.91131019592285      Current accuracy:  14.0      Comp_time of one batch:  -0.8957891464233398\n",
            "Iteration:  13725      Current loss:  24.34951400756836      Current accuracy:  14.0      Comp_time of one batch:  -0.8767242431640625\n",
            "Iteration:  13730      Current loss:  23.43378257751465      Current accuracy:  14.0      Comp_time of one batch:  -0.8893632888793945\n",
            "Iteration:  13735      Current loss:  19.539928436279297      Current accuracy:  15.0      Comp_time of one batch:  -0.8969762325286865\n",
            "Iteration:  13740      Current loss:  16.779165267944336      Current accuracy:  16.0      Comp_time of one batch:  -0.9130706787109375\n",
            "Iteration:  13745      Current loss:  31.24349021911621      Current accuracy:  16.0      Comp_time of one batch:  -0.8739936351776123\n",
            "Iteration:  13750      Current loss:  23.828163146972656      Current accuracy:  16.0      Comp_time of one batch:  -0.875126838684082\n",
            "Iteration:  13755      Current loss:  22.564905166625977      Current accuracy:  16.0      Comp_time of one batch:  -0.9117984771728516\n",
            "Iteration:  13760      Current loss:  25.073501586914062      Current accuracy:  16.0      Comp_time of one batch:  -0.8794693946838379\n",
            "Iteration:  13765      Current loss:  24.412439346313477      Current accuracy:  16.0      Comp_time of one batch:  -0.8864493370056152\n",
            "Iteration:  13770      Current loss:  29.0347957611084      Current accuracy:  16.0      Comp_time of one batch:  -0.8906893730163574\n",
            "Iteration:  13775      Current loss:  17.833314895629883      Current accuracy:  17.0      Comp_time of one batch:  -0.8839895725250244\n",
            "Iteration:  13780      Current loss:  28.43880844116211      Current accuracy:  17.0      Comp_time of one batch:  -0.9123709201812744\n",
            "Iteration:  13785      Current loss:  14.32946491241455      Current accuracy:  18.0      Comp_time of one batch:  -0.8886532783508301\n",
            "Iteration:  13790      Current loss:  34.91873550415039      Current accuracy:  18.0      Comp_time of one batch:  -0.8832924365997314\n",
            "Iteration:  13795      Current loss:  24.056119918823242      Current accuracy:  18.0      Comp_time of one batch:  -0.9014322757720947\n",
            "Iteration:  13800      Current loss:  19.131813049316406      Current accuracy:  19.0      Comp_time of one batch:  -0.8801553249359131\n",
            "Iteration:  13805      Current loss:  24.08679962158203      Current accuracy:  19.0      Comp_time of one batch:  -0.8771731853485107\n",
            "Iteration:  13810      Current loss:  17.081645965576172      Current accuracy:  20.0      Comp_time of one batch:  -0.8846521377563477\n",
            "Iteration:  13815      Current loss:  25.96109962463379      Current accuracy:  20.0      Comp_time of one batch:  -0.8993966579437256\n",
            "Iteration:  13820      Current loss:  30.27113151550293      Current accuracy:  20.0      Comp_time of one batch:  -0.8801369667053223\n",
            "Iteration:  13825      Current loss:  30.26957130432129      Current accuracy:  20.0      Comp_time of one batch:  -0.8913989067077637\n",
            "Iteration:  13830      Current loss:  18.068038940429688      Current accuracy:  21.0      Comp_time of one batch:  -0.899207592010498\n",
            "Iteration:  13835      Current loss:  22.02884292602539      Current accuracy:  22.0      Comp_time of one batch:  -0.8861565589904785\n",
            "Iteration:  13840      Current loss:  27.608896255493164      Current accuracy:  22.0      Comp_time of one batch:  -0.8905785083770752\n",
            "Iteration:  13845      Current loss:  20.596364974975586      Current accuracy:  23.0      Comp_time of one batch:  -0.878218412399292\n",
            "Iteration:  13850      Current loss:  29.655776977539062      Current accuracy:  23.0      Comp_time of one batch:  -0.8882737159729004\n",
            "Iteration:  13855      Current loss:  25.333911895751953      Current accuracy:  23.0      Comp_time of one batch:  -0.8907928466796875\n",
            "Iteration:  13860      Current loss:  21.51725196838379      Current accuracy:  24.0      Comp_time of one batch:  -0.8919918537139893\n",
            "Iteration:  13865      Current loss:  22.427268981933594      Current accuracy:  25.0      Comp_time of one batch:  -0.890383243560791\n",
            "Iteration:  13870      Current loss:  20.52623748779297      Current accuracy:  26.0      Comp_time of one batch:  -0.9030601978302002\n",
            "Iteration:  13875      Current loss:  24.48798370361328      Current accuracy:  26.0      Comp_time of one batch:  -0.8886008262634277\n",
            "Iteration:  13880      Current loss:  30.21784210205078      Current accuracy:  26.0      Comp_time of one batch:  -0.9065361022949219\n",
            "Iteration:  13885      Current loss:  17.83187484741211      Current accuracy:  27.0      Comp_time of one batch:  -0.9045898914337158\n",
            "Iteration:  13890      Current loss:  17.102205276489258      Current accuracy:  28.0      Comp_time of one batch:  -0.9332211017608643\n",
            "Iteration:  13895      Current loss:  17.410776138305664      Current accuracy:  29.0      Comp_time of one batch:  -0.8951089382171631\n",
            "Iteration:  13900      Current loss:  17.689746856689453      Current accuracy:  30.0      Comp_time of one batch:  -0.9199337959289551\n",
            "Iteration:  13905      Current loss:  19.518678665161133      Current accuracy:  31.0      Comp_time of one batch:  -0.8983726501464844\n",
            "Iteration:  13910      Current loss:  25.439786911010742      Current accuracy:  31.0      Comp_time of one batch:  -0.872971773147583\n",
            "Iteration:  13915      Current loss:  22.21223258972168      Current accuracy:  32.0      Comp_time of one batch:  -0.8943779468536377\n",
            "Iteration:  13920      Current loss:  17.704246520996094      Current accuracy:  33.0      Comp_time of one batch:  -0.8816864490509033\n",
            "Iteration:  13925      Current loss:  30.893611907958984      Current accuracy:  33.0      Comp_time of one batch:  -0.8882730007171631\n",
            "Iteration:  13930      Current loss:  19.283504486083984      Current accuracy:  34.0      Comp_time of one batch:  -0.9125781059265137\n",
            "Iteration:  13935      Current loss:  25.436513900756836      Current accuracy:  34.0      Comp_time of one batch:  -0.8994712829589844\n",
            "Iteration:  13940      Current loss:  18.499189376831055      Current accuracy:  35.0      Comp_time of one batch:  -0.8805816173553467\n",
            "Iteration:  13945      Current loss:  24.61176109313965      Current accuracy:  35.0      Comp_time of one batch:  -0.8833341598510742\n",
            "Iteration:  13950      Current loss:  27.929218292236328      Current accuracy:  35.0      Comp_time of one batch:  -0.8709156513214111\n",
            "Iteration:  13955      Current loss:  26.07273292541504      Current accuracy:  35.0      Comp_time of one batch:  -0.9178354740142822\n",
            "Iteration:  13960      Current loss:  19.482107162475586      Current accuracy:  36.0      Comp_time of one batch:  -0.8807199001312256\n",
            "Iteration:  13965      Current loss:  21.789207458496094      Current accuracy:  37.0      Comp_time of one batch:  -0.9128313064575195\n",
            "Iteration:  13970      Current loss:  22.39333724975586      Current accuracy:  38.0      Comp_time of one batch:  -0.8896310329437256\n",
            "Iteration:  13975      Current loss:  24.90083122253418      Current accuracy:  38.0      Comp_time of one batch:  -0.8786368370056152\n",
            "Iteration:  13980      Current loss:  22.618297576904297      Current accuracy:  39.0      Comp_time of one batch:  -0.8823344707489014\n",
            "Iteration:  13985      Current loss:  17.084625244140625      Current accuracy:  40.0      Comp_time of one batch:  -0.9053866863250732\n",
            "Iteration:  13990      Current loss:  25.484073638916016      Current accuracy:  40.0      Comp_time of one batch:  -0.8807880878448486\n",
            "Iteration:  13995      Current loss:  24.80558967590332      Current accuracy:  40.0      Comp_time of one batch:  -0.897303581237793\n",
            "Iteration:  14000      Current loss:  24.6334285736084      Current accuracy:  0.0      Comp_time of one batch:  -0.8882038593292236\n",
            "Iteration:  14005      Current loss:  20.34217643737793      Current accuracy:  1.0      Comp_time of one batch:  -0.8752505779266357\n",
            "Iteration:  14010      Current loss:  30.87265396118164      Current accuracy:  1.0      Comp_time of one batch:  -0.8938708305358887\n",
            "Iteration:  14015      Current loss:  19.171476364135742      Current accuracy:  2.0      Comp_time of one batch:  -0.8841691017150879\n",
            "Iteration:  14020      Current loss:  19.204633712768555      Current accuracy:  3.0      Comp_time of one batch:  -0.8807411193847656\n",
            "Iteration:  14025      Current loss:  23.557933807373047      Current accuracy:  3.0      Comp_time of one batch:  -0.8901829719543457\n",
            "Iteration:  14030      Current loss:  29.21853256225586      Current accuracy:  3.0      Comp_time of one batch:  -0.8944892883300781\n",
            "Iteration:  14035      Current loss:  21.628278732299805      Current accuracy:  4.0      Comp_time of one batch:  -0.9008224010467529\n",
            "Iteration:  14040      Current loss:  35.904441833496094      Current accuracy:  4.0      Comp_time of one batch:  -0.8800935745239258\n",
            "Iteration:  14045      Current loss:  29.534732818603516      Current accuracy:  4.0      Comp_time of one batch:  -0.8831655979156494\n",
            "Iteration:  14050      Current loss:  23.970651626586914      Current accuracy:  4.0      Comp_time of one batch:  -0.8765749931335449\n",
            "Iteration:  14055      Current loss:  22.106355667114258      Current accuracy:  5.0      Comp_time of one batch:  -0.8934800624847412\n",
            "Iteration:  14060      Current loss:  19.99773597717285      Current accuracy:  6.0      Comp_time of one batch:  -0.8920059204101562\n",
            "Iteration:  14065      Current loss:  22.055051803588867      Current accuracy:  7.0      Comp_time of one batch:  -0.8996491432189941\n",
            "Iteration:  14070      Current loss:  24.27299690246582      Current accuracy:  7.0      Comp_time of one batch:  -0.8977251052856445\n",
            "Iteration:  14075      Current loss:  28.43832778930664      Current accuracy:  7.0      Comp_time of one batch:  -0.9007954597473145\n",
            "Iteration:  14080      Current loss:  25.096776962280273      Current accuracy:  7.0      Comp_time of one batch:  -0.886934757232666\n",
            "Iteration:  14085      Current loss:  30.378129959106445      Current accuracy:  7.0      Comp_time of one batch:  -0.9180135726928711\n",
            "Iteration:  14090      Current loss:  21.91949462890625      Current accuracy:  8.0      Comp_time of one batch:  -0.912045955657959\n",
            "Iteration:  14095      Current loss:  19.285493850708008      Current accuracy:  9.0      Comp_time of one batch:  -0.9065284729003906\n",
            "Iteration:  14100      Current loss:  35.27812957763672      Current accuracy:  9.0      Comp_time of one batch:  -0.9247269630432129\n",
            "Iteration:  14105      Current loss:  24.67995262145996      Current accuracy:  9.0      Comp_time of one batch:  -0.8910260200500488\n",
            "Iteration:  14110      Current loss:  27.223793029785156      Current accuracy:  9.0      Comp_time of one batch:  -0.8948173522949219\n",
            "Iteration:  14115      Current loss:  23.735292434692383      Current accuracy:  9.0      Comp_time of one batch:  -0.8828995227813721\n",
            "Iteration:  14120      Current loss:  24.096826553344727      Current accuracy:  9.0      Comp_time of one batch:  -0.897749662399292\n",
            "Iteration:  14125      Current loss:  25.879976272583008      Current accuracy:  9.0      Comp_time of one batch:  -0.8784732818603516\n",
            "Iteration:  14130      Current loss:  22.225467681884766      Current accuracy:  10.0      Comp_time of one batch:  -0.8800902366638184\n",
            "Iteration:  14135      Current loss:  24.047714233398438      Current accuracy:  11.0      Comp_time of one batch:  -0.879021406173706\n",
            "Iteration:  14140      Current loss:  20.453676223754883      Current accuracy:  12.0      Comp_time of one batch:  -0.8887267112731934\n",
            "Iteration:  14145      Current loss:  21.83066749572754      Current accuracy:  13.0      Comp_time of one batch:  -0.8797380924224854\n",
            "Iteration:  14150      Current loss:  27.677921295166016      Current accuracy:  13.0      Comp_time of one batch:  -0.878443717956543\n",
            "Iteration:  14155      Current loss:  20.67784309387207      Current accuracy:  14.0      Comp_time of one batch:  -0.887427806854248\n",
            "Iteration:  14160      Current loss:  26.650484085083008      Current accuracy:  15.0      Comp_time of one batch:  -0.8835465908050537\n",
            "Iteration:  14165      Current loss:  21.093650817871094      Current accuracy:  16.0      Comp_time of one batch:  -0.8864326477050781\n",
            "Iteration:  14170      Current loss:  27.170528411865234      Current accuracy:  16.0      Comp_time of one batch:  -0.8777456283569336\n",
            "Iteration:  14175      Current loss:  20.96841812133789      Current accuracy:  17.0      Comp_time of one batch:  -0.8827455043792725\n",
            "Iteration:  14180      Current loss:  23.748525619506836      Current accuracy:  18.0      Comp_time of one batch:  -0.9211974143981934\n",
            "Iteration:  14185      Current loss:  30.164093017578125      Current accuracy:  18.0      Comp_time of one batch:  -0.9033873081207275\n",
            "Iteration:  14190      Current loss:  19.321895599365234      Current accuracy:  19.0      Comp_time of one batch:  -0.8764221668243408\n",
            "Iteration:  14195      Current loss:  20.756441116333008      Current accuracy:  20.0      Comp_time of one batch:  -0.8785738945007324\n",
            "Iteration:  14200      Current loss:  20.342134475708008      Current accuracy:  21.0      Comp_time of one batch:  -0.8754329681396484\n",
            "Iteration:  14205      Current loss:  19.75524139404297      Current accuracy:  22.0      Comp_time of one batch:  -0.8761131763458252\n",
            "Iteration:  14210      Current loss:  24.376605987548828      Current accuracy:  22.0      Comp_time of one batch:  -0.9212522506713867\n",
            "Iteration:  14215      Current loss:  29.850465774536133      Current accuracy:  22.0      Comp_time of one batch:  -0.8925790786743164\n",
            "Iteration:  14220      Current loss:  30.5241756439209      Current accuracy:  22.0      Comp_time of one batch:  -0.9151685237884521\n",
            "Iteration:  14225      Current loss:  20.18012237548828      Current accuracy:  23.0      Comp_time of one batch:  -0.901862382888794\n",
            "Iteration:  14230      Current loss:  14.380183219909668      Current accuracy:  24.0      Comp_time of one batch:  -0.8886351585388184\n",
            "Iteration:  14235      Current loss:  24.67013168334961      Current accuracy:  24.0      Comp_time of one batch:  -0.8832285404205322\n",
            "Iteration:  14240      Current loss:  29.021198272705078      Current accuracy:  24.0      Comp_time of one batch:  -0.8870723247528076\n",
            "Iteration:  14245      Current loss:  22.148544311523438      Current accuracy:  25.0      Comp_time of one batch:  -0.9008007049560547\n",
            "Iteration:  14250      Current loss:  22.889089584350586      Current accuracy:  26.0      Comp_time of one batch:  -0.8813254833221436\n",
            "Iteration:  14255      Current loss:  23.544387817382812      Current accuracy:  26.0      Comp_time of one batch:  -0.881385326385498\n",
            "Iteration:  14260      Current loss:  29.270845413208008      Current accuracy:  26.0      Comp_time of one batch:  -0.9167332649230957\n",
            "Iteration:  14265      Current loss:  18.632675170898438      Current accuracy:  27.0      Comp_time of one batch:  -0.9014124870300293\n",
            "Iteration:  14270      Current loss:  24.54983139038086      Current accuracy:  27.0      Comp_time of one batch:  -0.8899447917938232\n",
            "Iteration:  14275      Current loss:  29.527881622314453      Current accuracy:  27.0      Comp_time of one batch:  -0.8959438800811768\n",
            "Iteration:  14280      Current loss:  26.207195281982422      Current accuracy:  27.0      Comp_time of one batch:  -0.8924660682678223\n",
            "Iteration:  14285      Current loss:  21.078344345092773      Current accuracy:  28.0      Comp_time of one batch:  -0.8906698226928711\n",
            "Iteration:  14290      Current loss:  30.585832595825195      Current accuracy:  28.0      Comp_time of one batch:  -0.8779730796813965\n",
            "Iteration:  14295      Current loss:  22.65108871459961      Current accuracy:  29.0      Comp_time of one batch:  -0.8876986503601074\n",
            "Iteration:  14300      Current loss:  22.97942543029785      Current accuracy:  29.0      Comp_time of one batch:  -0.8984978199005127\n",
            "Iteration:  14305      Current loss:  24.8740234375      Current accuracy:  29.0      Comp_time of one batch:  -0.8799827098846436\n",
            "Iteration:  14310      Current loss:  29.216121673583984      Current accuracy:  29.0      Comp_time of one batch:  -0.909747838973999\n",
            "Iteration:  14315      Current loss:  30.323204040527344      Current accuracy:  29.0      Comp_time of one batch:  -0.8945906162261963\n",
            "Iteration:  14320      Current loss:  22.68516731262207      Current accuracy:  30.0      Comp_time of one batch:  -0.9092767238616943\n",
            "Iteration:  14325      Current loss:  20.471736907958984      Current accuracy:  31.0      Comp_time of one batch:  -0.9112169742584229\n",
            "Iteration:  14330      Current loss:  18.113927841186523      Current accuracy:  32.0      Comp_time of one batch:  -0.9112653732299805\n",
            "Iteration:  14335      Current loss:  18.769182205200195      Current accuracy:  33.0      Comp_time of one batch:  -0.8988234996795654\n",
            "Iteration:  14340      Current loss:  23.227176666259766      Current accuracy:  34.0      Comp_time of one batch:  -0.8994350433349609\n",
            "Iteration:  14345      Current loss:  21.939197540283203      Current accuracy:  35.0      Comp_time of one batch:  -0.9038164615631104\n",
            "Iteration:  14350      Current loss:  25.12419891357422      Current accuracy:  35.0      Comp_time of one batch:  -0.8871099948883057\n",
            "Iteration:  14355      Current loss:  27.03565788269043      Current accuracy:  35.0      Comp_time of one batch:  -0.893629789352417\n",
            "Iteration:  14360      Current loss:  23.000335693359375      Current accuracy:  36.0      Comp_time of one batch:  -0.8861563205718994\n",
            "Iteration:  14365      Current loss:  19.964031219482422      Current accuracy:  37.0      Comp_time of one batch:  -0.8967764377593994\n",
            "Iteration:  14370      Current loss:  21.285926818847656      Current accuracy:  38.0      Comp_time of one batch:  -0.885244607925415\n",
            "Iteration:  14375      Current loss:  19.568992614746094      Current accuracy:  39.0      Comp_time of one batch:  -0.8883097171783447\n",
            "Iteration:  14380      Current loss:  24.584091186523438      Current accuracy:  39.0      Comp_time of one batch:  -0.895622730255127\n",
            "Iteration:  14385      Current loss:  34.736637115478516      Current accuracy:  39.0      Comp_time of one batch:  -0.8838658332824707\n",
            "Iteration:  14390      Current loss:  27.678985595703125      Current accuracy:  39.0      Comp_time of one batch:  -0.8854019641876221\n",
            "Iteration:  14395      Current loss:  29.876113891601562      Current accuracy:  39.0      Comp_time of one batch:  -0.8884992599487305\n",
            "Iteration:  14400      Current loss:  17.638731002807617      Current accuracy:  40.0      Comp_time of one batch:  -0.899134635925293\n",
            "Iteration:  14405      Current loss:  19.074392318725586      Current accuracy:  41.0      Comp_time of one batch:  -0.8922805786132812\n",
            "Iteration:  14410      Current loss:  25.308910369873047      Current accuracy:  41.0      Comp_time of one batch:  -0.8858022689819336\n",
            "Iteration:  14415      Current loss:  17.76205825805664      Current accuracy:  42.0      Comp_time of one batch:  -0.8730230331420898\n",
            "Iteration:  14420      Current loss:  22.01386260986328      Current accuracy:  43.0      Comp_time of one batch:  -0.8943729400634766\n",
            "Iteration:  14425      Current loss:  21.260019302368164      Current accuracy:  44.0      Comp_time of one batch:  -0.915888786315918\n",
            "Iteration:  14430      Current loss:  31.260086059570312      Current accuracy:  44.0      Comp_time of one batch:  -0.8996829986572266\n",
            "Iteration:  14435      Current loss:  19.605533599853516      Current accuracy:  45.0      Comp_time of one batch:  -0.8799498081207275\n",
            "Iteration:  14440      Current loss:  28.389989852905273      Current accuracy:  45.0      Comp_time of one batch:  -0.8946456909179688\n",
            "Iteration:  14445      Current loss:  23.807920455932617      Current accuracy:  46.0      Comp_time of one batch:  -0.9089992046356201\n",
            "Iteration:  14450      Current loss:  31.430419921875      Current accuracy:  46.0      Comp_time of one batch:  -0.8845899105072021\n",
            "Iteration:  14455      Current loss:  27.878087997436523      Current accuracy:  46.0      Comp_time of one batch:  -0.9088382720947266\n",
            "Iteration:  14460      Current loss:  30.46822738647461      Current accuracy:  46.0      Comp_time of one batch:  -0.922015905380249\n",
            "Iteration:  14465      Current loss:  22.64411163330078      Current accuracy:  47.0      Comp_time of one batch:  -0.9032726287841797\n",
            "Iteration:  14470      Current loss:  19.292028427124023      Current accuracy:  48.0      Comp_time of one batch:  -0.8825094699859619\n",
            "Iteration:  14475      Current loss:  17.9405574798584      Current accuracy:  49.0      Comp_time of one batch:  -0.922593355178833\n",
            "Iteration:  14480      Current loss:  38.85292053222656      Current accuracy:  49.0      Comp_time of one batch:  -0.8719336986541748\n",
            "Iteration:  14485      Current loss:  18.788917541503906      Current accuracy:  50.0      Comp_time of one batch:  -0.8895049095153809\n",
            "Iteration:  14490      Current loss:  24.496538162231445      Current accuracy:  50.0      Comp_time of one batch:  -0.9240016937255859\n",
            "Iteration:  14495      Current loss:  29.824594497680664      Current accuracy:  50.0      Comp_time of one batch:  -0.8864717483520508\n",
            "Iteration:  14500      Current loss:  29.23418426513672      Current accuracy:  0.0      Comp_time of one batch:  -0.8794863224029541\n",
            "Iteration:  14505      Current loss:  22.816713333129883      Current accuracy:  1.0      Comp_time of one batch:  -0.8821041584014893\n",
            "Iteration:  14510      Current loss:  24.16999053955078      Current accuracy:  1.0      Comp_time of one batch:  -0.8888571262359619\n",
            "Iteration:  14515      Current loss:  22.85404396057129      Current accuracy:  1.0      Comp_time of one batch:  -0.8837718963623047\n",
            "Iteration:  14520      Current loss:  25.601099014282227      Current accuracy:  1.0      Comp_time of one batch:  -0.8827908039093018\n",
            "Iteration:  14525      Current loss:  17.97723960876465      Current accuracy:  2.0      Comp_time of one batch:  -0.8813674449920654\n",
            "Iteration:  14530      Current loss:  32.70161056518555      Current accuracy:  2.0      Comp_time of one batch:  -0.8805451393127441\n",
            "Iteration:  14535      Current loss:  19.97849464416504      Current accuracy:  3.0      Comp_time of one batch:  -0.9079651832580566\n",
            "Iteration:  14540      Current loss:  20.872413635253906      Current accuracy:  4.0      Comp_time of one batch:  -0.8819785118103027\n",
            "Iteration:  14545      Current loss:  23.746337890625      Current accuracy:  4.0      Comp_time of one batch:  -0.8704013824462891\n",
            "Iteration:  14550      Current loss:  17.283363342285156      Current accuracy:  5.0      Comp_time of one batch:  -0.9043498039245605\n",
            "Iteration:  14555      Current loss:  28.243877410888672      Current accuracy:  5.0      Comp_time of one batch:  -0.8965539932250977\n",
            "Iteration:  14560      Current loss:  20.886001586914062      Current accuracy:  6.0      Comp_time of one batch:  -0.8868396282196045\n",
            "Iteration:  14565      Current loss:  20.699338912963867      Current accuracy:  7.0      Comp_time of one batch:  -0.8988158702850342\n",
            "Iteration:  14570      Current loss:  27.078262329101562      Current accuracy:  7.0      Comp_time of one batch:  -0.8797867298126221\n",
            "Iteration:  14575      Current loss:  26.881954193115234      Current accuracy:  7.0      Comp_time of one batch:  -0.887596607208252\n",
            "Iteration:  14580      Current loss:  23.281070709228516      Current accuracy:  7.0      Comp_time of one batch:  -0.8979291915893555\n",
            "Iteration:  14585      Current loss:  29.775348663330078      Current accuracy:  7.0      Comp_time of one batch:  -0.877185583114624\n",
            "Iteration:  14590      Current loss:  24.588895797729492      Current accuracy:  7.0      Comp_time of one batch:  -0.8839952945709229\n",
            "Iteration:  14595      Current loss:  24.039722442626953      Current accuracy:  8.0      Comp_time of one batch:  -0.8881235122680664\n",
            "Iteration:  14600      Current loss:  24.7103214263916      Current accuracy:  8.0      Comp_time of one batch:  -0.8874063491821289\n",
            "Iteration:  14605      Current loss:  18.752408981323242      Current accuracy:  9.0      Comp_time of one batch:  -0.8809845447540283\n",
            "Iteration:  14610      Current loss:  22.92988395690918      Current accuracy:  9.0      Comp_time of one batch:  -0.880709171295166\n",
            "Iteration:  14615      Current loss:  17.732664108276367      Current accuracy:  10.0      Comp_time of one batch:  -0.8933718204498291\n",
            "Iteration:  14620      Current loss:  31.24168586730957      Current accuracy:  10.0      Comp_time of one batch:  -0.8985645771026611\n",
            "Iteration:  14625      Current loss:  21.39708709716797      Current accuracy:  11.0      Comp_time of one batch:  -0.8785138130187988\n",
            "Iteration:  14630      Current loss:  25.415098190307617      Current accuracy:  11.0      Comp_time of one batch:  -0.8812122344970703\n",
            "Iteration:  14635      Current loss:  21.593814849853516      Current accuracy:  12.0      Comp_time of one batch:  -0.8812060356140137\n",
            "Iteration:  14640      Current loss:  23.75454330444336      Current accuracy:  13.0      Comp_time of one batch:  -0.8819639682769775\n",
            "Iteration:  14645      Current loss:  23.56698226928711      Current accuracy:  13.0      Comp_time of one batch:  -0.8753042221069336\n",
            "Iteration:  14650      Current loss:  21.949668884277344      Current accuracy:  14.0      Comp_time of one batch:  -0.9182500839233398\n",
            "Iteration:  14655      Current loss:  20.765880584716797      Current accuracy:  15.0      Comp_time of one batch:  -0.9307296276092529\n",
            "Iteration:  14660      Current loss:  21.050928115844727      Current accuracy:  16.0      Comp_time of one batch:  -0.9092721939086914\n",
            "Iteration:  14665      Current loss:  29.98980140686035      Current accuracy:  16.0      Comp_time of one batch:  -0.8986368179321289\n",
            "Iteration:  14670      Current loss:  36.37213134765625      Current accuracy:  16.0      Comp_time of one batch:  -0.9103124141693115\n",
            "Iteration:  14675      Current loss:  22.421035766601562      Current accuracy:  17.0      Comp_time of one batch:  -0.8984363079071045\n",
            "Iteration:  14680      Current loss:  16.976890563964844      Current accuracy:  18.0      Comp_time of one batch:  -0.9010913372039795\n",
            "Iteration:  14685      Current loss:  26.67974853515625      Current accuracy:  18.0      Comp_time of one batch:  -0.901904821395874\n",
            "Iteration:  14690      Current loss:  34.273529052734375      Current accuracy:  18.0      Comp_time of one batch:  -0.9031832218170166\n",
            "Iteration:  14695      Current loss:  23.97206687927246      Current accuracy:  18.0      Comp_time of one batch:  -0.8992040157318115\n",
            "Iteration:  14700      Current loss:  31.657001495361328      Current accuracy:  18.0      Comp_time of one batch:  -0.9304883480072021\n",
            "Iteration:  14705      Current loss:  26.484956741333008      Current accuracy:  18.0      Comp_time of one batch:  -0.8983719348907471\n",
            "Iteration:  14710      Current loss:  31.935619354248047      Current accuracy:  18.0      Comp_time of one batch:  -0.9244012832641602\n",
            "Iteration:  14715      Current loss:  30.784557342529297      Current accuracy:  18.0      Comp_time of one batch:  -0.9180996417999268\n",
            "Iteration:  14720      Current loss:  36.14393997192383      Current accuracy:  18.0      Comp_time of one batch:  -0.8822522163391113\n",
            "Iteration:  14725      Current loss:  24.530916213989258      Current accuracy:  18.0      Comp_time of one batch:  -0.897594690322876\n",
            "Iteration:  14730      Current loss:  16.9340877532959      Current accuracy:  19.0      Comp_time of one batch:  -0.8846845626831055\n",
            "Iteration:  14735      Current loss:  31.957862854003906      Current accuracy:  19.0      Comp_time of one batch:  -0.8704097270965576\n",
            "Iteration:  14740      Current loss:  22.173568725585938      Current accuracy:  20.0      Comp_time of one batch:  -0.9026906490325928\n",
            "Iteration:  14745      Current loss:  27.897937774658203      Current accuracy:  20.0      Comp_time of one batch:  -0.8870153427124023\n",
            "Iteration:  14750      Current loss:  20.23516845703125      Current accuracy:  21.0      Comp_time of one batch:  -0.8991210460662842\n",
            "Iteration:  14755      Current loss:  24.189790725708008      Current accuracy:  21.0      Comp_time of one batch:  -0.9231064319610596\n",
            "Iteration:  14760      Current loss:  24.12511444091797      Current accuracy:  21.0      Comp_time of one batch:  -0.8970937728881836\n",
            "Iteration:  14765      Current loss:  21.00540542602539      Current accuracy:  22.0      Comp_time of one batch:  -0.9031591415405273\n",
            "Iteration:  14770      Current loss:  26.0297908782959      Current accuracy:  22.0      Comp_time of one batch:  -0.8947138786315918\n",
            "Iteration:  14775      Current loss:  25.031574249267578      Current accuracy:  22.0      Comp_time of one batch:  -0.886120080947876\n",
            "Iteration:  14780      Current loss:  22.810836791992188      Current accuracy:  23.0      Comp_time of one batch:  -0.9178133010864258\n",
            "Iteration:  14785      Current loss:  25.462360382080078      Current accuracy:  23.0      Comp_time of one batch:  -0.87827467918396\n",
            "Iteration:  14790      Current loss:  27.380050659179688      Current accuracy:  23.0      Comp_time of one batch:  -0.8920538425445557\n",
            "Iteration:  14795      Current loss:  18.17610740661621      Current accuracy:  24.0      Comp_time of one batch:  -0.8918042182922363\n",
            "Iteration:  14800      Current loss:  30.18450355529785      Current accuracy:  24.0      Comp_time of one batch:  -0.8869335651397705\n",
            "Iteration:  14805      Current loss:  27.22807502746582      Current accuracy:  24.0      Comp_time of one batch:  -0.8796818256378174\n",
            "Iteration:  14810      Current loss:  31.26609992980957      Current accuracy:  24.0      Comp_time of one batch:  -0.901721715927124\n",
            "Iteration:  14815      Current loss:  25.39328956604004      Current accuracy:  25.0      Comp_time of one batch:  -0.8820128440856934\n",
            "Iteration:  14820      Current loss:  27.00719451904297      Current accuracy:  25.0      Comp_time of one batch:  -0.9118499755859375\n",
            "Iteration:  14825      Current loss:  14.897371292114258      Current accuracy:  26.0      Comp_time of one batch:  -0.8640143871307373\n",
            "Iteration:  14830      Current loss:  23.639013290405273      Current accuracy:  26.0      Comp_time of one batch:  -0.8766121864318848\n",
            "Iteration:  14835      Current loss:  22.24587631225586      Current accuracy:  26.0      Comp_time of one batch:  -0.8853137493133545\n",
            "Iteration:  14840      Current loss:  29.57343864440918      Current accuracy:  26.0      Comp_time of one batch:  -0.8841378688812256\n",
            "Iteration:  14845      Current loss:  24.516584396362305      Current accuracy:  27.0      Comp_time of one batch:  -0.9088459014892578\n",
            "Iteration:  14850      Current loss:  27.849510192871094      Current accuracy:  27.0      Comp_time of one batch:  -0.8846731185913086\n",
            "Iteration:  14855      Current loss:  26.623777389526367      Current accuracy:  27.0      Comp_time of one batch:  -0.8922858238220215\n",
            "Iteration:  14860      Current loss:  28.703638076782227      Current accuracy:  27.0      Comp_time of one batch:  -0.8936789035797119\n",
            "Iteration:  14865      Current loss:  22.81179428100586      Current accuracy:  28.0      Comp_time of one batch:  -0.893362283706665\n",
            "Iteration:  14870      Current loss:  19.415124893188477      Current accuracy:  29.0      Comp_time of one batch:  -0.887467622756958\n",
            "Iteration:  14875      Current loss:  28.08123016357422      Current accuracy:  29.0      Comp_time of one batch:  -0.9006617069244385\n",
            "Iteration:  14880      Current loss:  20.21139144897461      Current accuracy:  30.0      Comp_time of one batch:  -0.8789770603179932\n",
            "Iteration:  14885      Current loss:  25.93604850769043      Current accuracy:  30.0      Comp_time of one batch:  -0.8775613307952881\n",
            "Iteration:  14890      Current loss:  24.98590660095215      Current accuracy:  30.0      Comp_time of one batch:  -0.8795621395111084\n",
            "Iteration:  14895      Current loss:  25.283042907714844      Current accuracy:  30.0      Comp_time of one batch:  -0.8802323341369629\n",
            "Iteration:  14900      Current loss:  26.50332260131836      Current accuracy:  30.0      Comp_time of one batch:  -0.886354923248291\n",
            "Iteration:  14905      Current loss:  25.797849655151367      Current accuracy:  30.0      Comp_time of one batch:  -0.8803541660308838\n",
            "Iteration:  14910      Current loss:  28.382122039794922      Current accuracy:  30.0      Comp_time of one batch:  -0.8840837478637695\n",
            "Iteration:  14915      Current loss:  19.317302703857422      Current accuracy:  31.0      Comp_time of one batch:  -0.9113662242889404\n",
            "Iteration:  14920      Current loss:  17.17181968688965      Current accuracy:  32.0      Comp_time of one batch:  -0.9000566005706787\n",
            "Iteration:  14925      Current loss:  20.26966094970703      Current accuracy:  33.0      Comp_time of one batch:  -0.8877172470092773\n",
            "Iteration:  14930      Current loss:  20.707399368286133      Current accuracy:  34.0      Comp_time of one batch:  -0.8919520378112793\n",
            "Iteration:  14935      Current loss:  20.262840270996094      Current accuracy:  35.0      Comp_time of one batch:  -0.8809053897857666\n",
            "Iteration:  14940      Current loss:  25.00696563720703      Current accuracy:  35.0      Comp_time of one batch:  -0.8769054412841797\n",
            "Iteration:  14945      Current loss:  21.03369903564453      Current accuracy:  36.0      Comp_time of one batch:  -0.9172718524932861\n",
            "Iteration:  14950      Current loss:  23.420005798339844      Current accuracy:  37.0      Comp_time of one batch:  -0.907900333404541\n",
            "Iteration:  14955      Current loss:  18.587020874023438      Current accuracy:  38.0      Comp_time of one batch:  -0.9079544544219971\n",
            "Iteration:  14960      Current loss:  22.101238250732422      Current accuracy:  39.0      Comp_time of one batch:  -0.8856658935546875\n",
            "Iteration:  14965      Current loss:  21.514305114746094      Current accuracy:  40.0      Comp_time of one batch:  -0.8788225650787354\n",
            "Iteration:  14970      Current loss:  36.89338302612305      Current accuracy:  40.0      Comp_time of one batch:  -0.9130196571350098\n",
            "Iteration:  14975      Current loss:  15.589883804321289      Current accuracy:  41.0      Comp_time of one batch:  -0.8994688987731934\n",
            "Iteration:  14980      Current loss:  17.56633949279785      Current accuracy:  42.0      Comp_time of one batch:  -0.8756113052368164\n",
            "Iteration:  14985      Current loss:  32.233646392822266      Current accuracy:  42.0      Comp_time of one batch:  -0.879796028137207\n",
            "Iteration:  14990      Current loss:  20.539411544799805      Current accuracy:  43.0      Comp_time of one batch:  -0.8866720199584961\n",
            "Iteration:  14995      Current loss:  27.062368392944336      Current accuracy:  43.0      Comp_time of one batch:  -0.8865013122558594\n",
            "Iteration:  15000      Current loss:  30.80520248413086      Current accuracy:  0.0      Comp_time of one batch:  -0.8948249816894531\n",
            "Iteration:  15005      Current loss:  19.206989288330078      Current accuracy:  1.0      Comp_time of one batch:  -0.8889751434326172\n",
            "Iteration:  15010      Current loss:  19.377286911010742      Current accuracy:  2.0      Comp_time of one batch:  -0.8796381950378418\n",
            "Iteration:  15015      Current loss:  26.9252872467041      Current accuracy:  2.0      Comp_time of one batch:  -0.8785562515258789\n",
            "Iteration:  15020      Current loss:  24.424148559570312      Current accuracy:  2.0      Comp_time of one batch:  -0.8809871673583984\n",
            "Iteration:  15025      Current loss:  26.469398498535156      Current accuracy:  2.0      Comp_time of one batch:  -0.8720455169677734\n",
            "Iteration:  15030      Current loss:  21.581298828125      Current accuracy:  3.0      Comp_time of one batch:  -0.8733551502227783\n",
            "Iteration:  15035      Current loss:  22.844890594482422      Current accuracy:  3.0      Comp_time of one batch:  -0.8891167640686035\n",
            "Iteration:  15040      Current loss:  21.372791290283203      Current accuracy:  4.0      Comp_time of one batch:  -0.8810131549835205\n",
            "Iteration:  15045      Current loss:  22.460012435913086      Current accuracy:  4.0      Comp_time of one batch:  -0.8880136013031006\n",
            "Iteration:  15050      Current loss:  21.749454498291016      Current accuracy:  5.0      Comp_time of one batch:  -0.885667085647583\n",
            "Iteration:  15055      Current loss:  24.097389221191406      Current accuracy:  5.0      Comp_time of one batch:  -0.891127347946167\n",
            "Iteration:  15060      Current loss:  21.771970748901367      Current accuracy:  6.0      Comp_time of one batch:  -0.8846797943115234\n",
            "Iteration:  15065      Current loss:  24.542287826538086      Current accuracy:  6.0      Comp_time of one batch:  -0.8814334869384766\n",
            "Iteration:  15070      Current loss:  22.483753204345703      Current accuracy:  6.0      Comp_time of one batch:  -0.8836493492126465\n",
            "Iteration:  15075      Current loss:  20.760107040405273      Current accuracy:  7.0      Comp_time of one batch:  -0.8941233158111572\n",
            "Iteration:  15080      Current loss:  25.353975296020508      Current accuracy:  7.0      Comp_time of one batch:  -0.8992981910705566\n",
            "Iteration:  15085      Current loss:  36.403324127197266      Current accuracy:  7.0      Comp_time of one batch:  -0.8953421115875244\n",
            "Iteration:  15090      Current loss:  20.025903701782227      Current accuracy:  8.0      Comp_time of one batch:  -0.9119384288787842\n",
            "Iteration:  15095      Current loss:  18.868282318115234      Current accuracy:  9.0      Comp_time of one batch:  -0.8880114555358887\n",
            "Iteration:  15100      Current loss:  20.051847457885742      Current accuracy:  10.0      Comp_time of one batch:  -0.8905391693115234\n",
            "Iteration:  15105      Current loss:  16.559389114379883      Current accuracy:  11.0      Comp_time of one batch:  -0.8831868171691895\n",
            "Iteration:  15110      Current loss:  20.420289993286133      Current accuracy:  12.0      Comp_time of one batch:  -0.8800945281982422\n",
            "Iteration:  15115      Current loss:  23.51922035217285      Current accuracy:  12.0      Comp_time of one batch:  -0.8805687427520752\n",
            "Iteration:  15120      Current loss:  27.120725631713867      Current accuracy:  12.0      Comp_time of one batch:  -0.8862812519073486\n",
            "Iteration:  15125      Current loss:  23.428253173828125      Current accuracy:  12.0      Comp_time of one batch:  -0.8926374912261963\n",
            "Iteration:  15130      Current loss:  21.876428604125977      Current accuracy:  13.0      Comp_time of one batch:  -0.8788526058197021\n",
            "Iteration:  15135      Current loss:  20.44715118408203      Current accuracy:  14.0      Comp_time of one batch:  -0.8781952857971191\n",
            "Iteration:  15140      Current loss:  16.25201988220215      Current accuracy:  15.0      Comp_time of one batch:  -0.8922808170318604\n",
            "Iteration:  15145      Current loss:  18.32783317565918      Current accuracy:  16.0      Comp_time of one batch:  -0.8882834911346436\n",
            "Iteration:  15150      Current loss:  25.577617645263672      Current accuracy:  16.0      Comp_time of one batch:  -0.9016883373260498\n",
            "Iteration:  15155      Current loss:  24.872509002685547      Current accuracy:  16.0      Comp_time of one batch:  -0.8988609313964844\n",
            "Iteration:  15160      Current loss:  20.465604782104492      Current accuracy:  17.0      Comp_time of one batch:  -0.9229836463928223\n",
            "Iteration:  15165      Current loss:  21.972496032714844      Current accuracy:  17.0      Comp_time of one batch:  -0.8870446681976318\n",
            "Iteration:  15170      Current loss:  33.4710807800293      Current accuracy:  17.0      Comp_time of one batch:  -0.8931689262390137\n",
            "Iteration:  15175      Current loss:  19.43235206604004      Current accuracy:  18.0      Comp_time of one batch:  -0.8933620452880859\n",
            "Iteration:  15180      Current loss:  21.98432731628418      Current accuracy:  19.0      Comp_time of one batch:  -0.9033589363098145\n",
            "Iteration:  15185      Current loss:  23.316987991333008      Current accuracy:  19.0      Comp_time of one batch:  -0.8885576725006104\n",
            "Iteration:  15190      Current loss:  23.170330047607422      Current accuracy:  19.0      Comp_time of one batch:  -0.9054367542266846\n",
            "Iteration:  15195      Current loss:  23.170711517333984      Current accuracy:  19.0      Comp_time of one batch:  -0.9088764190673828\n",
            "Iteration:  15200      Current loss:  23.985809326171875      Current accuracy:  19.0      Comp_time of one batch:  -0.8754479885101318\n",
            "Iteration:  15205      Current loss:  20.96662712097168      Current accuracy:  20.0      Comp_time of one batch:  -0.8949246406555176\n",
            "Iteration:  15210      Current loss:  30.75636100769043      Current accuracy:  20.0      Comp_time of one batch:  -0.8983340263366699\n",
            "Iteration:  15215      Current loss:  28.196413040161133      Current accuracy:  20.0      Comp_time of one batch:  -0.885056734085083\n",
            "Iteration:  15220      Current loss:  21.79355239868164      Current accuracy:  21.0      Comp_time of one batch:  -0.8887779712677002\n",
            "Iteration:  15225      Current loss:  24.377880096435547      Current accuracy:  22.0      Comp_time of one batch:  -0.9071519374847412\n",
            "Iteration:  15230      Current loss:  30.74493980407715      Current accuracy:  22.0      Comp_time of one batch:  -0.8894522190093994\n",
            "Iteration:  15235      Current loss:  22.93409538269043      Current accuracy:  23.0      Comp_time of one batch:  -0.8779785633087158\n",
            "Iteration:  15240      Current loss:  27.004261016845703      Current accuracy:  23.0      Comp_time of one batch:  -0.8774430751800537\n",
            "Iteration:  15245      Current loss:  26.646329879760742      Current accuracy:  23.0      Comp_time of one batch:  -0.9057629108428955\n",
            "Iteration:  15250      Current loss:  23.273714065551758      Current accuracy:  24.0      Comp_time of one batch:  -0.9045541286468506\n",
            "Iteration:  15255      Current loss:  29.228422164916992      Current accuracy:  24.0      Comp_time of one batch:  -0.9052550792694092\n",
            "Iteration:  15260      Current loss:  20.14452362060547      Current accuracy:  25.0      Comp_time of one batch:  -0.8807864189147949\n",
            "Iteration:  15265      Current loss:  30.005510330200195      Current accuracy:  25.0      Comp_time of one batch:  -0.8887391090393066\n",
            "Iteration:  15270      Current loss:  22.653072357177734      Current accuracy:  26.0      Comp_time of one batch:  -0.8957109451293945\n",
            "Iteration:  15275      Current loss:  22.329240798950195      Current accuracy:  27.0      Comp_time of one batch:  -0.884197473526001\n",
            "Iteration:  15280      Current loss:  18.057640075683594      Current accuracy:  28.0      Comp_time of one batch:  -0.8898591995239258\n",
            "Iteration:  15285      Current loss:  20.97433090209961      Current accuracy:  29.0      Comp_time of one batch:  -0.8819477558135986\n",
            "Iteration:  15290      Current loss:  21.851221084594727      Current accuracy:  30.0      Comp_time of one batch:  -0.8967404365539551\n",
            "Iteration:  15295      Current loss:  31.26364517211914      Current accuracy:  30.0      Comp_time of one batch:  -0.8753740787506104\n",
            "Iteration:  15300      Current loss:  15.389607429504395      Current accuracy:  31.0      Comp_time of one batch:  -0.8980982303619385\n",
            "Iteration:  15305      Current loss:  24.147071838378906      Current accuracy:  31.0      Comp_time of one batch:  -0.888054609298706\n",
            "Iteration:  15310      Current loss:  31.897872924804688      Current accuracy:  31.0      Comp_time of one batch:  -0.8892049789428711\n",
            "Iteration:  15315      Current loss:  19.346769332885742      Current accuracy:  32.0      Comp_time of one batch:  -0.8852810859680176\n",
            "Iteration:  15320      Current loss:  27.011428833007812      Current accuracy:  32.0      Comp_time of one batch:  -0.8819992542266846\n",
            "Iteration:  15325      Current loss:  23.542179107666016      Current accuracy:  32.0      Comp_time of one batch:  -0.8936111927032471\n",
            "Iteration:  15330      Current loss:  28.453903198242188      Current accuracy:  32.0      Comp_time of one batch:  -0.8795998096466064\n",
            "Iteration:  15335      Current loss:  20.268770217895508      Current accuracy:  33.0      Comp_time of one batch:  -0.925175666809082\n",
            "Iteration:  15340      Current loss:  29.238739013671875      Current accuracy:  33.0      Comp_time of one batch:  -0.8902068138122559\n",
            "Iteration:  15345      Current loss:  28.584875106811523      Current accuracy:  33.0      Comp_time of one batch:  -0.8973135948181152\n",
            "Iteration:  15350      Current loss:  21.983474731445312      Current accuracy:  34.0      Comp_time of one batch:  -0.8963286876678467\n",
            "Iteration:  15355      Current loss:  23.904882431030273      Current accuracy:  34.0      Comp_time of one batch:  -0.8798637390136719\n",
            "Iteration:  15360      Current loss:  24.224834442138672      Current accuracy:  34.0      Comp_time of one batch:  -0.8869829177856445\n",
            "Iteration:  15365      Current loss:  23.38558006286621      Current accuracy:  35.0      Comp_time of one batch:  -0.8956239223480225\n",
            "Iteration:  15370      Current loss:  23.640932083129883      Current accuracy:  36.0      Comp_time of one batch:  -0.882601261138916\n",
            "Iteration:  15375      Current loss:  30.665658950805664      Current accuracy:  36.0      Comp_time of one batch:  -0.8884084224700928\n",
            "Iteration:  15380      Current loss:  28.358083724975586      Current accuracy:  36.0      Comp_time of one batch:  -0.8912937641143799\n",
            "Iteration:  15385      Current loss:  16.427474975585938      Current accuracy:  37.0      Comp_time of one batch:  -0.9229495525360107\n",
            "Iteration:  15390      Current loss:  26.061948776245117      Current accuracy:  37.0      Comp_time of one batch:  -0.8999812602996826\n",
            "Iteration:  15395      Current loss:  26.783451080322266      Current accuracy:  37.0      Comp_time of one batch:  -0.8691728115081787\n",
            "Iteration:  15400      Current loss:  24.31092071533203      Current accuracy:  37.0      Comp_time of one batch:  -0.8674201965332031\n",
            "Iteration:  15405      Current loss:  20.16097068786621      Current accuracy:  38.0      Comp_time of one batch:  -0.9020113945007324\n",
            "Iteration:  15410      Current loss:  24.608444213867188      Current accuracy:  38.0      Comp_time of one batch:  -0.8755621910095215\n",
            "Iteration:  15415      Current loss:  22.145498275756836      Current accuracy:  39.0      Comp_time of one batch:  -0.8797512054443359\n",
            "Iteration:  15420      Current loss:  25.413644790649414      Current accuracy:  39.0      Comp_time of one batch:  -0.9009463787078857\n",
            "Iteration:  15425      Current loss:  20.6165828704834      Current accuracy:  40.0      Comp_time of one batch:  -0.8755884170532227\n",
            "Iteration:  15430      Current loss:  20.987947463989258      Current accuracy:  41.0      Comp_time of one batch:  -0.8952126502990723\n",
            "Iteration:  15435      Current loss:  27.463281631469727      Current accuracy:  41.0      Comp_time of one batch:  -0.8925976753234863\n",
            "Iteration:  15440      Current loss:  21.510774612426758      Current accuracy:  42.0      Comp_time of one batch:  -0.8854923248291016\n",
            "Iteration:  15445      Current loss:  27.08415412902832      Current accuracy:  42.0      Comp_time of one batch:  -0.8824045658111572\n",
            "Iteration:  15450      Current loss:  24.747329711914062      Current accuracy:  42.0      Comp_time of one batch:  -0.8976037502288818\n",
            "Iteration:  15455      Current loss:  20.31110954284668      Current accuracy:  43.0      Comp_time of one batch:  -0.9079203605651855\n",
            "Iteration:  15460      Current loss:  20.316364288330078      Current accuracy:  44.0      Comp_time of one batch:  -0.8874571323394775\n",
            "Iteration:  15465      Current loss:  31.53043556213379      Current accuracy:  44.0      Comp_time of one batch:  -0.8824021816253662\n",
            "Iteration:  15470      Current loss:  22.988513946533203      Current accuracy:  45.0      Comp_time of one batch:  -0.883150577545166\n",
            "Iteration:  15475      Current loss:  23.647655487060547      Current accuracy:  45.0      Comp_time of one batch:  -0.8948612213134766\n",
            "Iteration:  15480      Current loss:  21.944068908691406      Current accuracy:  46.0      Comp_time of one batch:  -0.8928971290588379\n",
            "Iteration:  15485      Current loss:  26.149127960205078      Current accuracy:  46.0      Comp_time of one batch:  -0.8834166526794434\n",
            "Iteration:  15490      Current loss:  21.233762741088867      Current accuracy:  47.0      Comp_time of one batch:  -0.8866252899169922\n",
            "Iteration:  15495      Current loss:  24.123546600341797      Current accuracy:  47.0      Comp_time of one batch:  -0.8816084861755371\n",
            "Iteration:  15500      Current loss:  25.724355697631836      Current accuracy:  0.0      Comp_time of one batch:  -0.8830981254577637\n",
            "Iteration:  15505      Current loss:  21.160686492919922      Current accuracy:  1.0      Comp_time of one batch:  -0.8819108009338379\n",
            "Iteration:  15510      Current loss:  21.829893112182617      Current accuracy:  2.0      Comp_time of one batch:  -0.9013030529022217\n",
            "Iteration:  15515      Current loss:  31.75933837890625      Current accuracy:  2.0      Comp_time of one batch:  -0.9147372245788574\n",
            "Iteration:  15520      Current loss:  25.623254776000977      Current accuracy:  2.0      Comp_time of one batch:  -0.8821501731872559\n",
            "Iteration:  15525      Current loss:  24.036073684692383      Current accuracy:  2.0      Comp_time of one batch:  -0.9170026779174805\n",
            "Iteration:  15530      Current loss:  18.808317184448242      Current accuracy:  3.0      Comp_time of one batch:  -0.9331467151641846\n",
            "Iteration:  15535      Current loss:  29.672361373901367      Current accuracy:  3.0      Comp_time of one batch:  -0.8759317398071289\n",
            "Iteration:  15540      Current loss:  21.1563777923584      Current accuracy:  4.0      Comp_time of one batch:  -0.8914408683776855\n",
            "Iteration:  15545      Current loss:  27.307117462158203      Current accuracy:  4.0      Comp_time of one batch:  -0.9256842136383057\n",
            "Iteration:  15550      Current loss:  22.42011833190918      Current accuracy:  5.0      Comp_time of one batch:  -0.892017126083374\n",
            "Iteration:  15555      Current loss:  23.458660125732422      Current accuracy:  6.0      Comp_time of one batch:  -0.894127607345581\n",
            "Iteration:  15560      Current loss:  19.275333404541016      Current accuracy:  7.0      Comp_time of one batch:  -0.9078927040100098\n",
            "Iteration:  15565      Current loss:  23.144018173217773      Current accuracy:  8.0      Comp_time of one batch:  -0.8883824348449707\n",
            "Iteration:  15570      Current loss:  17.532306671142578      Current accuracy:  9.0      Comp_time of one batch:  -0.9033851623535156\n",
            "Iteration:  15575      Current loss:  26.60370445251465      Current accuracy:  9.0      Comp_time of one batch:  -0.9067869186401367\n",
            "Iteration:  15580      Current loss:  24.23967933654785      Current accuracy:  9.0      Comp_time of one batch:  -0.9041180610656738\n",
            "Iteration:  15585      Current loss:  22.452966690063477      Current accuracy:  10.0      Comp_time of one batch:  -0.8859655857086182\n",
            "Iteration:  15590      Current loss:  35.17779541015625      Current accuracy:  10.0      Comp_time of one batch:  -0.9339950084686279\n",
            "Iteration:  15595      Current loss:  27.48263931274414      Current accuracy:  10.0      Comp_time of one batch:  -0.9107956886291504\n",
            "Iteration:  15600      Current loss:  40.68484115600586      Current accuracy:  10.0      Comp_time of one batch:  -0.8945696353912354\n",
            "Iteration:  15605      Current loss:  33.5332145690918      Current accuracy:  10.0      Comp_time of one batch:  -0.8931467533111572\n",
            "Iteration:  15610      Current loss:  24.794824600219727      Current accuracy:  10.0      Comp_time of one batch:  -0.8750195503234863\n",
            "Iteration:  15615      Current loss:  22.1695613861084      Current accuracy:  10.0      Comp_time of one batch:  -0.9267752170562744\n",
            "Iteration:  15620      Current loss:  28.809860229492188      Current accuracy:  10.0      Comp_time of one batch:  -0.8835139274597168\n",
            "Iteration:  15625      Current loss:  24.661056518554688      Current accuracy:  10.0      Comp_time of one batch:  -0.8878960609436035\n",
            "Iteration:  15630      Current loss:  21.280406951904297      Current accuracy:  11.0      Comp_time of one batch:  -0.9022636413574219\n",
            "Iteration:  15635      Current loss:  22.6108455657959      Current accuracy:  12.0      Comp_time of one batch:  -0.8829817771911621\n",
            "Iteration:  15640      Current loss:  20.197431564331055      Current accuracy:  13.0      Comp_time of one batch:  -0.8760280609130859\n",
            "Iteration:  15645      Current loss:  31.214702606201172      Current accuracy:  13.0      Comp_time of one batch:  -0.8932812213897705\n",
            "Iteration:  15650      Current loss:  25.04741096496582      Current accuracy:  13.0      Comp_time of one batch:  -0.9097013473510742\n",
            "Iteration:  15655      Current loss:  22.212942123413086      Current accuracy:  14.0      Comp_time of one batch:  -0.8932340145111084\n",
            "Iteration:  15660      Current loss:  29.11829376220703      Current accuracy:  14.0      Comp_time of one batch:  -0.8814396858215332\n",
            "Iteration:  15665      Current loss:  26.506410598754883      Current accuracy:  14.0      Comp_time of one batch:  -0.9003636837005615\n",
            "Iteration:  15670      Current loss:  22.98940086364746      Current accuracy:  14.0      Comp_time of one batch:  -0.8930466175079346\n",
            "Iteration:  15675      Current loss:  32.21720504760742      Current accuracy:  14.0      Comp_time of one batch:  -0.9027607440948486\n",
            "Iteration:  15680      Current loss:  24.827457427978516      Current accuracy:  14.0      Comp_time of one batch:  -0.870368242263794\n",
            "Iteration:  15685      Current loss:  29.15650177001953      Current accuracy:  14.0      Comp_time of one batch:  -0.8908250331878662\n",
            "Iteration:  15690      Current loss:  30.5946102142334      Current accuracy:  14.0      Comp_time of one batch:  -0.9104087352752686\n",
            "Iteration:  15695      Current loss:  19.91569709777832      Current accuracy:  15.0      Comp_time of one batch:  -0.8817296028137207\n",
            "Iteration:  15700      Current loss:  31.70233726501465      Current accuracy:  15.0      Comp_time of one batch:  -0.8982226848602295\n",
            "Iteration:  15705      Current loss:  27.25924301147461      Current accuracy:  15.0      Comp_time of one batch:  -0.8896288871765137\n",
            "Iteration:  15710      Current loss:  28.23556900024414      Current accuracy:  15.0      Comp_time of one batch:  -0.8944127559661865\n",
            "Iteration:  15715      Current loss:  21.57241439819336      Current accuracy:  16.0      Comp_time of one batch:  -0.9148862361907959\n",
            "Iteration:  15720      Current loss:  26.407703399658203      Current accuracy:  16.0      Comp_time of one batch:  -0.8724861145019531\n",
            "Iteration:  15725      Current loss:  17.170677185058594      Current accuracy:  17.0      Comp_time of one batch:  -0.8918590545654297\n",
            "Iteration:  15730      Current loss:  27.359209060668945      Current accuracy:  17.0      Comp_time of one batch:  -0.9158318042755127\n",
            "Iteration:  15735      Current loss:  21.0175838470459      Current accuracy:  18.0      Comp_time of one batch:  -0.8893754482269287\n",
            "Iteration:  15740      Current loss:  29.666078567504883      Current accuracy:  18.0      Comp_time of one batch:  -0.8838591575622559\n",
            "Iteration:  15745      Current loss:  26.806819915771484      Current accuracy:  18.0      Comp_time of one batch:  -0.915297269821167\n",
            "Iteration:  15750      Current loss:  21.25043296813965      Current accuracy:  19.0      Comp_time of one batch:  -0.8714830875396729\n",
            "Iteration:  15755      Current loss:  23.12288475036621      Current accuracy:  19.0      Comp_time of one batch:  -0.9034423828125\n",
            "Iteration:  15760      Current loss:  28.115947723388672      Current accuracy:  19.0      Comp_time of one batch:  -0.9046893119812012\n",
            "Iteration:  15765      Current loss:  25.053253173828125      Current accuracy:  19.0      Comp_time of one batch:  -0.8794496059417725\n",
            "Iteration:  15770      Current loss:  22.956653594970703      Current accuracy:  20.0      Comp_time of one batch:  -0.8771100044250488\n",
            "Iteration:  15775      Current loss:  18.634037017822266      Current accuracy:  21.0      Comp_time of one batch:  -0.898338794708252\n",
            "Iteration:  15780      Current loss:  21.68623924255371      Current accuracy:  22.0      Comp_time of one batch:  -0.8869879245758057\n",
            "Iteration:  15785      Current loss:  19.675352096557617      Current accuracy:  23.0      Comp_time of one batch:  -0.9091706275939941\n",
            "Iteration:  15790      Current loss:  20.072444915771484      Current accuracy:  24.0      Comp_time of one batch:  -0.9007840156555176\n",
            "Iteration:  15795      Current loss:  35.105411529541016      Current accuracy:  24.0      Comp_time of one batch:  -0.9013051986694336\n",
            "Iteration:  15800      Current loss:  37.496009826660156      Current accuracy:  24.0      Comp_time of one batch:  -0.9098544120788574\n",
            "Iteration:  15805      Current loss:  27.747987747192383      Current accuracy:  24.0      Comp_time of one batch:  -0.8852486610412598\n",
            "Iteration:  15810      Current loss:  26.874818801879883      Current accuracy:  24.0      Comp_time of one batch:  -0.8886086940765381\n",
            "Iteration:  15815      Current loss:  23.623062133789062      Current accuracy:  24.0      Comp_time of one batch:  -0.9052882194519043\n",
            "Iteration:  15820      Current loss:  27.5661563873291      Current accuracy:  24.0      Comp_time of one batch:  -0.9030075073242188\n",
            "Iteration:  15825      Current loss:  24.62904167175293      Current accuracy:  24.0      Comp_time of one batch:  -0.8730130195617676\n",
            "Iteration:  15830      Current loss:  22.414981842041016      Current accuracy:  25.0      Comp_time of one batch:  -0.9080734252929688\n",
            "Iteration:  15835      Current loss:  22.107704162597656      Current accuracy:  26.0      Comp_time of one batch:  -0.8967084884643555\n",
            "Iteration:  15840      Current loss:  25.524471282958984      Current accuracy:  26.0      Comp_time of one batch:  -0.8891642093658447\n",
            "Iteration:  15845      Current loss:  21.00897789001465      Current accuracy:  27.0      Comp_time of one batch:  -0.9198832511901855\n",
            "Iteration:  15850      Current loss:  24.00223731994629      Current accuracy:  28.0      Comp_time of one batch:  -0.8862912654876709\n",
            "Iteration:  15855      Current loss:  21.048046112060547      Current accuracy:  29.0      Comp_time of one batch:  -0.885939359664917\n",
            "Iteration:  15860      Current loss:  21.609071731567383      Current accuracy:  30.0      Comp_time of one batch:  -0.8969299793243408\n",
            "Iteration:  15865      Current loss:  22.247486114501953      Current accuracy:  31.0      Comp_time of one batch:  -0.8956184387207031\n",
            "Iteration:  15870      Current loss:  24.035968780517578      Current accuracy:  31.0      Comp_time of one batch:  -0.9025650024414062\n",
            "Iteration:  15875      Current loss:  26.610013961791992      Current accuracy:  31.0      Comp_time of one batch:  -0.874049186706543\n",
            "Iteration:  15880      Current loss:  26.532794952392578      Current accuracy:  31.0      Comp_time of one batch:  -0.892449140548706\n",
            "Iteration:  15885      Current loss:  27.224430084228516      Current accuracy:  31.0      Comp_time of one batch:  -0.8817458152770996\n",
            "Iteration:  15890      Current loss:  23.00459098815918      Current accuracy:  32.0      Comp_time of one batch:  -0.9196150302886963\n",
            "Iteration:  15895      Current loss:  23.11473274230957      Current accuracy:  32.0      Comp_time of one batch:  -0.8826031684875488\n",
            "Iteration:  15900      Current loss:  18.76964569091797      Current accuracy:  33.0      Comp_time of one batch:  -0.9055187702178955\n",
            "Iteration:  15905      Current loss:  16.80858612060547      Current accuracy:  34.0      Comp_time of one batch:  -0.8899118900299072\n",
            "Iteration:  15910      Current loss:  15.073760032653809      Current accuracy:  35.0      Comp_time of one batch:  -0.8782644271850586\n",
            "Iteration:  15915      Current loss:  20.386537551879883      Current accuracy:  36.0      Comp_time of one batch:  -0.8987977504730225\n",
            "Iteration:  15920      Current loss:  23.431671142578125      Current accuracy:  36.0      Comp_time of one batch:  -0.8860359191894531\n",
            "Iteration:  15925      Current loss:  17.224586486816406      Current accuracy:  37.0      Comp_time of one batch:  -0.8934228420257568\n",
            "Iteration:  15930      Current loss:  22.784093856811523      Current accuracy:  37.0      Comp_time of one batch:  -0.8932850360870361\n",
            "Iteration:  15935      Current loss:  29.171133041381836      Current accuracy:  37.0      Comp_time of one batch:  -0.8773102760314941\n",
            "Iteration:  15940      Current loss:  20.43103790283203      Current accuracy:  38.0      Comp_time of one batch:  -0.8832786083221436\n",
            "Iteration:  15945      Current loss:  24.442964553833008      Current accuracy:  38.0      Comp_time of one batch:  -0.8782994747161865\n",
            "Iteration:  15950      Current loss:  34.77846908569336      Current accuracy:  38.0      Comp_time of one batch:  -0.8760924339294434\n",
            "Iteration:  15955      Current loss:  25.146312713623047      Current accuracy:  38.0      Comp_time of one batch:  -0.8871560096740723\n",
            "Iteration:  15960      Current loss:  30.825410842895508      Current accuracy:  38.0      Comp_time of one batch:  -0.8905148506164551\n",
            "Iteration:  15965      Current loss:  24.331003189086914      Current accuracy:  38.0      Comp_time of one batch:  -0.8864293098449707\n",
            "Iteration:  15970      Current loss:  21.28081512451172      Current accuracy:  39.0      Comp_time of one batch:  -0.8905324935913086\n",
            "Iteration:  15975      Current loss:  25.3582763671875      Current accuracy:  40.0      Comp_time of one batch:  -0.9049320220947266\n",
            "Iteration:  15980      Current loss:  28.9360408782959      Current accuracy:  40.0      Comp_time of one batch:  -0.8710048198699951\n",
            "Iteration:  15985      Current loss:  24.911102294921875      Current accuracy:  40.0      Comp_time of one batch:  -0.8778483867645264\n",
            "Iteration:  15990      Current loss:  21.134986877441406      Current accuracy:  41.0      Comp_time of one batch:  -0.896275520324707\n",
            "Iteration:  15995      Current loss:  23.462982177734375      Current accuracy:  41.0      Comp_time of one batch:  -0.9169330596923828\n",
            "Iteration:  16000      Current loss:  24.03407096862793      Current accuracy:  0.0      Comp_time of one batch:  -0.8777527809143066\n",
            "Iteration:  16005      Current loss:  26.797000885009766      Current accuracy:  0.0      Comp_time of one batch:  -0.8705449104309082\n",
            "Iteration:  16010      Current loss:  17.10901641845703      Current accuracy:  1.0      Comp_time of one batch:  -0.8787920475006104\n",
            "Iteration:  16015      Current loss:  26.655805587768555      Current accuracy:  1.0      Comp_time of one batch:  -0.8898942470550537\n",
            "Iteration:  16020      Current loss:  21.424266815185547      Current accuracy:  2.0      Comp_time of one batch:  -0.8834784030914307\n",
            "Iteration:  16025      Current loss:  14.634221076965332      Current accuracy:  3.0      Comp_time of one batch:  -0.8875877857208252\n",
            "Iteration:  16030      Current loss:  31.283435821533203      Current accuracy:  3.0      Comp_time of one batch:  -0.8883931636810303\n",
            "Iteration:  16035      Current loss:  18.724822998046875      Current accuracy:  4.0      Comp_time of one batch:  -0.8810179233551025\n",
            "Iteration:  16040      Current loss:  27.96100425720215      Current accuracy:  4.0      Comp_time of one batch:  -0.8901419639587402\n",
            "Iteration:  16045      Current loss:  36.406036376953125      Current accuracy:  4.0      Comp_time of one batch:  -0.879324197769165\n",
            "Iteration:  16050      Current loss:  25.84902000427246      Current accuracy:  4.0      Comp_time of one batch:  -0.9003303050994873\n",
            "Iteration:  16055      Current loss:  25.884323120117188      Current accuracy:  4.0      Comp_time of one batch:  -0.8935608863830566\n",
            "Iteration:  16060      Current loss:  26.24838638305664      Current accuracy:  4.0      Comp_time of one batch:  -0.8892204761505127\n",
            "Iteration:  16065      Current loss:  19.366077423095703      Current accuracy:  5.0      Comp_time of one batch:  -0.8819196224212646\n",
            "Iteration:  16070      Current loss:  23.092342376708984      Current accuracy:  6.0      Comp_time of one batch:  -0.876115083694458\n",
            "Iteration:  16075      Current loss:  26.112810134887695      Current accuracy:  6.0      Comp_time of one batch:  -0.890362024307251\n",
            "Iteration:  16080      Current loss:  39.973846435546875      Current accuracy:  6.0      Comp_time of one batch:  -0.9083924293518066\n",
            "Iteration:  16085      Current loss:  24.529497146606445      Current accuracy:  7.0      Comp_time of one batch:  -0.8732995986938477\n",
            "Iteration:  16090      Current loss:  21.725658416748047      Current accuracy:  8.0      Comp_time of one batch:  -0.9189481735229492\n",
            "Iteration:  16095      Current loss:  19.660558700561523      Current accuracy:  9.0      Comp_time of one batch:  -0.9167895317077637\n",
            "Iteration:  16100      Current loss:  30.892478942871094      Current accuracy:  9.0      Comp_time of one batch:  -0.8814165592193604\n",
            "Iteration:  16105      Current loss:  35.23872375488281      Current accuracy:  9.0      Comp_time of one batch:  -0.8869771957397461\n",
            "Iteration:  16110      Current loss:  24.13433837890625      Current accuracy:  9.0      Comp_time of one batch:  -0.8980112075805664\n",
            "Iteration:  16115      Current loss:  25.661109924316406      Current accuracy:  9.0      Comp_time of one batch:  -0.9007515907287598\n",
            "Iteration:  16120      Current loss:  34.36762237548828      Current accuracy:  9.0      Comp_time of one batch:  -0.8854708671569824\n",
            "Iteration:  16125      Current loss:  22.427827835083008      Current accuracy:  10.0      Comp_time of one batch:  -0.907167911529541\n",
            "Iteration:  16130      Current loss:  25.507741928100586      Current accuracy:  10.0      Comp_time of one batch:  -0.8774535655975342\n",
            "Iteration:  16135      Current loss:  19.028383255004883      Current accuracy:  11.0      Comp_time of one batch:  -0.9141385555267334\n",
            "Iteration:  16140      Current loss:  18.1007080078125      Current accuracy:  12.0      Comp_time of one batch:  -0.9310071468353271\n",
            "Iteration:  16145      Current loss:  32.53035354614258      Current accuracy:  12.0      Comp_time of one batch:  -0.8818168640136719\n",
            "Iteration:  16150      Current loss:  24.497892379760742      Current accuracy:  12.0      Comp_time of one batch:  -0.9020247459411621\n",
            "Iteration:  16155      Current loss:  21.00812339782715      Current accuracy:  13.0      Comp_time of one batch:  -0.876213550567627\n",
            "Iteration:  16160      Current loss:  19.32424545288086      Current accuracy:  14.0      Comp_time of one batch:  -0.8796331882476807\n",
            "Iteration:  16165      Current loss:  25.193763732910156      Current accuracy:  14.0      Comp_time of one batch:  -0.8812038898468018\n",
            "Iteration:  16170      Current loss:  19.83072280883789      Current accuracy:  15.0      Comp_time of one batch:  -0.882622480392456\n",
            "Iteration:  16175      Current loss:  26.36558723449707      Current accuracy:  15.0      Comp_time of one batch:  -0.8859257698059082\n",
            "Iteration:  16180      Current loss:  17.72968292236328      Current accuracy:  16.0      Comp_time of one batch:  -0.9357709884643555\n",
            "Iteration:  16185      Current loss:  27.400962829589844      Current accuracy:  16.0      Comp_time of one batch:  -0.9181418418884277\n",
            "Iteration:  16190      Current loss:  22.19857406616211      Current accuracy:  17.0      Comp_time of one batch:  -0.8979198932647705\n",
            "Iteration:  16195      Current loss:  24.16285514831543      Current accuracy:  17.0      Comp_time of one batch:  -0.8887202739715576\n",
            "Iteration:  16200      Current loss:  19.481800079345703      Current accuracy:  18.0      Comp_time of one batch:  -0.9070935249328613\n",
            "Iteration:  16205      Current loss:  21.80425262451172      Current accuracy:  19.0      Comp_time of one batch:  -0.9056248664855957\n",
            "Iteration:  16210      Current loss:  20.629060745239258      Current accuracy:  20.0      Comp_time of one batch:  -0.8903088569641113\n",
            "Iteration:  16215      Current loss:  33.309104919433594      Current accuracy:  20.0      Comp_time of one batch:  -0.8785264492034912\n",
            "Iteration:  16220      Current loss:  20.66910171508789      Current accuracy:  21.0      Comp_time of one batch:  -0.8743495941162109\n",
            "Iteration:  16225      Current loss:  19.1868839263916      Current accuracy:  22.0      Comp_time of one batch:  -0.9064953327178955\n",
            "Iteration:  16230      Current loss:  23.65776824951172      Current accuracy:  23.0      Comp_time of one batch:  -0.9020035266876221\n",
            "Iteration:  16235      Current loss:  24.98936653137207      Current accuracy:  23.0      Comp_time of one batch:  -0.8924698829650879\n",
            "Iteration:  16240      Current loss:  39.618160247802734      Current accuracy:  23.0      Comp_time of one batch:  -0.911865234375\n",
            "Iteration:  16245      Current loss:  30.140514373779297      Current accuracy:  23.0      Comp_time of one batch:  -0.8943150043487549\n",
            "Iteration:  16250      Current loss:  22.25786781311035      Current accuracy:  24.0      Comp_time of one batch:  -0.8950691223144531\n",
            "Iteration:  16255      Current loss:  32.83675003051758      Current accuracy:  24.0      Comp_time of one batch:  -0.8727097511291504\n",
            "Iteration:  16260      Current loss:  24.571849822998047      Current accuracy:  25.0      Comp_time of one batch:  -0.9155585765838623\n",
            "Iteration:  16265      Current loss:  20.67973518371582      Current accuracy:  26.0      Comp_time of one batch:  -0.8720448017120361\n",
            "Iteration:  16270      Current loss:  22.98060417175293      Current accuracy:  27.0      Comp_time of one batch:  -0.9042947292327881\n",
            "Iteration:  16275      Current loss:  33.68305969238281      Current accuracy:  27.0      Comp_time of one batch:  -0.8907699584960938\n",
            "Iteration:  16280      Current loss:  34.57362365722656      Current accuracy:  27.0      Comp_time of one batch:  -0.8780443668365479\n",
            "Iteration:  16285      Current loss:  18.797590255737305      Current accuracy:  28.0      Comp_time of one batch:  -0.8710525035858154\n",
            "Iteration:  16290      Current loss:  33.22648239135742      Current accuracy:  28.0      Comp_time of one batch:  -0.8748841285705566\n",
            "Iteration:  16295      Current loss:  20.60654067993164      Current accuracy:  29.0      Comp_time of one batch:  -0.9194586277008057\n",
            "Iteration:  16300      Current loss:  29.9085693359375      Current accuracy:  29.0      Comp_time of one batch:  -0.9142065048217773\n",
            "Iteration:  16305      Current loss:  27.80389976501465      Current accuracy:  29.0      Comp_time of one batch:  -0.8838467597961426\n",
            "Iteration:  16310      Current loss:  37.55242919921875      Current accuracy:  29.0      Comp_time of one batch:  -0.8893780708312988\n",
            "Iteration:  16315      Current loss:  21.79267692565918      Current accuracy:  30.0      Comp_time of one batch:  -0.8871088027954102\n",
            "Iteration:  16320      Current loss:  33.49481201171875      Current accuracy:  30.0      Comp_time of one batch:  -0.8785781860351562\n",
            "Iteration:  16325      Current loss:  19.22327423095703      Current accuracy:  31.0      Comp_time of one batch:  -0.9078526496887207\n",
            "Iteration:  16330      Current loss:  32.98166275024414      Current accuracy:  31.0      Comp_time of one batch:  -0.9016458988189697\n",
            "Iteration:  16335      Current loss:  40.433406829833984      Current accuracy:  31.0      Comp_time of one batch:  -0.8992273807525635\n",
            "Iteration:  16340      Current loss:  23.254467010498047      Current accuracy:  31.0      Comp_time of one batch:  -0.8801627159118652\n",
            "Iteration:  16345      Current loss:  20.263442993164062      Current accuracy:  32.0      Comp_time of one batch:  -0.8811864852905273\n",
            "Iteration:  16350      Current loss:  25.296979904174805      Current accuracy:  32.0      Comp_time of one batch:  -0.8941857814788818\n",
            "Iteration:  16355      Current loss:  18.60017204284668      Current accuracy:  33.0      Comp_time of one batch:  -0.8938541412353516\n",
            "Iteration:  16360      Current loss:  18.797666549682617      Current accuracy:  34.0      Comp_time of one batch:  -0.8858957290649414\n",
            "Iteration:  16365      Current loss:  22.269515991210938      Current accuracy:  35.0      Comp_time of one batch:  -0.8815929889678955\n",
            "Iteration:  16370      Current loss:  28.082202911376953      Current accuracy:  35.0      Comp_time of one batch:  -0.8832838535308838\n",
            "Iteration:  16375      Current loss:  30.976346969604492      Current accuracy:  35.0      Comp_time of one batch:  -0.9070732593536377\n",
            "Iteration:  16380      Current loss:  20.646120071411133      Current accuracy:  36.0      Comp_time of one batch:  -0.9057333469390869\n",
            "Iteration:  16385      Current loss:  20.209152221679688      Current accuracy:  37.0      Comp_time of one batch:  -0.8813285827636719\n",
            "Iteration:  16390      Current loss:  25.776132583618164      Current accuracy:  37.0      Comp_time of one batch:  -0.899115800857544\n",
            "Iteration:  16395      Current loss:  19.261348724365234      Current accuracy:  38.0      Comp_time of one batch:  -0.8876268863677979\n",
            "Iteration:  16400      Current loss:  29.08587074279785      Current accuracy:  38.0      Comp_time of one batch:  -0.9093372821807861\n",
            "Iteration:  16405      Current loss:  26.509376525878906      Current accuracy:  38.0      Comp_time of one batch:  -0.9123506546020508\n",
            "Iteration:  16410      Current loss:  23.208797454833984      Current accuracy:  38.0      Comp_time of one batch:  -0.8917262554168701\n",
            "Iteration:  16415      Current loss:  24.10158348083496      Current accuracy:  38.0      Comp_time of one batch:  -0.9135982990264893\n",
            "Iteration:  16420      Current loss:  22.030908584594727      Current accuracy:  39.0      Comp_time of one batch:  -0.8811075687408447\n",
            "Iteration:  16425      Current loss:  22.827226638793945      Current accuracy:  40.0      Comp_time of one batch:  -0.8853497505187988\n",
            "Iteration:  16430      Current loss:  18.61621856689453      Current accuracy:  41.0      Comp_time of one batch:  -0.8806462287902832\n",
            "Iteration:  16435      Current loss:  30.13738441467285      Current accuracy:  41.0      Comp_time of one batch:  -0.8793017864227295\n",
            "Iteration:  16440      Current loss:  21.171586990356445      Current accuracy:  42.0      Comp_time of one batch:  -0.8941302299499512\n",
            "Iteration:  16445      Current loss:  19.99118423461914      Current accuracy:  43.0      Comp_time of one batch:  -0.8836870193481445\n",
            "Iteration:  16450      Current loss:  18.570390701293945      Current accuracy:  44.0      Comp_time of one batch:  -0.8829038143157959\n",
            "Iteration:  16455      Current loss:  36.5460090637207      Current accuracy:  44.0      Comp_time of one batch:  -0.8841824531555176\n",
            "Iteration:  16460      Current loss:  20.951343536376953      Current accuracy:  45.0      Comp_time of one batch:  -0.8763728141784668\n",
            "Iteration:  16465      Current loss:  18.097898483276367      Current accuracy:  46.0      Comp_time of one batch:  -0.8781700134277344\n",
            "Iteration:  16470      Current loss:  21.117931365966797      Current accuracy:  47.0      Comp_time of one batch:  -0.878849983215332\n",
            "Iteration:  16475      Current loss:  24.31024932861328      Current accuracy:  47.0      Comp_time of one batch:  -0.9062609672546387\n",
            "Iteration:  16480      Current loss:  28.283029556274414      Current accuracy:  47.0      Comp_time of one batch:  -0.8977949619293213\n",
            "Iteration:  16485      Current loss:  26.393089294433594      Current accuracy:  47.0      Comp_time of one batch:  -0.907872200012207\n",
            "Iteration:  16490      Current loss:  22.217397689819336      Current accuracy:  48.0      Comp_time of one batch:  -0.9051785469055176\n",
            "Iteration:  16495      Current loss:  23.332902908325195      Current accuracy:  48.0      Comp_time of one batch:  -0.8803210258483887\n",
            "Iteration:  16500      Current loss:  21.060802459716797      Current accuracy:  0.0      Comp_time of one batch:  -0.8740346431732178\n",
            "Iteration:  16505      Current loss:  23.809457778930664      Current accuracy:  0.0      Comp_time of one batch:  -0.9099912643432617\n",
            "Iteration:  16510      Current loss:  18.00833511352539      Current accuracy:  1.0      Comp_time of one batch:  -0.9136643409729004\n",
            "Iteration:  16515      Current loss:  22.971885681152344      Current accuracy:  2.0      Comp_time of one batch:  -0.9196319580078125\n",
            "Iteration:  16520      Current loss:  23.106786727905273      Current accuracy:  2.0      Comp_time of one batch:  -0.8989162445068359\n",
            "Iteration:  16525      Current loss:  19.159147262573242      Current accuracy:  3.0      Comp_time of one batch:  -0.9051496982574463\n",
            "Iteration:  16530      Current loss:  31.774110794067383      Current accuracy:  3.0      Comp_time of one batch:  -0.8827905654907227\n",
            "Iteration:  16535      Current loss:  24.795703887939453      Current accuracy:  3.0      Comp_time of one batch:  -0.8848650455474854\n",
            "Iteration:  16540      Current loss:  31.44843864440918      Current accuracy:  3.0      Comp_time of one batch:  -0.9038259983062744\n",
            "Iteration:  16545      Current loss:  42.175819396972656      Current accuracy:  3.0      Comp_time of one batch:  -0.8781321048736572\n",
            "Iteration:  16550      Current loss:  24.028736114501953      Current accuracy:  3.0      Comp_time of one batch:  -0.9090008735656738\n",
            "Iteration:  16555      Current loss:  29.092479705810547      Current accuracy:  3.0      Comp_time of one batch:  -0.9057269096374512\n",
            "Iteration:  16560      Current loss:  27.176788330078125      Current accuracy:  3.0      Comp_time of one batch:  -0.9214379787445068\n",
            "Iteration:  16565      Current loss:  22.570209503173828      Current accuracy:  4.0      Comp_time of one batch:  -0.913459300994873\n",
            "Iteration:  16570      Current loss:  23.388490676879883      Current accuracy:  4.0      Comp_time of one batch:  -0.8772008419036865\n",
            "Iteration:  16575      Current loss:  19.82852554321289      Current accuracy:  5.0      Comp_time of one batch:  -0.8894572257995605\n",
            "Iteration:  16580      Current loss:  23.52311897277832      Current accuracy:  6.0      Comp_time of one batch:  -0.9311628341674805\n",
            "Iteration:  16585      Current loss:  30.96849250793457      Current accuracy:  6.0      Comp_time of one batch:  -0.8803551197052002\n",
            "Iteration:  16590      Current loss:  19.528989791870117      Current accuracy:  7.0      Comp_time of one batch:  -0.9064154624938965\n",
            "Iteration:  16595      Current loss:  27.928001403808594      Current accuracy:  7.0      Comp_time of one batch:  -0.8836767673492432\n",
            "Iteration:  16600      Current loss:  18.690135955810547      Current accuracy:  8.0      Comp_time of one batch:  -0.9287936687469482\n",
            "Iteration:  16605      Current loss:  23.76754379272461      Current accuracy:  8.0      Comp_time of one batch:  -0.9310698509216309\n",
            "Iteration:  16610      Current loss:  20.022275924682617      Current accuracy:  9.0      Comp_time of one batch:  -0.9244585037231445\n",
            "Iteration:  16615      Current loss:  25.383947372436523      Current accuracy:  9.0      Comp_time of one batch:  -0.9121747016906738\n",
            "Iteration:  16620      Current loss:  18.5631103515625      Current accuracy:  10.0      Comp_time of one batch:  -0.9087343215942383\n",
            "Iteration:  16625      Current loss:  17.823650360107422      Current accuracy:  11.0      Comp_time of one batch:  -0.8719737529754639\n",
            "Iteration:  16630      Current loss:  32.62855911254883      Current accuracy:  11.0      Comp_time of one batch:  -0.9130866527557373\n",
            "Iteration:  16635      Current loss:  18.479598999023438      Current accuracy:  12.0      Comp_time of one batch:  -0.9339783191680908\n",
            "Iteration:  16640      Current loss:  25.2938232421875      Current accuracy:  13.0      Comp_time of one batch:  -0.9184606075286865\n",
            "Iteration:  16645      Current loss:  22.18571662902832      Current accuracy:  14.0      Comp_time of one batch:  -0.8878161907196045\n",
            "Iteration:  16650      Current loss:  26.086223602294922      Current accuracy:  14.0      Comp_time of one batch:  -0.8720388412475586\n",
            "Iteration:  16655      Current loss:  19.081132888793945      Current accuracy:  15.0      Comp_time of one batch:  -0.9158828258514404\n",
            "Iteration:  16660      Current loss:  27.57206916809082      Current accuracy:  15.0      Comp_time of one batch:  -0.8947169780731201\n",
            "Iteration:  16665      Current loss:  29.368566513061523      Current accuracy:  15.0      Comp_time of one batch:  -0.8743040561676025\n",
            "Iteration:  16670      Current loss:  22.485811233520508      Current accuracy:  16.0      Comp_time of one batch:  -0.8728420734405518\n",
            "Iteration:  16675      Current loss:  25.20212745666504      Current accuracy:  16.0      Comp_time of one batch:  -0.8909063339233398\n",
            "Iteration:  16680      Current loss:  20.426956176757812      Current accuracy:  17.0      Comp_time of one batch:  -0.8818695545196533\n",
            "Iteration:  16685      Current loss:  34.21647262573242      Current accuracy:  17.0      Comp_time of one batch:  -0.8875505924224854\n",
            "Iteration:  16690      Current loss:  23.177709579467773      Current accuracy:  17.0      Comp_time of one batch:  -0.9118454456329346\n",
            "Iteration:  16695      Current loss:  24.16529083251953      Current accuracy:  17.0      Comp_time of one batch:  -0.8863499164581299\n",
            "Iteration:  16700      Current loss:  24.22926139831543      Current accuracy:  18.0      Comp_time of one batch:  -0.8989596366882324\n",
            "Iteration:  16705      Current loss:  27.934965133666992      Current accuracy:  18.0      Comp_time of one batch:  -0.8881185054779053\n",
            "Iteration:  16710      Current loss:  24.349882125854492      Current accuracy:  19.0      Comp_time of one batch:  -0.8681704998016357\n",
            "Iteration:  16715      Current loss:  24.481433868408203      Current accuracy:  19.0      Comp_time of one batch:  -0.9122486114501953\n",
            "Iteration:  16720      Current loss:  28.76215934753418      Current accuracy:  19.0      Comp_time of one batch:  -0.8952031135559082\n",
            "Iteration:  16725      Current loss:  22.748743057250977      Current accuracy:  19.0      Comp_time of one batch:  -0.9217917919158936\n",
            "Iteration:  16730      Current loss:  26.37136459350586      Current accuracy:  19.0      Comp_time of one batch:  -0.8965609073638916\n",
            "Iteration:  16735      Current loss:  25.403385162353516      Current accuracy:  19.0      Comp_time of one batch:  -0.914207935333252\n",
            "Iteration:  16740      Current loss:  23.05673599243164      Current accuracy:  20.0      Comp_time of one batch:  -0.8933923244476318\n",
            "Iteration:  16745      Current loss:  19.03278160095215      Current accuracy:  21.0      Comp_time of one batch:  -0.8842611312866211\n",
            "Iteration:  16750      Current loss:  25.62668228149414      Current accuracy:  21.0      Comp_time of one batch:  -0.9139809608459473\n",
            "Iteration:  16755      Current loss:  20.722196578979492      Current accuracy:  22.0      Comp_time of one batch:  -0.8695652484893799\n",
            "Iteration:  16760      Current loss:  29.835887908935547      Current accuracy:  22.0      Comp_time of one batch:  -0.8745877742767334\n",
            "Iteration:  16765      Current loss:  33.30292510986328      Current accuracy:  22.0      Comp_time of one batch:  -0.885382890701294\n",
            "Iteration:  16770      Current loss:  35.418121337890625      Current accuracy:  22.0      Comp_time of one batch:  -0.878636360168457\n",
            "Iteration:  16775      Current loss:  24.615596771240234      Current accuracy:  22.0      Comp_time of one batch:  -0.8877952098846436\n",
            "Iteration:  16780      Current loss:  19.616806030273438      Current accuracy:  23.0      Comp_time of one batch:  -0.8794372081756592\n",
            "Iteration:  16785      Current loss:  27.604734420776367      Current accuracy:  23.0      Comp_time of one batch:  -0.8797228336334229\n",
            "Iteration:  16790      Current loss:  39.274658203125      Current accuracy:  23.0      Comp_time of one batch:  -0.9082574844360352\n",
            "Iteration:  16795      Current loss:  24.369138717651367      Current accuracy:  23.0      Comp_time of one batch:  -0.8890738487243652\n",
            "Iteration:  16800      Current loss:  26.496374130249023      Current accuracy:  23.0      Comp_time of one batch:  -0.8820891380310059\n",
            "Iteration:  16805      Current loss:  25.845943450927734      Current accuracy:  23.0      Comp_time of one batch:  -0.8766481876373291\n",
            "Iteration:  16810      Current loss:  26.793725967407227      Current accuracy:  23.0      Comp_time of one batch:  -0.8870205879211426\n",
            "Iteration:  16815      Current loss:  21.10856819152832      Current accuracy:  24.0      Comp_time of one batch:  -0.8877623081207275\n",
            "Iteration:  16820      Current loss:  21.487701416015625      Current accuracy:  25.0      Comp_time of one batch:  -0.8776118755340576\n",
            "Iteration:  16825      Current loss:  22.982446670532227      Current accuracy:  25.0      Comp_time of one batch:  -0.8843367099761963\n",
            "Iteration:  16830      Current loss:  21.228055953979492      Current accuracy:  26.0      Comp_time of one batch:  -0.8912527561187744\n",
            "Iteration:  16835      Current loss:  20.279455184936523      Current accuracy:  27.0      Comp_time of one batch:  -0.8853394985198975\n",
            "Iteration:  16840      Current loss:  26.915634155273438      Current accuracy:  27.0      Comp_time of one batch:  -0.8806798458099365\n",
            "Iteration:  16845      Current loss:  21.85883140563965      Current accuracy:  28.0      Comp_time of one batch:  -0.8925590515136719\n",
            "Iteration:  16850      Current loss:  19.348560333251953      Current accuracy:  29.0      Comp_time of one batch:  -0.9042661190032959\n",
            "Iteration:  16855      Current loss:  20.050844192504883      Current accuracy:  30.0      Comp_time of one batch:  -0.8845462799072266\n",
            "Iteration:  16860      Current loss:  23.409387588500977      Current accuracy:  30.0      Comp_time of one batch:  -0.9076015949249268\n",
            "Iteration:  16865      Current loss:  24.07413101196289      Current accuracy:  30.0      Comp_time of one batch:  -0.9045228958129883\n",
            "Iteration:  16870      Current loss:  16.44481086730957      Current accuracy:  31.0      Comp_time of one batch:  -0.8931310176849365\n",
            "Iteration:  16875      Current loss:  27.196670532226562      Current accuracy:  31.0      Comp_time of one batch:  -0.9095776081085205\n",
            "Iteration:  16880      Current loss:  20.76010513305664      Current accuracy:  32.0      Comp_time of one batch:  -0.8719933032989502\n",
            "Iteration:  16885      Current loss:  18.2222900390625      Current accuracy:  33.0      Comp_time of one batch:  -0.9031872749328613\n",
            "Iteration:  16890      Current loss:  24.05636978149414      Current accuracy:  33.0      Comp_time of one batch:  -0.8842334747314453\n",
            "Iteration:  16895      Current loss:  22.767141342163086      Current accuracy:  34.0      Comp_time of one batch:  -0.8904435634613037\n",
            "Iteration:  16900      Current loss:  23.74662971496582      Current accuracy:  35.0      Comp_time of one batch:  -0.9073746204376221\n",
            "Iteration:  16905      Current loss:  26.233964920043945      Current accuracy:  35.0      Comp_time of one batch:  -0.9114463329315186\n",
            "Iteration:  16910      Current loss:  23.808263778686523      Current accuracy:  35.0      Comp_time of one batch:  -0.8865132331848145\n",
            "Iteration:  16915      Current loss:  23.0728759765625      Current accuracy:  35.0      Comp_time of one batch:  -0.9064276218414307\n",
            "Iteration:  16920      Current loss:  24.336671829223633      Current accuracy:  35.0      Comp_time of one batch:  -0.8857405185699463\n",
            "Iteration:  16925      Current loss:  21.758602142333984      Current accuracy:  36.0      Comp_time of one batch:  -0.8901352882385254\n",
            "Iteration:  16930      Current loss:  23.65418815612793      Current accuracy:  37.0      Comp_time of one batch:  -0.9000763893127441\n",
            "Iteration:  16935      Current loss:  24.819236755371094      Current accuracy:  37.0      Comp_time of one batch:  -0.919480562210083\n",
            "Iteration:  16940      Current loss:  18.9157657623291      Current accuracy:  38.0      Comp_time of one batch:  -0.8954124450683594\n",
            "Iteration:  16945      Current loss:  30.509580612182617      Current accuracy:  38.0      Comp_time of one batch:  -0.9328553676605225\n",
            "Iteration:  16950      Current loss:  22.756515502929688      Current accuracy:  38.0      Comp_time of one batch:  -0.8827028274536133\n",
            "Iteration:  16955      Current loss:  22.101314544677734      Current accuracy:  39.0      Comp_time of one batch:  -0.8791046142578125\n",
            "Iteration:  16960      Current loss:  19.569726943969727      Current accuracy:  40.0      Comp_time of one batch:  -0.9184966087341309\n",
            "Iteration:  16965      Current loss:  21.2271671295166      Current accuracy:  41.0      Comp_time of one batch:  -0.895500659942627\n",
            "Iteration:  16970      Current loss:  21.215707778930664      Current accuracy:  42.0      Comp_time of one batch:  -0.8953711986541748\n",
            "Iteration:  16975      Current loss:  30.767601013183594      Current accuracy:  42.0      Comp_time of one batch:  -0.8831188678741455\n",
            "Iteration:  16980      Current loss:  24.729597091674805      Current accuracy:  42.0      Comp_time of one batch:  -0.9257783889770508\n",
            "Iteration:  16985      Current loss:  32.679935455322266      Current accuracy:  42.0      Comp_time of one batch:  -0.8811862468719482\n",
            "Iteration:  16990      Current loss:  24.817646026611328      Current accuracy:  42.0      Comp_time of one batch:  -0.8823287487030029\n",
            "Iteration:  16995      Current loss:  20.945341110229492      Current accuracy:  43.0      Comp_time of one batch:  -0.8953356742858887\n",
            "Iteration:  17000      Current loss:  21.693958282470703      Current accuracy:  0.0      Comp_time of one batch:  -0.934638500213623\n",
            "Iteration:  17005      Current loss:  33.48307418823242      Current accuracy:  0.0      Comp_time of one batch:  -0.9096500873565674\n",
            "Iteration:  17010      Current loss:  23.290401458740234      Current accuracy:  1.0      Comp_time of one batch:  -0.8815591335296631\n",
            "Iteration:  17015      Current loss:  28.48123550415039      Current accuracy:  1.0      Comp_time of one batch:  -0.8786313533782959\n",
            "Iteration:  17020      Current loss:  28.45557403564453      Current accuracy:  1.0      Comp_time of one batch:  -0.8839902877807617\n",
            "Iteration:  17025      Current loss:  28.08696746826172      Current accuracy:  1.0      Comp_time of one batch:  -0.9120876789093018\n",
            "Iteration:  17030      Current loss:  22.203794479370117      Current accuracy:  2.0      Comp_time of one batch:  -0.8756933212280273\n",
            "Iteration:  17035      Current loss:  23.92112159729004      Current accuracy:  2.0      Comp_time of one batch:  -0.9122180938720703\n",
            "Iteration:  17040      Current loss:  21.158885955810547      Current accuracy:  3.0      Comp_time of one batch:  -0.887183427810669\n",
            "Iteration:  17045      Current loss:  21.93084716796875      Current accuracy:  4.0      Comp_time of one batch:  -0.9157378673553467\n",
            "Iteration:  17050      Current loss:  19.171741485595703      Current accuracy:  5.0      Comp_time of one batch:  -0.8819825649261475\n",
            "Iteration:  17055      Current loss:  20.227306365966797      Current accuracy:  6.0      Comp_time of one batch:  -0.8928802013397217\n",
            "Iteration:  17060      Current loss:  24.57729148864746      Current accuracy:  6.0      Comp_time of one batch:  -0.8810539245605469\n",
            "Iteration:  17065      Current loss:  34.38643264770508      Current accuracy:  6.0      Comp_time of one batch:  -0.8798038959503174\n",
            "Iteration:  17070      Current loss:  18.58180809020996      Current accuracy:  7.0      Comp_time of one batch:  -0.8878486156463623\n",
            "Iteration:  17075      Current loss:  29.210336685180664      Current accuracy:  7.0      Comp_time of one batch:  -0.8968920707702637\n",
            "Iteration:  17080      Current loss:  25.451404571533203      Current accuracy:  8.0      Comp_time of one batch:  -0.9126040935516357\n",
            "Iteration:  17085      Current loss:  25.61304473876953      Current accuracy:  8.0      Comp_time of one batch:  -0.8999128341674805\n",
            "Iteration:  17090      Current loss:  21.87249183654785      Current accuracy:  9.0      Comp_time of one batch:  -0.9250421524047852\n",
            "Iteration:  17095      Current loss:  17.13523292541504      Current accuracy:  10.0      Comp_time of one batch:  -0.9015460014343262\n",
            "Iteration:  17100      Current loss:  18.440351486206055      Current accuracy:  11.0      Comp_time of one batch:  -0.8966708183288574\n",
            "Iteration:  17105      Current loss:  21.008283615112305      Current accuracy:  12.0      Comp_time of one batch:  -0.879246711730957\n",
            "Iteration:  17110      Current loss:  26.02690315246582      Current accuracy:  12.0      Comp_time of one batch:  -0.8844003677368164\n",
            "Iteration:  17115      Current loss:  45.806236267089844      Current accuracy:  12.0      Comp_time of one batch:  -0.9001283645629883\n",
            "Iteration:  17120      Current loss:  38.1578483581543      Current accuracy:  12.0      Comp_time of one batch:  -0.8740143775939941\n",
            "Iteration:  17125      Current loss:  20.996723175048828      Current accuracy:  13.0      Comp_time of one batch:  -0.8760592937469482\n",
            "Iteration:  17130      Current loss:  24.414087295532227      Current accuracy:  13.0      Comp_time of one batch:  -0.8902690410614014\n",
            "Iteration:  17135      Current loss:  29.685546875      Current accuracy:  13.0      Comp_time of one batch:  -0.904240608215332\n",
            "Iteration:  17140      Current loss:  19.45355224609375      Current accuracy:  14.0      Comp_time of one batch:  -0.8788015842437744\n",
            "Iteration:  17145      Current loss:  23.595457077026367      Current accuracy:  15.0      Comp_time of one batch:  -0.8761305809020996\n",
            "Iteration:  17150      Current loss:  25.42293357849121      Current accuracy:  15.0      Comp_time of one batch:  -0.9163756370544434\n",
            "Iteration:  17155      Current loss:  27.503786087036133      Current accuracy:  15.0      Comp_time of one batch:  -0.8903756141662598\n",
            "Iteration:  17160      Current loss:  34.72873306274414      Current accuracy:  15.0      Comp_time of one batch:  -0.9221620559692383\n",
            "Iteration:  17165      Current loss:  22.05264663696289      Current accuracy:  16.0      Comp_time of one batch:  -0.899796724319458\n",
            "Iteration:  17170      Current loss:  21.667612075805664      Current accuracy:  17.0      Comp_time of one batch:  -0.8868813514709473\n",
            "Iteration:  17175      Current loss:  21.003833770751953      Current accuracy:  18.0      Comp_time of one batch:  -0.8766601085662842\n",
            "Iteration:  17180      Current loss:  22.55759620666504      Current accuracy:  19.0      Comp_time of one batch:  -0.8797295093536377\n",
            "Iteration:  17185      Current loss:  20.770381927490234      Current accuracy:  20.0      Comp_time of one batch:  -0.9035210609436035\n",
            "Iteration:  17190      Current loss:  23.58127212524414      Current accuracy:  20.0      Comp_time of one batch:  -0.9095087051391602\n",
            "Iteration:  17195      Current loss:  22.014524459838867      Current accuracy:  21.0      Comp_time of one batch:  -0.9055054187774658\n",
            "Iteration:  17200      Current loss:  23.973602294921875      Current accuracy:  21.0      Comp_time of one batch:  -0.8912560939788818\n",
            "Iteration:  17205      Current loss:  20.519447326660156      Current accuracy:  22.0      Comp_time of one batch:  -0.8863167762756348\n",
            "Iteration:  17210      Current loss:  25.419790267944336      Current accuracy:  22.0      Comp_time of one batch:  -0.8869662284851074\n",
            "Iteration:  17215      Current loss:  27.72650718688965      Current accuracy:  22.0      Comp_time of one batch:  -0.878368616104126\n",
            "Iteration:  17220      Current loss:  15.412601470947266      Current accuracy:  23.0      Comp_time of one batch:  -0.8862380981445312\n",
            "Iteration:  17225      Current loss:  18.76496696472168      Current accuracy:  24.0      Comp_time of one batch:  -0.8837978839874268\n",
            "Iteration:  17230      Current loss:  20.15349578857422      Current accuracy:  25.0      Comp_time of one batch:  -0.8980271816253662\n",
            "Iteration:  17235      Current loss:  20.158615112304688      Current accuracy:  26.0      Comp_time of one batch:  -0.8958690166473389\n",
            "Iteration:  17240      Current loss:  22.96445083618164      Current accuracy:  27.0      Comp_time of one batch:  -0.8976659774780273\n",
            "Iteration:  17245      Current loss:  25.02012825012207      Current accuracy:  27.0      Comp_time of one batch:  -0.8810603618621826\n",
            "Iteration:  17250      Current loss:  26.678913116455078      Current accuracy:  27.0      Comp_time of one batch:  -0.8791787624359131\n",
            "Iteration:  17255      Current loss:  20.06385040283203      Current accuracy:  28.0      Comp_time of one batch:  -0.904000997543335\n",
            "Iteration:  17260      Current loss:  24.053239822387695      Current accuracy:  28.0      Comp_time of one batch:  -0.887864351272583\n",
            "Iteration:  17265      Current loss:  27.193693161010742      Current accuracy:  28.0      Comp_time of one batch:  -0.8817191123962402\n",
            "Iteration:  17270      Current loss:  26.533979415893555      Current accuracy:  28.0      Comp_time of one batch:  -0.8945848941802979\n",
            "Iteration:  17275      Current loss:  21.80143165588379      Current accuracy:  29.0      Comp_time of one batch:  -0.9026329517364502\n",
            "Iteration:  17280      Current loss:  25.691579818725586      Current accuracy:  29.0      Comp_time of one batch:  -0.8900268077850342\n",
            "Iteration:  17285      Current loss:  29.90081787109375      Current accuracy:  29.0      Comp_time of one batch:  -0.9044954776763916\n",
            "Iteration:  17290      Current loss:  31.688920974731445      Current accuracy:  29.0      Comp_time of one batch:  -0.8902878761291504\n",
            "Iteration:  17295      Current loss:  30.196874618530273      Current accuracy:  29.0      Comp_time of one batch:  -0.9088876247406006\n",
            "Iteration:  17300      Current loss:  33.62149429321289      Current accuracy:  29.0      Comp_time of one batch:  -0.8840785026550293\n",
            "Iteration:  17305      Current loss:  24.552261352539062      Current accuracy:  30.0      Comp_time of one batch:  -0.88889479637146\n",
            "Iteration:  17310      Current loss:  19.019729614257812      Current accuracy:  31.0      Comp_time of one batch:  -0.8988363742828369\n",
            "Iteration:  17315      Current loss:  20.648460388183594      Current accuracy:  32.0      Comp_time of one batch:  -0.8682658672332764\n",
            "Iteration:  17320      Current loss:  18.01104164123535      Current accuracy:  33.0      Comp_time of one batch:  -0.8806138038635254\n",
            "Iteration:  17325      Current loss:  22.222219467163086      Current accuracy:  34.0      Comp_time of one batch:  -0.8820128440856934\n",
            "Iteration:  17330      Current loss:  24.983179092407227      Current accuracy:  34.0      Comp_time of one batch:  -0.903278112411499\n",
            "Iteration:  17335      Current loss:  25.70053482055664      Current accuracy:  34.0      Comp_time of one batch:  -0.9093418121337891\n",
            "Iteration:  17340      Current loss:  17.498058319091797      Current accuracy:  35.0      Comp_time of one batch:  -0.8834595680236816\n",
            "Iteration:  17345      Current loss:  18.246139526367188      Current accuracy:  36.0      Comp_time of one batch:  -0.9058411121368408\n",
            "Iteration:  17350      Current loss:  25.381074905395508      Current accuracy:  36.0      Comp_time of one batch:  -0.8861808776855469\n",
            "Iteration:  17355      Current loss:  28.446115493774414      Current accuracy:  36.0      Comp_time of one batch:  -0.9020586013793945\n",
            "Iteration:  17360      Current loss:  23.44403076171875      Current accuracy:  37.0      Comp_time of one batch:  -0.8761296272277832\n",
            "Iteration:  17365      Current loss:  17.83385467529297      Current accuracy:  38.0      Comp_time of one batch:  -0.8855457305908203\n",
            "Iteration:  17370      Current loss:  29.803918838500977      Current accuracy:  38.0      Comp_time of one batch:  -0.9161808490753174\n",
            "Iteration:  17375      Current loss:  36.76945114135742      Current accuracy:  38.0      Comp_time of one batch:  -0.9154396057128906\n",
            "Iteration:  17380      Current loss:  22.89542007446289      Current accuracy:  38.0      Comp_time of one batch:  -0.9098246097564697\n",
            "Iteration:  17385      Current loss:  24.602100372314453      Current accuracy:  39.0      Comp_time of one batch:  -0.8957271575927734\n",
            "Iteration:  17390      Current loss:  19.936382293701172      Current accuracy:  40.0      Comp_time of one batch:  -0.935788631439209\n",
            "Iteration:  17395      Current loss:  23.9243221282959      Current accuracy:  41.0      Comp_time of one batch:  -0.9303481578826904\n",
            "Iteration:  17400      Current loss:  25.008882522583008      Current accuracy:  41.0      Comp_time of one batch:  -0.8727071285247803\n",
            "Iteration:  17405      Current loss:  22.705625534057617      Current accuracy:  42.0      Comp_time of one batch:  -0.9054272174835205\n",
            "Iteration:  17410      Current loss:  21.82237434387207      Current accuracy:  43.0      Comp_time of one batch:  -0.8791744709014893\n",
            "Iteration:  17415      Current loss:  24.24097442626953      Current accuracy:  43.0      Comp_time of one batch:  -0.882396936416626\n",
            "Iteration:  17420      Current loss:  23.721080780029297      Current accuracy:  44.0      Comp_time of one batch:  -0.8814735412597656\n",
            "Iteration:  17425      Current loss:  28.971792221069336      Current accuracy:  44.0      Comp_time of one batch:  -0.8777656555175781\n",
            "Iteration:  17430      Current loss:  31.76947021484375      Current accuracy:  44.0      Comp_time of one batch:  -0.8927652835845947\n",
            "Iteration:  17435      Current loss:  31.427274703979492      Current accuracy:  44.0      Comp_time of one batch:  -0.899083137512207\n",
            "Iteration:  17440      Current loss:  27.3407039642334      Current accuracy:  44.0      Comp_time of one batch:  -0.9038164615631104\n",
            "Iteration:  17445      Current loss:  20.860082626342773      Current accuracy:  45.0      Comp_time of one batch:  -0.8792548179626465\n",
            "Iteration:  17450      Current loss:  25.766265869140625      Current accuracy:  45.0      Comp_time of one batch:  -0.8874373435974121\n",
            "Iteration:  17455      Current loss:  24.412670135498047      Current accuracy:  45.0      Comp_time of one batch:  -0.8989026546478271\n",
            "Iteration:  17460      Current loss:  33.28831100463867      Current accuracy:  45.0      Comp_time of one batch:  -0.9002549648284912\n",
            "Iteration:  17465      Current loss:  26.934980392456055      Current accuracy:  46.0      Comp_time of one batch:  -0.8937551975250244\n",
            "Iteration:  17470      Current loss:  30.404638290405273      Current accuracy:  46.0      Comp_time of one batch:  -0.9098773002624512\n",
            "Iteration:  17475      Current loss:  26.40336799621582      Current accuracy:  46.0      Comp_time of one batch:  -0.870060920715332\n",
            "Iteration:  17480      Current loss:  23.319164276123047      Current accuracy:  47.0      Comp_time of one batch:  -0.8878195285797119\n",
            "Iteration:  17485      Current loss:  26.58201026916504      Current accuracy:  47.0      Comp_time of one batch:  -0.9280297756195068\n",
            "Iteration:  17490      Current loss:  20.24574089050293      Current accuracy:  48.0      Comp_time of one batch:  -0.8897807598114014\n",
            "Iteration:  17495      Current loss:  19.769622802734375      Current accuracy:  49.0      Comp_time of one batch:  -0.8900201320648193\n",
            "Iteration:  17500      Current loss:  16.092321395874023      Current accuracy:  0.0      Comp_time of one batch:  -0.8697474002838135\n",
            "Iteration:  17505      Current loss:  21.653024673461914      Current accuracy:  1.0      Comp_time of one batch:  -0.9007511138916016\n",
            "Iteration:  17510      Current loss:  23.06785011291504      Current accuracy:  1.0      Comp_time of one batch:  -0.8869972229003906\n",
            "Iteration:  17515      Current loss:  19.826189041137695      Current accuracy:  2.0      Comp_time of one batch:  -0.8784973621368408\n",
            "Iteration:  17520      Current loss:  19.601919174194336      Current accuracy:  3.0      Comp_time of one batch:  -0.9103689193725586\n",
            "Iteration:  17525      Current loss:  29.356491088867188      Current accuracy:  3.0      Comp_time of one batch:  -0.8826687335968018\n",
            "Iteration:  17530      Current loss:  22.539268493652344      Current accuracy:  4.0      Comp_time of one batch:  -0.8825783729553223\n",
            "Iteration:  17535      Current loss:  24.73567008972168      Current accuracy:  4.0      Comp_time of one batch:  -0.9045450687408447\n",
            "Iteration:  17540      Current loss:  23.698938369750977      Current accuracy:  4.0      Comp_time of one batch:  -0.8855686187744141\n",
            "Iteration:  17545      Current loss:  19.832286834716797      Current accuracy:  5.0      Comp_time of one batch:  -0.9117603302001953\n",
            "Iteration:  17550      Current loss:  26.027164459228516      Current accuracy:  5.0      Comp_time of one batch:  -0.9113366603851318\n",
            "Iteration:  17555      Current loss:  21.67447280883789      Current accuracy:  6.0      Comp_time of one batch:  -0.8853824138641357\n",
            "Iteration:  17560      Current loss:  23.683319091796875      Current accuracy:  7.0      Comp_time of one batch:  -0.8836076259613037\n",
            "Iteration:  17565      Current loss:  29.355403900146484      Current accuracy:  7.0      Comp_time of one batch:  -0.9150278568267822\n",
            "Iteration:  17570      Current loss:  19.03645133972168      Current accuracy:  8.0      Comp_time of one batch:  -0.8805387020111084\n",
            "Iteration:  17575      Current loss:  19.587099075317383      Current accuracy:  9.0      Comp_time of one batch:  -0.8991150856018066\n",
            "Iteration:  17580      Current loss:  24.287784576416016      Current accuracy:  9.0      Comp_time of one batch:  -0.9216916561126709\n",
            "Iteration:  17585      Current loss:  25.985570907592773      Current accuracy:  9.0      Comp_time of one batch:  -0.8804550170898438\n",
            "Iteration:  17590      Current loss:  26.068925857543945      Current accuracy:  9.0      Comp_time of one batch:  -0.8855733871459961\n",
            "Iteration:  17595      Current loss:  42.576961517333984      Current accuracy:  9.0      Comp_time of one batch:  -0.8871784210205078\n",
            "Iteration:  17600      Current loss:  20.62397575378418      Current accuracy:  10.0      Comp_time of one batch:  -0.8922388553619385\n",
            "Iteration:  17605      Current loss:  22.239931106567383      Current accuracy:  11.0      Comp_time of one batch:  -0.8907320499420166\n",
            "Iteration:  17610      Current loss:  19.7377986907959      Current accuracy:  12.0      Comp_time of one batch:  -0.8994290828704834\n",
            "Iteration:  17615      Current loss:  18.828386306762695      Current accuracy:  13.0      Comp_time of one batch:  -0.8892309665679932\n",
            "Iteration:  17620      Current loss:  18.249412536621094      Current accuracy:  14.0      Comp_time of one batch:  -0.8882777690887451\n",
            "Iteration:  17625      Current loss:  23.655794143676758      Current accuracy:  14.0      Comp_time of one batch:  -0.8976891040802002\n",
            "Iteration:  17630      Current loss:  29.932729721069336      Current accuracy:  14.0      Comp_time of one batch:  -0.9052093029022217\n",
            "Iteration:  17635      Current loss:  18.156944274902344      Current accuracy:  15.0      Comp_time of one batch:  -0.9011328220367432\n",
            "Iteration:  17640      Current loss:  21.08212661743164      Current accuracy:  16.0      Comp_time of one batch:  -0.9157075881958008\n",
            "Iteration:  17645      Current loss:  27.23942756652832      Current accuracy:  16.0      Comp_time of one batch:  -0.8924298286437988\n",
            "Iteration:  17650      Current loss:  23.894195556640625      Current accuracy:  16.0      Comp_time of one batch:  -0.8911654949188232\n",
            "Iteration:  17655      Current loss:  18.79344367980957      Current accuracy:  17.0      Comp_time of one batch:  -0.9029195308685303\n",
            "Iteration:  17660      Current loss:  18.627111434936523      Current accuracy:  18.0      Comp_time of one batch:  -0.8984355926513672\n",
            "Iteration:  17665      Current loss:  25.883209228515625      Current accuracy:  18.0      Comp_time of one batch:  -0.8753490447998047\n",
            "Iteration:  17670      Current loss:  23.126636505126953      Current accuracy:  19.0      Comp_time of one batch:  -0.8840208053588867\n",
            "Iteration:  17675      Current loss:  21.954538345336914      Current accuracy:  20.0      Comp_time of one batch:  -0.8970601558685303\n",
            "Iteration:  17680      Current loss:  28.154558181762695      Current accuracy:  20.0      Comp_time of one batch:  -0.8780293464660645\n",
            "Iteration:  17685      Current loss:  22.704708099365234      Current accuracy:  21.0      Comp_time of one batch:  -0.8882725238800049\n",
            "Iteration:  17690      Current loss:  18.60732650756836      Current accuracy:  22.0      Comp_time of one batch:  -0.9083061218261719\n",
            "Iteration:  17695      Current loss:  32.5269660949707      Current accuracy:  22.0      Comp_time of one batch:  -0.883558988571167\n",
            "Iteration:  17700      Current loss:  21.438247680664062      Current accuracy:  23.0      Comp_time of one batch:  -0.8731949329376221\n",
            "Iteration:  17705      Current loss:  24.225757598876953      Current accuracy:  23.0      Comp_time of one batch:  -0.8684689998626709\n",
            "Iteration:  17710      Current loss:  24.333925247192383      Current accuracy:  23.0      Comp_time of one batch:  -0.8982393741607666\n",
            "Iteration:  17715      Current loss:  24.013946533203125      Current accuracy:  23.0      Comp_time of one batch:  -0.8814737796783447\n",
            "Iteration:  17720      Current loss:  36.093875885009766      Current accuracy:  23.0      Comp_time of one batch:  -0.88338303565979\n",
            "Iteration:  17725      Current loss:  27.93607521057129      Current accuracy:  23.0      Comp_time of one batch:  -0.8973362445831299\n",
            "Iteration:  17730      Current loss:  28.01442527770996      Current accuracy:  23.0      Comp_time of one batch:  -0.8866186141967773\n",
            "Iteration:  17735      Current loss:  33.58796310424805      Current accuracy:  23.0      Comp_time of one batch:  -0.8817234039306641\n",
            "Iteration:  17740      Current loss:  29.23332405090332      Current accuracy:  23.0      Comp_time of one batch:  -0.8855540752410889\n",
            "Iteration:  17745      Current loss:  23.22129249572754      Current accuracy:  23.0      Comp_time of one batch:  -0.8856065273284912\n",
            "Iteration:  17750      Current loss:  28.703182220458984      Current accuracy:  23.0      Comp_time of one batch:  -0.9077320098876953\n",
            "Iteration:  17755      Current loss:  27.00603675842285      Current accuracy:  23.0      Comp_time of one batch:  -0.8787400722503662\n",
            "Iteration:  17760      Current loss:  21.490718841552734      Current accuracy:  24.0      Comp_time of one batch:  -0.9007785320281982\n",
            "Iteration:  17765      Current loss:  24.963058471679688      Current accuracy:  24.0      Comp_time of one batch:  -0.8991820812225342\n",
            "Iteration:  17770      Current loss:  28.211061477661133      Current accuracy:  24.0      Comp_time of one batch:  -0.9134068489074707\n",
            "Iteration:  17775      Current loss:  18.699325561523438      Current accuracy:  25.0      Comp_time of one batch:  -0.9169294834136963\n",
            "Iteration:  17780      Current loss:  26.18681526184082      Current accuracy:  25.0      Comp_time of one batch:  -0.9047396183013916\n",
            "Iteration:  17785      Current loss:  26.42093849182129      Current accuracy:  25.0      Comp_time of one batch:  -0.8960797786712646\n",
            "Iteration:  17790      Current loss:  27.341859817504883      Current accuracy:  25.0      Comp_time of one batch:  -0.887826681137085\n",
            "Iteration:  17795      Current loss:  22.179555892944336      Current accuracy:  26.0      Comp_time of one batch:  -0.9266560077667236\n",
            "Iteration:  17800      Current loss:  24.48412322998047      Current accuracy:  26.0      Comp_time of one batch:  -0.9354689121246338\n",
            "Iteration:  17805      Current loss:  23.882118225097656      Current accuracy:  26.0      Comp_time of one batch:  -0.877216100692749\n",
            "Iteration:  17810      Current loss:  24.982343673706055      Current accuracy:  26.0      Comp_time of one batch:  -0.9318122863769531\n",
            "Iteration:  17815      Current loss:  13.332033157348633      Current accuracy:  27.0      Comp_time of one batch:  -0.8901984691619873\n",
            "Iteration:  17820      Current loss:  22.48365020751953      Current accuracy:  28.0      Comp_time of one batch:  -0.8910183906555176\n",
            "Iteration:  17825      Current loss:  21.23963737487793      Current accuracy:  29.0      Comp_time of one batch:  -0.8967459201812744\n",
            "Iteration:  17830      Current loss:  27.96689224243164      Current accuracy:  29.0      Comp_time of one batch:  -0.8889942169189453\n",
            "Iteration:  17835      Current loss:  19.439233779907227      Current accuracy:  30.0      Comp_time of one batch:  -0.9210572242736816\n",
            "Iteration:  17840      Current loss:  18.809810638427734      Current accuracy:  31.0      Comp_time of one batch:  -0.9004850387573242\n",
            "Iteration:  17845      Current loss:  16.23798370361328      Current accuracy:  32.0      Comp_time of one batch:  -0.8781566619873047\n",
            "Iteration:  17850      Current loss:  23.24820899963379      Current accuracy:  33.0      Comp_time of one batch:  -0.89634108543396\n",
            "Iteration:  17855      Current loss:  34.77170181274414      Current accuracy:  33.0      Comp_time of one batch:  -0.8750996589660645\n",
            "Iteration:  17860      Current loss:  17.20921516418457      Current accuracy:  34.0      Comp_time of one batch:  -0.8778901100158691\n",
            "Iteration:  17865      Current loss:  31.697383880615234      Current accuracy:  34.0      Comp_time of one batch:  -0.8843252658843994\n",
            "Iteration:  17870      Current loss:  19.25023078918457      Current accuracy:  35.0      Comp_time of one batch:  -0.8791191577911377\n",
            "Iteration:  17875      Current loss:  19.75627899169922      Current accuracy:  36.0      Comp_time of one batch:  -0.8761641979217529\n",
            "Iteration:  17880      Current loss:  34.07482147216797      Current accuracy:  36.0      Comp_time of one batch:  -0.8963489532470703\n",
            "Iteration:  17885      Current loss:  23.33185577392578      Current accuracy:  37.0      Comp_time of one batch:  -0.8777477741241455\n",
            "Iteration:  17890      Current loss:  19.904714584350586      Current accuracy:  38.0      Comp_time of one batch:  -0.9070026874542236\n",
            "Iteration:  17895      Current loss:  25.19097328186035      Current accuracy:  38.0      Comp_time of one batch:  -0.8706300258636475\n",
            "Iteration:  17900      Current loss:  29.12881088256836      Current accuracy:  38.0      Comp_time of one batch:  -0.8948888778686523\n",
            "Iteration:  17905      Current loss:  29.664012908935547      Current accuracy:  38.0      Comp_time of one batch:  -0.8913094997406006\n",
            "Iteration:  17910      Current loss:  26.709880828857422      Current accuracy:  38.0      Comp_time of one batch:  -0.9070348739624023\n",
            "Iteration:  17915      Current loss:  19.20309829711914      Current accuracy:  39.0      Comp_time of one batch:  -0.8887135982513428\n",
            "Iteration:  17920      Current loss:  18.357337951660156      Current accuracy:  40.0      Comp_time of one batch:  -0.8799521923065186\n",
            "Iteration:  17925      Current loss:  22.268081665039062      Current accuracy:  40.0      Comp_time of one batch:  -0.8955032825469971\n",
            "Iteration:  17930      Current loss:  27.582111358642578      Current accuracy:  40.0      Comp_time of one batch:  -0.8875012397766113\n",
            "Iteration:  17935      Current loss:  28.18701171875      Current accuracy:  40.0      Comp_time of one batch:  -0.920274019241333\n",
            "Iteration:  17940      Current loss:  34.389915466308594      Current accuracy:  40.0      Comp_time of one batch:  -0.9138715267181396\n",
            "Iteration:  17945      Current loss:  20.908843994140625      Current accuracy:  41.0      Comp_time of one batch:  -0.9051272869110107\n",
            "Iteration:  17950      Current loss:  19.939170837402344      Current accuracy:  42.0      Comp_time of one batch:  -0.9005005359649658\n",
            "Iteration:  17955      Current loss:  19.993877410888672      Current accuracy:  43.0      Comp_time of one batch:  -0.9097328186035156\n",
            "Iteration:  17960      Current loss:  24.128564834594727      Current accuracy:  43.0      Comp_time of one batch:  -0.8980648517608643\n",
            "Iteration:  17965      Current loss:  23.146930694580078      Current accuracy:  44.0      Comp_time of one batch:  -0.9014925956726074\n",
            "Iteration:  17970      Current loss:  32.48406982421875      Current accuracy:  44.0      Comp_time of one batch:  -0.8791351318359375\n",
            "Iteration:  17975      Current loss:  23.145275115966797      Current accuracy:  45.0      Comp_time of one batch:  -0.888085126876831\n",
            "Iteration:  17980      Current loss:  22.59926986694336      Current accuracy:  46.0      Comp_time of one batch:  -0.8817110061645508\n",
            "Iteration:  17985      Current loss:  28.20265007019043      Current accuracy:  46.0      Comp_time of one batch:  -0.9092390537261963\n",
            "Iteration:  17990      Current loss:  38.95405578613281      Current accuracy:  46.0      Comp_time of one batch:  -0.9156973361968994\n",
            "Iteration:  17995      Current loss:  31.24565887451172      Current accuracy:  46.0      Comp_time of one batch:  -0.88553786277771\n",
            "Iteration:  18000      Current loss:  19.64852523803711      Current accuracy:  0.0      Comp_time of one batch:  -0.8815159797668457\n",
            "Iteration:  18005      Current loss:  28.20907974243164      Current accuracy:  0.0      Comp_time of one batch:  -0.8969864845275879\n",
            "Iteration:  18010      Current loss:  19.65365219116211      Current accuracy:  1.0      Comp_time of one batch:  -0.8837261199951172\n",
            "Iteration:  18015      Current loss:  24.61591339111328      Current accuracy:  1.0      Comp_time of one batch:  -0.9134879112243652\n",
            "Iteration:  18020      Current loss:  21.81393051147461      Current accuracy:  2.0      Comp_time of one batch:  -0.8830101490020752\n",
            "Iteration:  18025      Current loss:  33.444820404052734      Current accuracy:  2.0      Comp_time of one batch:  -0.8690130710601807\n",
            "Iteration:  18030      Current loss:  21.2514591217041      Current accuracy:  3.0      Comp_time of one batch:  -0.8834068775177002\n",
            "Iteration:  18035      Current loss:  24.952402114868164      Current accuracy:  3.0      Comp_time of one batch:  -0.893726110458374\n",
            "Iteration:  18040      Current loss:  17.311420440673828      Current accuracy:  4.0      Comp_time of one batch:  -0.9399998188018799\n",
            "Iteration:  18045      Current loss:  39.39852523803711      Current accuracy:  4.0      Comp_time of one batch:  -0.8980233669281006\n",
            "Iteration:  18050      Current loss:  32.9953498840332      Current accuracy:  4.0      Comp_time of one batch:  -0.8808383941650391\n",
            "Iteration:  18055      Current loss:  22.975500106811523      Current accuracy:  5.0      Comp_time of one batch:  -0.9031996726989746\n",
            "Iteration:  18060      Current loss:  25.344892501831055      Current accuracy:  5.0      Comp_time of one batch:  -0.8915646076202393\n",
            "Iteration:  18065      Current loss:  23.13022804260254      Current accuracy:  5.0      Comp_time of one batch:  -0.8855185508728027\n",
            "Iteration:  18070      Current loss:  25.912687301635742      Current accuracy:  5.0      Comp_time of one batch:  -0.8961029052734375\n",
            "Iteration:  18075      Current loss:  32.30162048339844      Current accuracy:  5.0      Comp_time of one batch:  -0.8903326988220215\n",
            "Iteration:  18080      Current loss:  24.75441551208496      Current accuracy:  5.0      Comp_time of one batch:  -0.8959650993347168\n",
            "Iteration:  18085      Current loss:  38.75252914428711      Current accuracy:  5.0      Comp_time of one batch:  -0.9023244380950928\n",
            "Iteration:  18090      Current loss:  19.4935245513916      Current accuracy:  6.0      Comp_time of one batch:  -0.8742494583129883\n",
            "Iteration:  18095      Current loss:  26.323631286621094      Current accuracy:  6.0      Comp_time of one batch:  -0.8815391063690186\n",
            "Iteration:  18100      Current loss:  22.24057960510254      Current accuracy:  7.0      Comp_time of one batch:  -0.8811748027801514\n",
            "Iteration:  18105      Current loss:  26.943735122680664      Current accuracy:  7.0      Comp_time of one batch:  -0.9066851139068604\n",
            "Iteration:  18110      Current loss:  22.035049438476562      Current accuracy:  8.0      Comp_time of one batch:  -0.9016909599304199\n",
            "Iteration:  18115      Current loss:  20.52861976623535      Current accuracy:  9.0      Comp_time of one batch:  -0.8985896110534668\n",
            "Iteration:  18120      Current loss:  21.418506622314453      Current accuracy:  10.0      Comp_time of one batch:  -0.8820133209228516\n",
            "Iteration:  18125      Current loss:  23.174728393554688      Current accuracy:  10.0      Comp_time of one batch:  -0.889103889465332\n",
            "Iteration:  18130      Current loss:  21.473976135253906      Current accuracy:  11.0      Comp_time of one batch:  -0.8821640014648438\n",
            "Iteration:  18135      Current loss:  33.50468444824219      Current accuracy:  11.0      Comp_time of one batch:  -0.90411376953125\n",
            "Iteration:  18140      Current loss:  23.71249771118164      Current accuracy:  12.0      Comp_time of one batch:  -0.9068124294281006\n",
            "Iteration:  18145      Current loss:  25.39686393737793      Current accuracy:  12.0      Comp_time of one batch:  -0.8805086612701416\n",
            "Iteration:  18150      Current loss:  28.57441520690918      Current accuracy:  12.0      Comp_time of one batch:  -0.8827548027038574\n",
            "Iteration:  18155      Current loss:  28.77324104309082      Current accuracy:  12.0      Comp_time of one batch:  -0.8928177356719971\n",
            "Iteration:  18160      Current loss:  16.403730392456055      Current accuracy:  13.0      Comp_time of one batch:  -0.8805921077728271\n",
            "Iteration:  18165      Current loss:  20.775630950927734      Current accuracy:  14.0      Comp_time of one batch:  -0.8881986141204834\n",
            "Iteration:  18170      Current loss:  27.86163902282715      Current accuracy:  14.0      Comp_time of one batch:  -0.8859124183654785\n",
            "Iteration:  18175      Current loss:  25.383573532104492      Current accuracy:  14.0      Comp_time of one batch:  -0.8859884738922119\n",
            "Iteration:  18180      Current loss:  22.972118377685547      Current accuracy:  15.0      Comp_time of one batch:  -0.8816287517547607\n",
            "Iteration:  18185      Current loss:  26.393329620361328      Current accuracy:  15.0      Comp_time of one batch:  -0.8696978092193604\n",
            "Iteration:  18190      Current loss:  23.98086166381836      Current accuracy:  15.0      Comp_time of one batch:  -0.8725666999816895\n",
            "Iteration:  18195      Current loss:  19.295969009399414      Current accuracy:  16.0      Comp_time of one batch:  -0.8822088241577148\n",
            "Iteration:  18200      Current loss:  25.96195411682129      Current accuracy:  16.0      Comp_time of one batch:  -0.8974084854125977\n",
            "Iteration:  18205      Current loss:  24.644094467163086      Current accuracy:  16.0      Comp_time of one batch:  -0.8907289505004883\n",
            "Iteration:  18210      Current loss:  30.614990234375      Current accuracy:  16.0      Comp_time of one batch:  -0.8839931488037109\n",
            "Iteration:  18215      Current loss:  24.380990982055664      Current accuracy:  17.0      Comp_time of one batch:  -0.8794059753417969\n",
            "Iteration:  18220      Current loss:  21.673049926757812      Current accuracy:  18.0      Comp_time of one batch:  -0.8759794235229492\n",
            "Iteration:  18225      Current loss:  29.375669479370117      Current accuracy:  18.0      Comp_time of one batch:  -0.8971014022827148\n",
            "Iteration:  18230      Current loss:  28.33466911315918      Current accuracy:  18.0      Comp_time of one batch:  -0.8865911960601807\n",
            "Iteration:  18235      Current loss:  23.730789184570312      Current accuracy:  18.0      Comp_time of one batch:  -0.8909523487091064\n",
            "Iteration:  18240      Current loss:  25.36678695678711      Current accuracy:  18.0      Comp_time of one batch:  -0.9042966365814209\n",
            "Iteration:  18245      Current loss:  23.22821044921875      Current accuracy:  19.0      Comp_time of one batch:  -0.9005670547485352\n",
            "Iteration:  18250      Current loss:  27.060195922851562      Current accuracy:  19.0      Comp_time of one batch:  -0.89438796043396\n",
            "Iteration:  18255      Current loss:  21.68485450744629      Current accuracy:  20.0      Comp_time of one batch:  -0.8828985691070557\n",
            "Iteration:  18260      Current loss:  25.331649780273438      Current accuracy:  20.0      Comp_time of one batch:  -0.8941481113433838\n",
            "Iteration:  18265      Current loss:  17.044200897216797      Current accuracy:  21.0      Comp_time of one batch:  -0.8945059776306152\n",
            "Iteration:  18270      Current loss:  22.101306915283203      Current accuracy:  21.0      Comp_time of one batch:  -0.8976478576660156\n",
            "Iteration:  18275      Current loss:  22.586828231811523      Current accuracy:  22.0      Comp_time of one batch:  -0.8935270309448242\n",
            "Iteration:  18280      Current loss:  33.28262710571289      Current accuracy:  22.0      Comp_time of one batch:  -0.8945503234863281\n",
            "Iteration:  18285      Current loss:  16.425752639770508      Current accuracy:  23.0      Comp_time of one batch:  -0.8987360000610352\n",
            "Iteration:  18290      Current loss:  32.383541107177734      Current accuracy:  23.0      Comp_time of one batch:  -0.8943519592285156\n",
            "Iteration:  18295      Current loss:  25.373544692993164      Current accuracy:  23.0      Comp_time of one batch:  -0.8991405963897705\n",
            "Iteration:  18300      Current loss:  26.908082962036133      Current accuracy:  23.0      Comp_time of one batch:  -0.8768625259399414\n",
            "Iteration:  18305      Current loss:  18.362245559692383      Current accuracy:  24.0      Comp_time of one batch:  -0.8793187141418457\n",
            "Iteration:  18310      Current loss:  18.43097496032715      Current accuracy:  25.0      Comp_time of one batch:  -0.8851380348205566\n",
            "Iteration:  18315      Current loss:  21.74900245666504      Current accuracy:  26.0      Comp_time of one batch:  -0.8920907974243164\n",
            "Iteration:  18320      Current loss:  25.195716857910156      Current accuracy:  26.0      Comp_time of one batch:  -0.8693132400512695\n",
            "Iteration:  18325      Current loss:  29.12788963317871      Current accuracy:  26.0      Comp_time of one batch:  -0.9038538932800293\n",
            "Iteration:  18330      Current loss:  31.223018646240234      Current accuracy:  26.0      Comp_time of one batch:  -0.9093637466430664\n",
            "Iteration:  18335      Current loss:  27.853897094726562      Current accuracy:  26.0      Comp_time of one batch:  -0.8872699737548828\n",
            "Iteration:  18340      Current loss:  23.53786849975586      Current accuracy:  26.0      Comp_time of one batch:  -0.8868489265441895\n",
            "Iteration:  18345      Current loss:  26.48686981201172      Current accuracy:  26.0      Comp_time of one batch:  -0.8780808448791504\n",
            "Iteration:  18350      Current loss:  24.457612991333008      Current accuracy:  27.0      Comp_time of one batch:  -0.8740103244781494\n",
            "Iteration:  18355      Current loss:  19.595766067504883      Current accuracy:  28.0      Comp_time of one batch:  -0.8897976875305176\n",
            "Iteration:  18360      Current loss:  21.202486038208008      Current accuracy:  29.0      Comp_time of one batch:  -0.8834519386291504\n",
            "Iteration:  18365      Current loss:  29.88996696472168      Current accuracy:  29.0      Comp_time of one batch:  -0.8905429840087891\n",
            "Iteration:  18370      Current loss:  20.49968147277832      Current accuracy:  30.0      Comp_time of one batch:  -0.8958542346954346\n",
            "Iteration:  18375      Current loss:  21.983205795288086      Current accuracy:  30.0      Comp_time of one batch:  -0.8905596733093262\n",
            "Iteration:  18380      Current loss:  25.858280181884766      Current accuracy:  30.0      Comp_time of one batch:  -0.8878564834594727\n",
            "Iteration:  18385      Current loss:  22.099355697631836      Current accuracy:  31.0      Comp_time of one batch:  -0.8967692852020264\n",
            "Iteration:  18390      Current loss:  20.47323226928711      Current accuracy:  32.0      Comp_time of one batch:  -0.8823368549346924\n",
            "Iteration:  18395      Current loss:  27.526208877563477      Current accuracy:  32.0      Comp_time of one batch:  -0.8770792484283447\n",
            "Iteration:  18400      Current loss:  27.451820373535156      Current accuracy:  32.0      Comp_time of one batch:  -0.8920667171478271\n",
            "Iteration:  18405      Current loss:  17.26559066772461      Current accuracy:  33.0      Comp_time of one batch:  -0.9061384201049805\n",
            "Iteration:  18410      Current loss:  22.558982849121094      Current accuracy:  34.0      Comp_time of one batch:  -0.8912034034729004\n",
            "Iteration:  18415      Current loss:  19.873937606811523      Current accuracy:  35.0      Comp_time of one batch:  -0.8997411727905273\n",
            "Iteration:  18420      Current loss:  20.151994705200195      Current accuracy:  36.0      Comp_time of one batch:  -0.9027397632598877\n",
            "Iteration:  18425      Current loss:  24.344585418701172      Current accuracy:  36.0      Comp_time of one batch:  -0.9131708145141602\n",
            "Iteration:  18430      Current loss:  22.522258758544922      Current accuracy:  37.0      Comp_time of one batch:  -0.9025857448577881\n",
            "Iteration:  18435      Current loss:  18.80912971496582      Current accuracy:  38.0      Comp_time of one batch:  -0.8851239681243896\n",
            "Iteration:  18440      Current loss:  21.646533966064453      Current accuracy:  39.0      Comp_time of one batch:  -0.8982694149017334\n",
            "Iteration:  18445      Current loss:  28.132266998291016      Current accuracy:  39.0      Comp_time of one batch:  -0.8899991512298584\n",
            "Iteration:  18450      Current loss:  16.988584518432617      Current accuracy:  40.0      Comp_time of one batch:  -0.889833927154541\n",
            "Iteration:  18455      Current loss:  22.069459915161133      Current accuracy:  41.0      Comp_time of one batch:  -0.8773021697998047\n",
            "Iteration:  18460      Current loss:  21.37782096862793      Current accuracy:  42.0      Comp_time of one batch:  -0.8945968151092529\n",
            "Iteration:  18465      Current loss:  29.2972412109375      Current accuracy:  42.0      Comp_time of one batch:  -0.907181978225708\n",
            "Iteration:  18470      Current loss:  21.266929626464844      Current accuracy:  43.0      Comp_time of one batch:  -0.8842298984527588\n",
            "Iteration:  18475      Current loss:  20.130037307739258      Current accuracy:  44.0      Comp_time of one batch:  -0.8987693786621094\n",
            "Iteration:  18480      Current loss:  20.355077743530273      Current accuracy:  45.0      Comp_time of one batch:  -0.909095287322998\n",
            "Iteration:  18485      Current loss:  21.44692611694336      Current accuracy:  46.0      Comp_time of one batch:  -0.8800003528594971\n",
            "Iteration:  18490      Current loss:  24.329008102416992      Current accuracy:  47.0      Comp_time of one batch:  -0.8816797733306885\n",
            "Iteration:  18495      Current loss:  23.37676429748535      Current accuracy:  47.0      Comp_time of one batch:  -0.8909249305725098\n",
            "Iteration:  18500      Current loss:  21.69898223876953      Current accuracy:  0.0      Comp_time of one batch:  -0.8740253448486328\n",
            "Iteration:  18505      Current loss:  24.607385635375977      Current accuracy:  0.0      Comp_time of one batch:  -0.9069430828094482\n",
            "Iteration:  18510      Current loss:  19.163619995117188      Current accuracy:  1.0      Comp_time of one batch:  -0.8850367069244385\n",
            "Iteration:  18515      Current loss:  26.933473587036133      Current accuracy:  1.0      Comp_time of one batch:  -0.879997730255127\n",
            "Iteration:  18520      Current loss:  29.961654663085938      Current accuracy:  1.0      Comp_time of one batch:  -0.8868215084075928\n",
            "Iteration:  18525      Current loss:  20.889270782470703      Current accuracy:  2.0      Comp_time of one batch:  -0.8982217311859131\n",
            "Iteration:  18530      Current loss:  32.82188415527344      Current accuracy:  2.0      Comp_time of one batch:  -0.8908205032348633\n",
            "Iteration:  18535      Current loss:  18.40851593017578      Current accuracy:  3.0      Comp_time of one batch:  -0.8985152244567871\n",
            "Iteration:  18540      Current loss:  25.043716430664062      Current accuracy:  3.0      Comp_time of one batch:  -0.8891148567199707\n",
            "Iteration:  18545      Current loss:  17.59128761291504      Current accuracy:  4.0      Comp_time of one batch:  -0.8859817981719971\n",
            "Iteration:  18550      Current loss:  18.795801162719727      Current accuracy:  5.0      Comp_time of one batch:  -0.8859562873840332\n",
            "Iteration:  18555      Current loss:  22.012197494506836      Current accuracy:  6.0      Comp_time of one batch:  -0.9058215618133545\n",
            "Iteration:  18560      Current loss:  25.29014015197754      Current accuracy:  6.0      Comp_time of one batch:  -0.881361722946167\n",
            "Iteration:  18565      Current loss:  22.99896240234375      Current accuracy:  6.0      Comp_time of one batch:  -0.9032313823699951\n",
            "Iteration:  18570      Current loss:  24.358591079711914      Current accuracy:  7.0      Comp_time of one batch:  -0.9368598461151123\n",
            "Iteration:  18575      Current loss:  21.393848419189453      Current accuracy:  8.0      Comp_time of one batch:  -0.906440019607544\n",
            "Iteration:  18580      Current loss:  15.082152366638184      Current accuracy:  9.0      Comp_time of one batch:  -0.8803789615631104\n",
            "Iteration:  18585      Current loss:  26.961952209472656      Current accuracy:  9.0      Comp_time of one batch:  -0.8974337577819824\n",
            "Iteration:  18590      Current loss:  22.63750648498535      Current accuracy:  10.0      Comp_time of one batch:  -0.8945348262786865\n",
            "Iteration:  18595      Current loss:  24.1424617767334      Current accuracy:  10.0      Comp_time of one batch:  -0.8824000358581543\n",
            "Iteration:  18600      Current loss:  32.11104202270508      Current accuracy:  10.0      Comp_time of one batch:  -0.8838796615600586\n",
            "Iteration:  18605      Current loss:  19.580156326293945      Current accuracy:  11.0      Comp_time of one batch:  -0.8897624015808105\n",
            "Iteration:  18610      Current loss:  22.406103134155273      Current accuracy:  12.0      Comp_time of one batch:  -0.8871629238128662\n",
            "Iteration:  18615      Current loss:  18.355655670166016      Current accuracy:  13.0      Comp_time of one batch:  -0.8856728076934814\n",
            "Iteration:  18620      Current loss:  24.791555404663086      Current accuracy:  13.0      Comp_time of one batch:  -0.8980910778045654\n",
            "Iteration:  18625      Current loss:  18.209625244140625      Current accuracy:  14.0      Comp_time of one batch:  -0.8856613636016846\n",
            "Iteration:  18630      Current loss:  17.942848205566406      Current accuracy:  15.0      Comp_time of one batch:  -0.8904492855072021\n",
            "Iteration:  18635      Current loss:  24.400432586669922      Current accuracy:  15.0      Comp_time of one batch:  -0.8887002468109131\n",
            "Iteration:  18640      Current loss:  19.470605850219727      Current accuracy:  16.0      Comp_time of one batch:  -0.8920178413391113\n",
            "Iteration:  18645      Current loss:  21.691001892089844      Current accuracy:  17.0      Comp_time of one batch:  -0.8902378082275391\n",
            "Iteration:  18650      Current loss:  24.299251556396484      Current accuracy:  17.0      Comp_time of one batch:  -0.888275146484375\n",
            "Iteration:  18655      Current loss:  15.658364295959473      Current accuracy:  18.0      Comp_time of one batch:  -0.8863363265991211\n",
            "Iteration:  18660      Current loss:  23.149450302124023      Current accuracy:  19.0      Comp_time of one batch:  -0.8775937557220459\n",
            "Iteration:  18665      Current loss:  22.816598892211914      Current accuracy:  20.0      Comp_time of one batch:  -0.8855268955230713\n",
            "Iteration:  18670      Current loss:  27.580411911010742      Current accuracy:  20.0      Comp_time of one batch:  -0.8926160335540771\n",
            "Iteration:  18675      Current loss:  23.228055953979492      Current accuracy:  20.0      Comp_time of one batch:  -0.8798575401306152\n",
            "Iteration:  18680      Current loss:  26.08795928955078      Current accuracy:  20.0      Comp_time of one batch:  -0.876262903213501\n",
            "Iteration:  18685      Current loss:  32.7373046875      Current accuracy:  20.0      Comp_time of one batch:  -0.900557279586792\n",
            "Iteration:  18690      Current loss:  20.568206787109375      Current accuracy:  21.0      Comp_time of one batch:  -0.8890299797058105\n",
            "Iteration:  18695      Current loss:  21.328811645507812      Current accuracy:  22.0      Comp_time of one batch:  -0.8864693641662598\n",
            "Iteration:  18700      Current loss:  24.71971321105957      Current accuracy:  22.0      Comp_time of one batch:  -0.880728006362915\n",
            "Iteration:  18705      Current loss:  26.645191192626953      Current accuracy:  22.0      Comp_time of one batch:  -0.8995375633239746\n",
            "Iteration:  18710      Current loss:  29.579015731811523      Current accuracy:  22.0      Comp_time of one batch:  -0.9013669490814209\n",
            "Iteration:  18715      Current loss:  16.06492805480957      Current accuracy:  23.0      Comp_time of one batch:  -0.9290776252746582\n",
            "Iteration:  18720      Current loss:  24.448688507080078      Current accuracy:  23.0      Comp_time of one batch:  -0.912604808807373\n",
            "Iteration:  18725      Current loss:  21.107633590698242      Current accuracy:  24.0      Comp_time of one batch:  -0.9020404815673828\n",
            "Iteration:  18730      Current loss:  17.255596160888672      Current accuracy:  25.0      Comp_time of one batch:  -0.8851397037506104\n",
            "Iteration:  18735      Current loss:  18.716821670532227      Current accuracy:  26.0      Comp_time of one batch:  -0.873469352722168\n",
            "Iteration:  18740      Current loss:  24.556644439697266      Current accuracy:  26.0      Comp_time of one batch:  -0.8734545707702637\n",
            "Iteration:  18745      Current loss:  18.58554458618164      Current accuracy:  27.0      Comp_time of one batch:  -0.8759145736694336\n",
            "Iteration:  18750      Current loss:  24.64056396484375      Current accuracy:  28.0      Comp_time of one batch:  -0.8916034698486328\n",
            "Iteration:  18755      Current loss:  30.368547439575195      Current accuracy:  28.0      Comp_time of one batch:  -0.8908596038818359\n",
            "Iteration:  18760      Current loss:  31.419252395629883      Current accuracy:  28.0      Comp_time of one batch:  -0.9048933982849121\n",
            "Iteration:  18765      Current loss:  24.133333206176758      Current accuracy:  28.0      Comp_time of one batch:  -0.8851706981658936\n",
            "Iteration:  18770      Current loss:  29.315656661987305      Current accuracy:  28.0      Comp_time of one batch:  -0.903156042098999\n",
            "Iteration:  18775      Current loss:  16.962636947631836      Current accuracy:  29.0      Comp_time of one batch:  -0.8780641555786133\n",
            "Iteration:  18780      Current loss:  26.574188232421875      Current accuracy:  29.0      Comp_time of one batch:  -0.909123420715332\n",
            "Iteration:  18785      Current loss:  24.885263442993164      Current accuracy:  29.0      Comp_time of one batch:  -0.8807451725006104\n",
            "Iteration:  18790      Current loss:  27.36258316040039      Current accuracy:  29.0      Comp_time of one batch:  -0.8775310516357422\n",
            "Iteration:  18795      Current loss:  21.70558738708496      Current accuracy:  30.0      Comp_time of one batch:  -0.8862638473510742\n",
            "Iteration:  18800      Current loss:  24.658727645874023      Current accuracy:  30.0      Comp_time of one batch:  -0.8962750434875488\n",
            "Iteration:  18805      Current loss:  28.792545318603516      Current accuracy:  30.0      Comp_time of one batch:  -0.8998186588287354\n",
            "Iteration:  18810      Current loss:  23.4273738861084      Current accuracy:  30.0      Comp_time of one batch:  -0.89530348777771\n",
            "Iteration:  18815      Current loss:  24.29326820373535      Current accuracy:  30.0      Comp_time of one batch:  -0.8795089721679688\n",
            "Iteration:  18820      Current loss:  23.260513305664062      Current accuracy:  31.0      Comp_time of one batch:  -0.8692834377288818\n",
            "Iteration:  18825      Current loss:  21.033397674560547      Current accuracy:  32.0      Comp_time of one batch:  -0.8806860446929932\n",
            "Iteration:  18830      Current loss:  27.352794647216797      Current accuracy:  33.0      Comp_time of one batch:  -0.8783059120178223\n",
            "Iteration:  18835      Current loss:  29.999771118164062      Current accuracy:  33.0      Comp_time of one batch:  -0.8882744312286377\n",
            "Iteration:  18840      Current loss:  19.205650329589844      Current accuracy:  34.0      Comp_time of one batch:  -0.9062507152557373\n",
            "Iteration:  18845      Current loss:  26.016677856445312      Current accuracy:  34.0      Comp_time of one batch:  -0.8706920146942139\n",
            "Iteration:  18850      Current loss:  18.542285919189453      Current accuracy:  35.0      Comp_time of one batch:  -0.8917944431304932\n",
            "Iteration:  18855      Current loss:  24.88646125793457      Current accuracy:  35.0      Comp_time of one batch:  -0.8816428184509277\n",
            "Iteration:  18860      Current loss:  20.619813919067383      Current accuracy:  36.0      Comp_time of one batch:  -0.8744747638702393\n",
            "Iteration:  18865      Current loss:  22.59148406982422      Current accuracy:  36.0      Comp_time of one batch:  -0.917773962020874\n",
            "Iteration:  18870      Current loss:  30.41579818725586      Current accuracy:  36.0      Comp_time of one batch:  -0.8908994197845459\n",
            "Iteration:  18875      Current loss:  22.243980407714844      Current accuracy:  36.0      Comp_time of one batch:  -0.9000647068023682\n",
            "Iteration:  18880      Current loss:  24.278547286987305      Current accuracy:  36.0      Comp_time of one batch:  -0.8818414211273193\n",
            "Iteration:  18885      Current loss:  22.274255752563477      Current accuracy:  36.0      Comp_time of one batch:  -0.8853685855865479\n",
            "Iteration:  18890      Current loss:  22.808670043945312      Current accuracy:  37.0      Comp_time of one batch:  -0.8838253021240234\n",
            "Iteration:  18895      Current loss:  34.13521957397461      Current accuracy:  37.0      Comp_time of one batch:  -0.9030437469482422\n",
            "Iteration:  18900      Current loss:  14.786094665527344      Current accuracy:  38.0      Comp_time of one batch:  -0.9283785820007324\n",
            "Iteration:  18905      Current loss:  24.055330276489258      Current accuracy:  38.0      Comp_time of one batch:  -0.9192171096801758\n",
            "Iteration:  18910      Current loss:  23.642648696899414      Current accuracy:  38.0      Comp_time of one batch:  -0.8922638893127441\n",
            "Iteration:  18915      Current loss:  28.73602867126465      Current accuracy:  38.0      Comp_time of one batch:  -0.9172616004943848\n",
            "Iteration:  18920      Current loss:  22.93010902404785      Current accuracy:  39.0      Comp_time of one batch:  -0.8935868740081787\n",
            "Iteration:  18925      Current loss:  25.700782775878906      Current accuracy:  40.0      Comp_time of one batch:  -0.8751440048217773\n",
            "Iteration:  18930      Current loss:  15.930011749267578      Current accuracy:  41.0      Comp_time of one batch:  -0.8728208541870117\n",
            "Iteration:  18935      Current loss:  26.554641723632812      Current accuracy:  41.0      Comp_time of one batch:  -0.881587028503418\n",
            "Iteration:  18940      Current loss:  26.039884567260742      Current accuracy:  41.0      Comp_time of one batch:  -0.9001588821411133\n",
            "Iteration:  18945      Current loss:  24.491870880126953      Current accuracy:  41.0      Comp_time of one batch:  -0.8742022514343262\n",
            "Iteration:  18950      Current loss:  30.195539474487305      Current accuracy:  41.0      Comp_time of one batch:  -0.8928391933441162\n",
            "Iteration:  18955      Current loss:  37.94970703125      Current accuracy:  41.0      Comp_time of one batch:  -0.8855366706848145\n",
            "Iteration:  18960      Current loss:  21.601776123046875      Current accuracy:  42.0      Comp_time of one batch:  -0.8881804943084717\n",
            "Iteration:  18965      Current loss:  26.845829010009766      Current accuracy:  42.0      Comp_time of one batch:  -0.8964593410491943\n",
            "Iteration:  18970      Current loss:  26.209354400634766      Current accuracy:  42.0      Comp_time of one batch:  -0.879180908203125\n",
            "Iteration:  18975      Current loss:  26.008859634399414      Current accuracy:  42.0      Comp_time of one batch:  -0.8735940456390381\n",
            "Iteration:  18980      Current loss:  25.08976936340332      Current accuracy:  42.0      Comp_time of one batch:  -0.8937630653381348\n",
            "Iteration:  18985      Current loss:  27.950923919677734      Current accuracy:  42.0      Comp_time of one batch:  -0.8911747932434082\n",
            "Iteration:  18990      Current loss:  22.623870849609375      Current accuracy:  43.0      Comp_time of one batch:  -0.8827648162841797\n",
            "Iteration:  18995      Current loss:  31.913084030151367      Current accuracy:  43.0      Comp_time of one batch:  -0.8804821968078613\n",
            "Iteration:  19000      Current loss:  25.002342224121094      Current accuracy:  0.0      Comp_time of one batch:  -0.8806781768798828\n",
            "Iteration:  19005      Current loss:  16.019954681396484      Current accuracy:  1.0      Comp_time of one batch:  -0.8808369636535645\n",
            "Iteration:  19010      Current loss:  22.858430862426758      Current accuracy:  2.0      Comp_time of one batch:  -0.898806095123291\n",
            "Iteration:  19015      Current loss:  30.507129669189453      Current accuracy:  2.0      Comp_time of one batch:  -0.8757297992706299\n",
            "Iteration:  19020      Current loss:  20.8967342376709      Current accuracy:  3.0      Comp_time of one batch:  -0.8753516674041748\n",
            "Iteration:  19025      Current loss:  17.252033233642578      Current accuracy:  4.0      Comp_time of one batch:  -0.895998477935791\n",
            "Iteration:  19030      Current loss:  34.46955871582031      Current accuracy:  4.0      Comp_time of one batch:  -0.8855745792388916\n",
            "Iteration:  19035      Current loss:  24.91735076904297      Current accuracy:  4.0      Comp_time of one batch:  -0.884681224822998\n",
            "Iteration:  19040      Current loss:  32.67703628540039      Current accuracy:  4.0      Comp_time of one batch:  -0.9060447216033936\n",
            "Iteration:  19045      Current loss:  20.144479751586914      Current accuracy:  5.0      Comp_time of one batch:  -0.8823115825653076\n",
            "Iteration:  19050      Current loss:  24.681596755981445      Current accuracy:  5.0      Comp_time of one batch:  -0.8866519927978516\n",
            "Iteration:  19055      Current loss:  20.595861434936523      Current accuracy:  6.0      Comp_time of one batch:  -0.8919234275817871\n",
            "Iteration:  19060      Current loss:  21.08018684387207      Current accuracy:  7.0      Comp_time of one batch:  -0.8847482204437256\n",
            "Iteration:  19065      Current loss:  22.086383819580078      Current accuracy:  8.0      Comp_time of one batch:  -0.8934776782989502\n",
            "Iteration:  19070      Current loss:  15.958189964294434      Current accuracy:  9.0      Comp_time of one batch:  -0.8907198905944824\n",
            "Iteration:  19075      Current loss:  21.150890350341797      Current accuracy:  10.0      Comp_time of one batch:  -0.8871324062347412\n",
            "Iteration:  19080      Current loss:  28.209556579589844      Current accuracy:  10.0      Comp_time of one batch:  -0.885934591293335\n",
            "Iteration:  19085      Current loss:  15.864235877990723      Current accuracy:  11.0      Comp_time of one batch:  -0.8893225193023682\n",
            "Iteration:  19090      Current loss:  20.4299259185791      Current accuracy:  12.0      Comp_time of one batch:  -0.8865664005279541\n",
            "Iteration:  19095      Current loss:  24.084186553955078      Current accuracy:  12.0      Comp_time of one batch:  -0.8940343856811523\n",
            "Iteration:  19100      Current loss:  16.79025650024414      Current accuracy:  13.0      Comp_time of one batch:  -0.8894987106323242\n",
            "Iteration:  19105      Current loss:  27.402362823486328      Current accuracy:  13.0      Comp_time of one batch:  -0.8980674743652344\n",
            "Iteration:  19110      Current loss:  18.87847137451172      Current accuracy:  14.0      Comp_time of one batch:  -0.8943080902099609\n",
            "Iteration:  19115      Current loss:  25.43894386291504      Current accuracy:  15.0      Comp_time of one batch:  -0.8864901065826416\n",
            "Iteration:  19120      Current loss:  20.147611618041992      Current accuracy:  16.0      Comp_time of one batch:  -0.8941223621368408\n",
            "Iteration:  19125      Current loss:  22.03311538696289      Current accuracy:  17.0      Comp_time of one batch:  -0.8949294090270996\n",
            "Iteration:  19130      Current loss:  30.211124420166016      Current accuracy:  17.0      Comp_time of one batch:  -0.8876218795776367\n",
            "Iteration:  19135      Current loss:  23.246000289916992      Current accuracy:  18.0      Comp_time of one batch:  -0.8797905445098877\n",
            "Iteration:  19140      Current loss:  24.228071212768555      Current accuracy:  18.0      Comp_time of one batch:  -0.8929524421691895\n",
            "Iteration:  19145      Current loss:  22.06029510498047      Current accuracy:  18.0      Comp_time of one batch:  -0.8776655197143555\n",
            "Iteration:  19150      Current loss:  19.048830032348633      Current accuracy:  19.0      Comp_time of one batch:  -0.8759253025054932\n",
            "Iteration:  19155      Current loss:  23.63836097717285      Current accuracy:  19.0      Comp_time of one batch:  -0.8932325839996338\n",
            "Iteration:  19160      Current loss:  17.677410125732422      Current accuracy:  20.0      Comp_time of one batch:  -0.8857216835021973\n",
            "Iteration:  19165      Current loss:  23.94806480407715      Current accuracy:  20.0      Comp_time of one batch:  -0.8778672218322754\n",
            "Iteration:  19170      Current loss:  28.566938400268555      Current accuracy:  20.0      Comp_time of one batch:  -0.8868496417999268\n",
            "Iteration:  19175      Current loss:  23.58660125732422      Current accuracy:  20.0      Comp_time of one batch:  -0.903822660446167\n",
            "Iteration:  19180      Current loss:  26.477428436279297      Current accuracy:  20.0      Comp_time of one batch:  -0.8853533267974854\n",
            "Iteration:  19185      Current loss:  30.084993362426758      Current accuracy:  20.0      Comp_time of one batch:  -0.8995134830474854\n",
            "Iteration:  19190      Current loss:  22.68191146850586      Current accuracy:  21.0      Comp_time of one batch:  -0.8990991115570068\n",
            "Iteration:  19195      Current loss:  34.063926696777344      Current accuracy:  21.0      Comp_time of one batch:  -0.8764569759368896\n",
            "Iteration:  19200      Current loss:  22.888864517211914      Current accuracy:  21.0      Comp_time of one batch:  -0.8878054618835449\n",
            "Iteration:  19205      Current loss:  23.756914138793945      Current accuracy:  22.0      Comp_time of one batch:  -0.9001383781433105\n",
            "Iteration:  19210      Current loss:  21.11518669128418      Current accuracy:  23.0      Comp_time of one batch:  -0.8967001438140869\n",
            "Iteration:  19215      Current loss:  28.959613800048828      Current accuracy:  23.0      Comp_time of one batch:  -0.8856823444366455\n",
            "Iteration:  19220      Current loss:  18.306884765625      Current accuracy:  24.0      Comp_time of one batch:  -0.8828380107879639\n",
            "Iteration:  19225      Current loss:  22.569700241088867      Current accuracy:  25.0      Comp_time of one batch:  -0.8887944221496582\n",
            "Iteration:  19230      Current loss:  22.015140533447266      Current accuracy:  26.0      Comp_time of one batch:  -0.9088180065155029\n",
            "Iteration:  19235      Current loss:  25.47646141052246      Current accuracy:  26.0      Comp_time of one batch:  -0.9003238677978516\n",
            "Iteration:  19240      Current loss:  27.984830856323242      Current accuracy:  26.0      Comp_time of one batch:  -0.8889243602752686\n",
            "Iteration:  19245      Current loss:  23.084291458129883      Current accuracy:  27.0      Comp_time of one batch:  -0.9147505760192871\n",
            "Iteration:  19250      Current loss:  33.32863998413086      Current accuracy:  27.0      Comp_time of one batch:  -0.8892369270324707\n",
            "Iteration:  19255      Current loss:  30.844823837280273      Current accuracy:  27.0      Comp_time of one batch:  -0.8896825313568115\n",
            "Iteration:  19260      Current loss:  25.278234481811523      Current accuracy:  27.0      Comp_time of one batch:  -0.8701152801513672\n",
            "Iteration:  19265      Current loss:  36.83424377441406      Current accuracy:  27.0      Comp_time of one batch:  -0.8818774223327637\n",
            "Iteration:  19270      Current loss:  26.39862823486328      Current accuracy:  27.0      Comp_time of one batch:  -0.8842658996582031\n",
            "Iteration:  19275      Current loss:  18.664264678955078      Current accuracy:  28.0      Comp_time of one batch:  -0.9105145931243896\n",
            "Iteration:  19280      Current loss:  32.836692810058594      Current accuracy:  28.0      Comp_time of one batch:  -0.9113917350769043\n",
            "Iteration:  19285      Current loss:  27.300430297851562      Current accuracy:  28.0      Comp_time of one batch:  -0.8743865489959717\n",
            "Iteration:  19290      Current loss:  23.88041877746582      Current accuracy:  28.0      Comp_time of one batch:  -0.8914229869842529\n",
            "Iteration:  19295      Current loss:  26.451086044311523      Current accuracy:  28.0      Comp_time of one batch:  -0.8917667865753174\n",
            "Iteration:  19300      Current loss:  25.54595184326172      Current accuracy:  28.0      Comp_time of one batch:  -0.9046618938446045\n",
            "Iteration:  19305      Current loss:  17.068941116333008      Current accuracy:  29.0      Comp_time of one batch:  -0.8853750228881836\n",
            "Iteration:  19310      Current loss:  22.301687240600586      Current accuracy:  30.0      Comp_time of one batch:  -0.8836119174957275\n",
            "Iteration:  19315      Current loss:  21.626441955566406      Current accuracy:  31.0      Comp_time of one batch:  -0.9009530544281006\n",
            "Iteration:  19320      Current loss:  27.66483497619629      Current accuracy:  31.0      Comp_time of one batch:  -0.8876934051513672\n",
            "Iteration:  19325      Current loss:  22.69713020324707      Current accuracy:  32.0      Comp_time of one batch:  -0.9123740196228027\n",
            "Iteration:  19330      Current loss:  38.02963638305664      Current accuracy:  32.0      Comp_time of one batch:  -0.9009039402008057\n",
            "Iteration:  19335      Current loss:  26.97662925720215      Current accuracy:  32.0      Comp_time of one batch:  -0.9260940551757812\n",
            "Iteration:  19340      Current loss:  26.25492286682129      Current accuracy:  32.0      Comp_time of one batch:  -0.9190592765808105\n",
            "Iteration:  19345      Current loss:  28.57756805419922      Current accuracy:  32.0      Comp_time of one batch:  -0.8851306438446045\n",
            "Iteration:  19350      Current loss:  21.96805191040039      Current accuracy:  33.0      Comp_time of one batch:  -0.9014804363250732\n",
            "Iteration:  19355      Current loss:  34.1933708190918      Current accuracy:  33.0      Comp_time of one batch:  -0.9033639430999756\n",
            "Iteration:  19360      Current loss:  20.410137176513672      Current accuracy:  34.0      Comp_time of one batch:  -0.8760495185852051\n",
            "Iteration:  19365      Current loss:  22.259510040283203      Current accuracy:  34.0      Comp_time of one batch:  -0.9360272884368896\n",
            "Iteration:  19370      Current loss:  21.251670837402344      Current accuracy:  35.0      Comp_time of one batch:  -0.9192008972167969\n",
            "Iteration:  19375      Current loss:  19.274154663085938      Current accuracy:  36.0      Comp_time of one batch:  -0.8880631923675537\n",
            "Iteration:  19380      Current loss:  20.04022789001465      Current accuracy:  37.0      Comp_time of one batch:  -0.9041726589202881\n",
            "Iteration:  19385      Current loss:  32.68355178833008      Current accuracy:  37.0      Comp_time of one batch:  -0.8853724002838135\n",
            "Iteration:  19390      Current loss:  16.789804458618164      Current accuracy:  38.0      Comp_time of one batch:  -0.9068999290466309\n",
            "Iteration:  19395      Current loss:  38.04306411743164      Current accuracy:  38.0      Comp_time of one batch:  -0.8829011917114258\n",
            "Iteration:  19400      Current loss:  18.91970443725586      Current accuracy:  39.0      Comp_time of one batch:  -0.9004299640655518\n",
            "Iteration:  19405      Current loss:  27.886394500732422      Current accuracy:  39.0      Comp_time of one batch:  -0.8817250728607178\n",
            "Iteration:  19410      Current loss:  28.749393463134766      Current accuracy:  39.0      Comp_time of one batch:  -0.8955307006835938\n",
            "Iteration:  19415      Current loss:  22.95270347595215      Current accuracy:  40.0      Comp_time of one batch:  -0.882455587387085\n",
            "Iteration:  19420      Current loss:  23.694570541381836      Current accuracy:  41.0      Comp_time of one batch:  -0.8843362331390381\n",
            "Iteration:  19425      Current loss:  24.44496726989746      Current accuracy:  41.0      Comp_time of one batch:  -0.8821899890899658\n",
            "Iteration:  19430      Current loss:  29.458011627197266      Current accuracy:  41.0      Comp_time of one batch:  -0.8846991062164307\n",
            "Iteration:  19435      Current loss:  30.665355682373047      Current accuracy:  41.0      Comp_time of one batch:  -0.9067585468292236\n",
            "Iteration:  19440      Current loss:  31.21682357788086      Current accuracy:  41.0      Comp_time of one batch:  -0.9136147499084473\n",
            "Iteration:  19445      Current loss:  29.103561401367188      Current accuracy:  41.0      Comp_time of one batch:  -0.8999810218811035\n",
            "Iteration:  19450      Current loss:  24.384132385253906      Current accuracy:  42.0      Comp_time of one batch:  -0.8917384147644043\n",
            "Iteration:  19455      Current loss:  23.284950256347656      Current accuracy:  42.0      Comp_time of one batch:  -0.8884005546569824\n",
            "Iteration:  19460      Current loss:  21.96567153930664      Current accuracy:  43.0      Comp_time of one batch:  -0.8905529975891113\n",
            "Iteration:  19465      Current loss:  23.922204971313477      Current accuracy:  43.0      Comp_time of one batch:  -0.8713033199310303\n",
            "Iteration:  19470      Current loss:  17.596323013305664      Current accuracy:  44.0      Comp_time of one batch:  -0.8704583644866943\n",
            "Iteration:  19475      Current loss:  24.53518295288086      Current accuracy:  44.0      Comp_time of one batch:  -0.89141845703125\n",
            "Iteration:  19480      Current loss:  22.30494499206543      Current accuracy:  45.0      Comp_time of one batch:  -0.8874332904815674\n",
            "Iteration:  19485      Current loss:  26.304048538208008      Current accuracy:  45.0      Comp_time of one batch:  -0.931948184967041\n",
            "Iteration:  19490      Current loss:  22.401058197021484      Current accuracy:  46.0      Comp_time of one batch:  -0.8801615238189697\n",
            "Iteration:  19495      Current loss:  19.008785247802734      Current accuracy:  47.0      Comp_time of one batch:  -0.9142172336578369\n",
            "Iteration:  19500      Current loss:  25.572175979614258      Current accuracy:  0.0      Comp_time of one batch:  -0.8820152282714844\n",
            "Iteration:  19505      Current loss:  20.044763565063477      Current accuracy:  1.0      Comp_time of one batch:  -0.9000585079193115\n",
            "Iteration:  19510      Current loss:  18.085477828979492      Current accuracy:  2.0      Comp_time of one batch:  -0.9008362293243408\n",
            "Iteration:  19515      Current loss:  24.151029586791992      Current accuracy:  2.0      Comp_time of one batch:  -0.8903172016143799\n",
            "Iteration:  19520      Current loss:  28.153411865234375      Current accuracy:  2.0      Comp_time of one batch:  -0.886336088180542\n",
            "Iteration:  19525      Current loss:  29.014545440673828      Current accuracy:  2.0      Comp_time of one batch:  -0.8919920921325684\n",
            "Iteration:  19530      Current loss:  35.93418502807617      Current accuracy:  2.0      Comp_time of one batch:  -0.8829994201660156\n",
            "Iteration:  19535      Current loss:  25.037513732910156      Current accuracy:  2.0      Comp_time of one batch:  -0.8967406749725342\n",
            "Iteration:  19540      Current loss:  18.779521942138672      Current accuracy:  3.0      Comp_time of one batch:  -0.9052534103393555\n",
            "Iteration:  19545      Current loss:  24.52358627319336      Current accuracy:  3.0      Comp_time of one batch:  -0.9032578468322754\n",
            "Iteration:  19550      Current loss:  24.911449432373047      Current accuracy:  3.0      Comp_time of one batch:  -0.896507740020752\n",
            "Iteration:  19555      Current loss:  26.90448760986328      Current accuracy:  3.0      Comp_time of one batch:  -0.9115033149719238\n",
            "Iteration:  19560      Current loss:  23.83603858947754      Current accuracy:  4.0      Comp_time of one batch:  -0.8881704807281494\n",
            "Iteration:  19565      Current loss:  23.08827781677246      Current accuracy:  4.0      Comp_time of one batch:  -0.8862171173095703\n",
            "Iteration:  19570      Current loss:  24.404817581176758      Current accuracy:  5.0      Comp_time of one batch:  -0.8821749687194824\n",
            "Iteration:  19575      Current loss:  27.231979370117188      Current accuracy:  5.0      Comp_time of one batch:  -0.8817694187164307\n",
            "Iteration:  19580      Current loss:  23.685222625732422      Current accuracy:  6.0      Comp_time of one batch:  -0.8861417770385742\n",
            "Iteration:  19585      Current loss:  24.30358123779297      Current accuracy:  7.0      Comp_time of one batch:  -0.9164290428161621\n",
            "Iteration:  19590      Current loss:  31.223478317260742      Current accuracy:  7.0      Comp_time of one batch:  -0.887768030166626\n",
            "Iteration:  19595      Current loss:  21.551660537719727      Current accuracy:  8.0      Comp_time of one batch:  -0.9103362560272217\n",
            "Iteration:  19600      Current loss:  18.67143440246582      Current accuracy:  9.0      Comp_time of one batch:  -0.8954365253448486\n",
            "Iteration:  19605      Current loss:  24.780731201171875      Current accuracy:  9.0      Comp_time of one batch:  -0.8886356353759766\n",
            "Iteration:  19610      Current loss:  31.67003059387207      Current accuracy:  9.0      Comp_time of one batch:  -0.9348843097686768\n",
            "Iteration:  19615      Current loss:  27.951635360717773      Current accuracy:  9.0      Comp_time of one batch:  -0.9092767238616943\n",
            "Iteration:  19620      Current loss:  24.02704429626465      Current accuracy:  9.0      Comp_time of one batch:  -0.9064953327178955\n",
            "Iteration:  19625      Current loss:  18.97115707397461      Current accuracy:  10.0      Comp_time of one batch:  -0.9104809761047363\n",
            "Iteration:  19630      Current loss:  18.607234954833984      Current accuracy:  11.0      Comp_time of one batch:  -0.8851022720336914\n",
            "Iteration:  19635      Current loss:  19.126678466796875      Current accuracy:  12.0      Comp_time of one batch:  -0.8959863185882568\n",
            "Iteration:  19640      Current loss:  20.82826805114746      Current accuracy:  13.0      Comp_time of one batch:  -0.9353704452514648\n",
            "Iteration:  19645      Current loss:  24.28521728515625      Current accuracy:  13.0      Comp_time of one batch:  -0.9170377254486084\n",
            "Iteration:  19650      Current loss:  20.791885375976562      Current accuracy:  14.0      Comp_time of one batch:  -0.880037784576416\n",
            "Iteration:  19655      Current loss:  26.688501358032227      Current accuracy:  14.0      Comp_time of one batch:  -0.9017360210418701\n",
            "Iteration:  19660      Current loss:  21.483978271484375      Current accuracy:  15.0      Comp_time of one batch:  -0.87931227684021\n",
            "Iteration:  19665      Current loss:  30.880298614501953      Current accuracy:  15.0      Comp_time of one batch:  -0.9148077964782715\n",
            "Iteration:  19670      Current loss:  28.412336349487305      Current accuracy:  15.0      Comp_time of one batch:  -0.9076888561248779\n",
            "Iteration:  19675      Current loss:  23.801950454711914      Current accuracy:  15.0      Comp_time of one batch:  -0.8929595947265625\n",
            "Iteration:  19680      Current loss:  19.99944305419922      Current accuracy:  16.0      Comp_time of one batch:  -0.9036638736724854\n",
            "Iteration:  19685      Current loss:  20.83600425720215      Current accuracy:  17.0      Comp_time of one batch:  -0.9075531959533691\n",
            "Iteration:  19690      Current loss:  23.199560165405273      Current accuracy:  18.0      Comp_time of one batch:  -0.8873703479766846\n",
            "Iteration:  19695      Current loss:  28.04393196105957      Current accuracy:  18.0      Comp_time of one batch:  -0.9233810901641846\n",
            "Iteration:  19700      Current loss:  23.137216567993164      Current accuracy:  18.0      Comp_time of one batch:  -0.8903496265411377\n",
            "Iteration:  19705      Current loss:  23.065555572509766      Current accuracy:  18.0      Comp_time of one batch:  -0.9180233478546143\n",
            "Iteration:  19710      Current loss:  24.085023880004883      Current accuracy:  19.0      Comp_time of one batch:  -0.8763768672943115\n",
            "Iteration:  19715      Current loss:  25.02852439880371      Current accuracy:  19.0      Comp_time of one batch:  -0.8896520137786865\n",
            "Iteration:  19720      Current loss:  26.24432945251465      Current accuracy:  19.0      Comp_time of one batch:  -0.8809792995452881\n",
            "Iteration:  19725      Current loss:  26.983549118041992      Current accuracy:  19.0      Comp_time of one batch:  -0.8841581344604492\n",
            "Iteration:  19730      Current loss:  33.610443115234375      Current accuracy:  19.0      Comp_time of one batch:  -0.9185001850128174\n",
            "Iteration:  19735      Current loss:  23.66846466064453      Current accuracy:  19.0      Comp_time of one batch:  -0.8772799968719482\n",
            "Iteration:  19740      Current loss:  38.559364318847656      Current accuracy:  19.0      Comp_time of one batch:  -0.8933863639831543\n",
            "Iteration:  19745      Current loss:  25.332979202270508      Current accuracy:  19.0      Comp_time of one batch:  -0.9086034297943115\n",
            "Iteration:  19750      Current loss:  29.515695571899414      Current accuracy:  19.0      Comp_time of one batch:  -0.8962624073028564\n",
            "Iteration:  19755      Current loss:  18.937299728393555      Current accuracy:  20.0      Comp_time of one batch:  -0.8735520839691162\n",
            "Iteration:  19760      Current loss:  20.959196090698242      Current accuracy:  21.0      Comp_time of one batch:  -0.883734941482544\n",
            "Iteration:  19765      Current loss:  22.480636596679688      Current accuracy:  22.0      Comp_time of one batch:  -0.8842949867248535\n",
            "Iteration:  19770      Current loss:  21.42317771911621      Current accuracy:  23.0      Comp_time of one batch:  -0.9106478691101074\n",
            "Iteration:  19775      Current loss:  33.5730094909668      Current accuracy:  23.0      Comp_time of one batch:  -0.8983948230743408\n",
            "Iteration:  19780      Current loss:  22.510087966918945      Current accuracy:  24.0      Comp_time of one batch:  -0.8871731758117676\n",
            "Iteration:  19785      Current loss:  23.740663528442383      Current accuracy:  24.0      Comp_time of one batch:  -0.9142346382141113\n",
            "Iteration:  19790      Current loss:  22.895328521728516      Current accuracy:  25.0      Comp_time of one batch:  -0.8920238018035889\n",
            "Iteration:  19795      Current loss:  24.903886795043945      Current accuracy:  25.0      Comp_time of one batch:  -0.8668179512023926\n",
            "Iteration:  19800      Current loss:  19.464338302612305      Current accuracy:  26.0      Comp_time of one batch:  -0.8812494277954102\n",
            "Iteration:  19805      Current loss:  26.230308532714844      Current accuracy:  26.0      Comp_time of one batch:  -0.8774843215942383\n",
            "Iteration:  19810      Current loss:  20.077856063842773      Current accuracy:  27.0      Comp_time of one batch:  -0.8980815410614014\n",
            "Iteration:  19815      Current loss:  25.609067916870117      Current accuracy:  27.0      Comp_time of one batch:  -0.8935279846191406\n",
            "Iteration:  19820      Current loss:  26.194599151611328      Current accuracy:  27.0      Comp_time of one batch:  -0.8984723091125488\n",
            "Iteration:  19825      Current loss:  15.9618558883667      Current accuracy:  28.0      Comp_time of one batch:  -0.887444257736206\n",
            "Iteration:  19830      Current loss:  24.62271499633789      Current accuracy:  28.0      Comp_time of one batch:  -0.879777193069458\n",
            "Iteration:  19835      Current loss:  28.919618606567383      Current accuracy:  28.0      Comp_time of one batch:  -0.9019625186920166\n",
            "Iteration:  19840      Current loss:  18.997037887573242      Current accuracy:  29.0      Comp_time of one batch:  -0.9142391681671143\n",
            "Iteration:  19845      Current loss:  25.060060501098633      Current accuracy:  30.0      Comp_time of one batch:  -0.8737804889678955\n",
            "Iteration:  19850      Current loss:  17.692764282226562      Current accuracy:  31.0      Comp_time of one batch:  -0.9187455177307129\n",
            "Iteration:  19855      Current loss:  21.839637756347656      Current accuracy:  32.0      Comp_time of one batch:  -0.9121506214141846\n",
            "Iteration:  19860      Current loss:  15.490378379821777      Current accuracy:  33.0      Comp_time of one batch:  -0.912243127822876\n",
            "Iteration:  19865      Current loss:  22.65298080444336      Current accuracy:  33.0      Comp_time of one batch:  -0.8911755084991455\n",
            "Iteration:  19870      Current loss:  22.925052642822266      Current accuracy:  33.0      Comp_time of one batch:  -0.8836135864257812\n",
            "Iteration:  19875      Current loss:  26.13288116455078      Current accuracy:  33.0      Comp_time of one batch:  -0.8943543434143066\n",
            "Iteration:  19880      Current loss:  20.27381134033203      Current accuracy:  34.0      Comp_time of one batch:  -0.8807368278503418\n",
            "Iteration:  19885      Current loss:  20.32200813293457      Current accuracy:  35.0      Comp_time of one batch:  -0.8830351829528809\n",
            "Iteration:  19890      Current loss:  23.206438064575195      Current accuracy:  36.0      Comp_time of one batch:  -0.8991529941558838\n",
            "Iteration:  19895      Current loss:  24.6142635345459      Current accuracy:  36.0      Comp_time of one batch:  -0.8775844573974609\n",
            "Iteration:  19900      Current loss:  27.173538208007812      Current accuracy:  36.0      Comp_time of one batch:  -0.891585111618042\n",
            "Iteration:  19905      Current loss:  26.040449142456055      Current accuracy:  36.0      Comp_time of one batch:  -0.8808205127716064\n",
            "Iteration:  19910      Current loss:  31.020305633544922      Current accuracy:  36.0      Comp_time of one batch:  -0.8968040943145752\n",
            "Iteration:  19915      Current loss:  30.067014694213867      Current accuracy:  36.0      Comp_time of one batch:  -0.894146203994751\n",
            "Iteration:  19920      Current loss:  26.03340721130371      Current accuracy:  36.0      Comp_time of one batch:  -0.9177985191345215\n",
            "Iteration:  19925      Current loss:  22.1651554107666      Current accuracy:  37.0      Comp_time of one batch:  -0.9058363437652588\n",
            "Iteration:  19930      Current loss:  19.550844192504883      Current accuracy:  38.0      Comp_time of one batch:  -0.8990225791931152\n",
            "Iteration:  19935      Current loss:  28.022567749023438      Current accuracy:  38.0      Comp_time of one batch:  -0.8926904201507568\n",
            "Iteration:  19940      Current loss:  36.85075378417969      Current accuracy:  38.0      Comp_time of one batch:  -0.8765659332275391\n",
            "Iteration:  19945      Current loss:  22.675100326538086      Current accuracy:  39.0      Comp_time of one batch:  -0.8934543132781982\n",
            "Iteration:  19950      Current loss:  24.2292423248291      Current accuracy:  39.0      Comp_time of one batch:  -0.8915376663208008\n",
            "Iteration:  19955      Current loss:  30.657651901245117      Current accuracy:  39.0      Comp_time of one batch:  -0.8862051963806152\n",
            "Iteration:  19960      Current loss:  20.743711471557617      Current accuracy:  40.0      Comp_time of one batch:  -0.8808243274688721\n",
            "Iteration:  19965      Current loss:  27.583219528198242      Current accuracy:  40.0      Comp_time of one batch:  -0.9096167087554932\n",
            "Iteration:  19970      Current loss:  20.944265365600586      Current accuracy:  41.0      Comp_time of one batch:  -0.8865385055541992\n",
            "Iteration:  19975      Current loss:  25.200918197631836      Current accuracy:  41.0      Comp_time of one batch:  -0.8890280723571777\n",
            "Iteration:  19980      Current loss:  22.21769142150879      Current accuracy:  42.0      Comp_time of one batch:  -0.8905148506164551\n",
            "Iteration:  19985      Current loss:  20.26471519470215      Current accuracy:  43.0      Comp_time of one batch:  -0.8954911231994629\n",
            "Iteration:  19990      Current loss:  30.836057662963867      Current accuracy:  43.0      Comp_time of one batch:  -0.8908121585845947\n",
            "Iteration:  19995      Current loss:  20.279516220092773      Current accuracy:  44.0      Comp_time of one batch:  -0.9078021049499512\n",
            "Iteration:  20000      Current loss:  51.849124908447266      Current accuracy:  0.0      Comp_time of one batch:  -0.9047601222991943\n",
            "Iteration:  20005      Current loss:  25.019367218017578      Current accuracy:  0.0      Comp_time of one batch:  -0.8802974224090576\n",
            "Iteration:  20010      Current loss:  23.907995223999023      Current accuracy:  0.0      Comp_time of one batch:  -0.8837029933929443\n",
            "Iteration:  20015      Current loss:  17.905590057373047      Current accuracy:  1.0      Comp_time of one batch:  -0.8900091648101807\n",
            "Iteration:  20020      Current loss:  21.95047378540039      Current accuracy:  2.0      Comp_time of one batch:  -0.8798031806945801\n",
            "Iteration:  20025      Current loss:  25.953712463378906      Current accuracy:  2.0      Comp_time of one batch:  -0.8827307224273682\n",
            "Iteration:  20030      Current loss:  34.77363967895508      Current accuracy:  2.0      Comp_time of one batch:  -0.881293773651123\n",
            "Iteration:  20035      Current loss:  21.830520629882812      Current accuracy:  3.0      Comp_time of one batch:  -0.8919498920440674\n",
            "Iteration:  20040      Current loss:  24.459001541137695      Current accuracy:  3.0      Comp_time of one batch:  -0.8883397579193115\n",
            "Iteration:  20045      Current loss:  28.210872650146484      Current accuracy:  3.0      Comp_time of one batch:  -0.8837568759918213\n",
            "Iteration:  20050      Current loss:  25.306398391723633      Current accuracy:  3.0      Comp_time of one batch:  -0.8749384880065918\n",
            "Iteration:  20055      Current loss:  22.537893295288086      Current accuracy:  4.0      Comp_time of one batch:  -0.8797247409820557\n",
            "Iteration:  20060      Current loss:  17.726484298706055      Current accuracy:  5.0      Comp_time of one batch:  -0.8862009048461914\n",
            "Iteration:  20065      Current loss:  31.679637908935547      Current accuracy:  5.0      Comp_time of one batch:  -0.8950786590576172\n",
            "Iteration:  20070      Current loss:  29.811697006225586      Current accuracy:  5.0      Comp_time of one batch:  -0.8718538284301758\n",
            "Iteration:  20075      Current loss:  25.585559844970703      Current accuracy:  5.0      Comp_time of one batch:  -0.9087889194488525\n",
            "Iteration:  20080      Current loss:  35.365257263183594      Current accuracy:  5.0      Comp_time of one batch:  -0.8983981609344482\n",
            "Iteration:  20085      Current loss:  27.801101684570312      Current accuracy:  5.0      Comp_time of one batch:  -0.9113869667053223\n",
            "Iteration:  20090      Current loss:  27.334096908569336      Current accuracy:  5.0      Comp_time of one batch:  -0.893160343170166\n",
            "Iteration:  20095      Current loss:  28.29053497314453      Current accuracy:  5.0      Comp_time of one batch:  -0.8959183692932129\n",
            "Iteration:  20100      Current loss:  20.425045013427734      Current accuracy:  6.0      Comp_time of one batch:  -0.8992831707000732\n",
            "Iteration:  20105      Current loss:  26.76620864868164      Current accuracy:  6.0      Comp_time of one batch:  -0.9002988338470459\n",
            "Iteration:  20110      Current loss:  23.915903091430664      Current accuracy:  7.0      Comp_time of one batch:  -0.9169766902923584\n",
            "Iteration:  20115      Current loss:  22.24371337890625      Current accuracy:  8.0      Comp_time of one batch:  -0.8863193988800049\n",
            "Iteration:  20120      Current loss:  25.244335174560547      Current accuracy:  9.0      Comp_time of one batch:  -0.8974514007568359\n",
            "Iteration:  20125      Current loss:  38.35187911987305      Current accuracy:  9.0      Comp_time of one batch:  -0.8973085880279541\n",
            "Iteration:  20130      Current loss:  24.982664108276367      Current accuracy:  9.0      Comp_time of one batch:  -0.8828818798065186\n",
            "Iteration:  20135      Current loss:  24.7457332611084      Current accuracy:  9.0      Comp_time of one batch:  -0.8885159492492676\n",
            "Iteration:  20140      Current loss:  23.44658660888672      Current accuracy:  10.0      Comp_time of one batch:  -0.9211182594299316\n",
            "Iteration:  20145      Current loss:  21.762739181518555      Current accuracy:  11.0      Comp_time of one batch:  -0.8984382152557373\n",
            "Iteration:  20150      Current loss:  28.508970260620117      Current accuracy:  11.0      Comp_time of one batch:  -0.8960623741149902\n",
            "Iteration:  20155      Current loss:  23.47077751159668      Current accuracy:  11.0      Comp_time of one batch:  -0.9287145137786865\n",
            "Iteration:  20160      Current loss:  27.251808166503906      Current accuracy:  11.0      Comp_time of one batch:  -0.872610330581665\n",
            "Iteration:  20165      Current loss:  20.87785530090332      Current accuracy:  12.0      Comp_time of one batch:  -0.90728759765625\n",
            "Iteration:  20170      Current loss:  23.566471099853516      Current accuracy:  13.0      Comp_time of one batch:  -0.8968682289123535\n",
            "Iteration:  20175      Current loss:  23.33777618408203      Current accuracy:  14.0      Comp_time of one batch:  -0.8953371047973633\n",
            "Iteration:  20180      Current loss:  19.887704849243164      Current accuracy:  15.0      Comp_time of one batch:  -0.8710644245147705\n",
            "Iteration:  20185      Current loss:  30.25063133239746      Current accuracy:  15.0      Comp_time of one batch:  -0.8861205577850342\n",
            "Iteration:  20190      Current loss:  15.731423377990723      Current accuracy:  16.0      Comp_time of one batch:  -0.8874602317810059\n",
            "Iteration:  20195      Current loss:  22.890024185180664      Current accuracy:  16.0      Comp_time of one batch:  -0.915050745010376\n",
            "Iteration:  20200      Current loss:  20.961406707763672      Current accuracy:  17.0      Comp_time of one batch:  -0.8976080417633057\n",
            "Iteration:  20205      Current loss:  27.001537322998047      Current accuracy:  17.0      Comp_time of one batch:  -0.9053158760070801\n",
            "Iteration:  20210      Current loss:  29.545074462890625      Current accuracy:  17.0      Comp_time of one batch:  -0.9030046463012695\n",
            "Iteration:  20215      Current loss:  19.420475006103516      Current accuracy:  18.0      Comp_time of one batch:  -0.8872466087341309\n",
            "Iteration:  20220      Current loss:  24.392004013061523      Current accuracy:  18.0      Comp_time of one batch:  -0.8697459697723389\n",
            "Iteration:  20225      Current loss:  30.35031509399414      Current accuracy:  18.0      Comp_time of one batch:  -0.8846700191497803\n",
            "Iteration:  20230      Current loss:  24.423389434814453      Current accuracy:  19.0      Comp_time of one batch:  -0.9100043773651123\n",
            "Iteration:  20235      Current loss:  22.029830932617188      Current accuracy:  20.0      Comp_time of one batch:  -0.8908274173736572\n",
            "Iteration:  20240      Current loss:  24.224559783935547      Current accuracy:  20.0      Comp_time of one batch:  -0.8950815200805664\n",
            "Iteration:  20245      Current loss:  33.73191452026367      Current accuracy:  20.0      Comp_time of one batch:  -0.8904387950897217\n",
            "Iteration:  20250      Current loss:  23.557981491088867      Current accuracy:  20.0      Comp_time of one batch:  -0.9067418575286865\n",
            "Iteration:  20255      Current loss:  29.42003631591797      Current accuracy:  20.0      Comp_time of one batch:  -0.8965084552764893\n",
            "Iteration:  20260      Current loss:  26.993314743041992      Current accuracy:  20.0      Comp_time of one batch:  -0.8881604671478271\n",
            "Iteration:  20265      Current loss:  21.368051528930664      Current accuracy:  21.0      Comp_time of one batch:  -0.891669750213623\n",
            "Iteration:  20270      Current loss:  25.12679100036621      Current accuracy:  21.0      Comp_time of one batch:  -0.899336576461792\n",
            "Iteration:  20275      Current loss:  29.779394149780273      Current accuracy:  21.0      Comp_time of one batch:  -0.9044990539550781\n",
            "Iteration:  20280      Current loss:  37.49934768676758      Current accuracy:  21.0      Comp_time of one batch:  -0.8948574066162109\n",
            "Iteration:  20285      Current loss:  26.12518882751465      Current accuracy:  21.0      Comp_time of one batch:  -0.8857192993164062\n",
            "Iteration:  20290      Current loss:  21.4632511138916      Current accuracy:  22.0      Comp_time of one batch:  -0.8760218620300293\n",
            "Iteration:  20295      Current loss:  24.561588287353516      Current accuracy:  22.0      Comp_time of one batch:  -0.8971982002258301\n",
            "Iteration:  20300      Current loss:  23.400362014770508      Current accuracy:  23.0      Comp_time of one batch:  -0.8839683532714844\n",
            "Iteration:  20305      Current loss:  22.892274856567383      Current accuracy:  23.0      Comp_time of one batch:  -0.9066143035888672\n",
            "Iteration:  20310      Current loss:  18.974761962890625      Current accuracy:  24.0      Comp_time of one batch:  -0.8816559314727783\n",
            "Iteration:  20315      Current loss:  25.155061721801758      Current accuracy:  24.0      Comp_time of one batch:  -0.8930065631866455\n",
            "Iteration:  20320      Current loss:  19.482818603515625      Current accuracy:  25.0      Comp_time of one batch:  -0.8981595039367676\n",
            "Iteration:  20325      Current loss:  19.55190658569336      Current accuracy:  26.0      Comp_time of one batch:  -0.8801624774932861\n",
            "Iteration:  20330      Current loss:  19.128023147583008      Current accuracy:  27.0      Comp_time of one batch:  -0.8829731941223145\n",
            "Iteration:  20335      Current loss:  31.79525375366211      Current accuracy:  27.0      Comp_time of one batch:  -0.8820946216583252\n",
            "Iteration:  20340      Current loss:  24.49787712097168      Current accuracy:  27.0      Comp_time of one batch:  -0.8954286575317383\n",
            "Iteration:  20345      Current loss:  24.292951583862305      Current accuracy:  27.0      Comp_time of one batch:  -0.8955650329589844\n",
            "Iteration:  20350      Current loss:  24.61990737915039      Current accuracy:  27.0      Comp_time of one batch:  -0.8834521770477295\n",
            "Iteration:  20355      Current loss:  30.432950973510742      Current accuracy:  27.0      Comp_time of one batch:  -0.8808801174163818\n",
            "Iteration:  20360      Current loss:  25.764345169067383      Current accuracy:  27.0      Comp_time of one batch:  -0.9173617362976074\n",
            "Iteration:  20365      Current loss:  20.912025451660156      Current accuracy:  28.0      Comp_time of one batch:  -0.8974251747131348\n",
            "Iteration:  20370      Current loss:  25.008243560791016      Current accuracy:  28.0      Comp_time of one batch:  -0.8805325031280518\n",
            "Iteration:  20375      Current loss:  30.87616539001465      Current accuracy:  28.0      Comp_time of one batch:  -0.90077805519104\n",
            "Iteration:  20380      Current loss:  20.028409957885742      Current accuracy:  29.0      Comp_time of one batch:  -0.9023375511169434\n",
            "Iteration:  20385      Current loss:  24.71371078491211      Current accuracy:  29.0      Comp_time of one batch:  -0.885812520980835\n",
            "Iteration:  20390      Current loss:  25.41680335998535      Current accuracy:  29.0      Comp_time of one batch:  -0.8958754539489746\n",
            "Iteration:  20395      Current loss:  26.834184646606445      Current accuracy:  29.0      Comp_time of one batch:  -0.8853776454925537\n",
            "Iteration:  20400      Current loss:  32.19624710083008      Current accuracy:  29.0      Comp_time of one batch:  -0.8887028694152832\n",
            "Iteration:  20405      Current loss:  20.280353546142578      Current accuracy:  30.0      Comp_time of one batch:  -0.8936872482299805\n",
            "Iteration:  20410      Current loss:  22.309175491333008      Current accuracy:  31.0      Comp_time of one batch:  -0.8857645988464355\n",
            "Iteration:  20415      Current loss:  25.479995727539062      Current accuracy:  31.0      Comp_time of one batch:  -0.89385986328125\n",
            "Iteration:  20420      Current loss:  25.96137046813965      Current accuracy:  31.0      Comp_time of one batch:  -0.8878872394561768\n",
            "Iteration:  20425      Current loss:  30.540607452392578      Current accuracy:  31.0      Comp_time of one batch:  -0.8840477466583252\n",
            "Iteration:  20430      Current loss:  20.875951766967773      Current accuracy:  32.0      Comp_time of one batch:  -0.8798689842224121\n",
            "Iteration:  20435      Current loss:  23.76546859741211      Current accuracy:  32.0      Comp_time of one batch:  -0.8883576393127441\n",
            "Iteration:  20440      Current loss:  26.56536293029785      Current accuracy:  32.0      Comp_time of one batch:  -0.899956464767456\n",
            "Iteration:  20445      Current loss:  20.518550872802734      Current accuracy:  33.0      Comp_time of one batch:  -0.8953268527984619\n",
            "Iteration:  20450      Current loss:  18.547164916992188      Current accuracy:  34.0      Comp_time of one batch:  -0.9189577102661133\n",
            "Iteration:  20455      Current loss:  26.7498722076416      Current accuracy:  34.0      Comp_time of one batch:  -0.8941495418548584\n",
            "Iteration:  20460      Current loss:  23.816051483154297      Current accuracy:  34.0      Comp_time of one batch:  -0.9162840843200684\n",
            "Iteration:  20465      Current loss:  25.790414810180664      Current accuracy:  34.0      Comp_time of one batch:  -0.8715887069702148\n",
            "Iteration:  20470      Current loss:  19.72306251525879      Current accuracy:  35.0      Comp_time of one batch:  -0.8845863342285156\n",
            "Iteration:  20475      Current loss:  27.070573806762695      Current accuracy:  35.0      Comp_time of one batch:  -0.8871190547943115\n",
            "Iteration:  20480      Current loss:  23.288347244262695      Current accuracy:  36.0      Comp_time of one batch:  -0.8885445594787598\n",
            "Iteration:  20485      Current loss:  25.14088249206543      Current accuracy:  36.0      Comp_time of one batch:  -0.9078660011291504\n",
            "Iteration:  20490      Current loss:  29.739423751831055      Current accuracy:  36.0      Comp_time of one batch:  -0.8855526447296143\n",
            "Iteration:  20495      Current loss:  30.09588623046875      Current accuracy:  36.0      Comp_time of one batch:  -0.9065558910369873\n",
            "Iteration:  20500      Current loss:  19.69760513305664      Current accuracy:  0.0      Comp_time of one batch:  -0.9200189113616943\n",
            "Iteration:  20505      Current loss:  31.635984420776367      Current accuracy:  0.0      Comp_time of one batch:  -0.8770713806152344\n",
            "Iteration:  20510      Current loss:  22.851360321044922      Current accuracy:  1.0      Comp_time of one batch:  -0.9104440212249756\n",
            "Iteration:  20515      Current loss:  19.173030853271484      Current accuracy:  2.0      Comp_time of one batch:  -0.88189697265625\n",
            "Iteration:  20520      Current loss:  24.479490280151367      Current accuracy:  2.0      Comp_time of one batch:  -0.9155244827270508\n",
            "Iteration:  20525      Current loss:  23.655569076538086      Current accuracy:  3.0      Comp_time of one batch:  -0.8821742534637451\n",
            "Iteration:  20530      Current loss:  22.190412521362305      Current accuracy:  4.0      Comp_time of one batch:  -0.8993823528289795\n",
            "Iteration:  20535      Current loss:  26.91559600830078      Current accuracy:  4.0      Comp_time of one batch:  -0.9100444316864014\n",
            "Iteration:  20540      Current loss:  17.67989730834961      Current accuracy:  5.0      Comp_time of one batch:  -0.904705286026001\n",
            "Iteration:  20545      Current loss:  38.14453887939453      Current accuracy:  5.0      Comp_time of one batch:  -0.9091811180114746\n",
            "Iteration:  20550      Current loss:  21.47871971130371      Current accuracy:  6.0      Comp_time of one batch:  -0.8995306491851807\n",
            "Iteration:  20555      Current loss:  20.279077529907227      Current accuracy:  7.0      Comp_time of one batch:  -0.8816161155700684\n",
            "Iteration:  20560      Current loss:  18.271657943725586      Current accuracy:  8.0      Comp_time of one batch:  -0.906846284866333\n",
            "Iteration:  20565      Current loss:  20.057403564453125      Current accuracy:  9.0      Comp_time of one batch:  -0.8840203285217285\n",
            "Iteration:  20570      Current loss:  24.84769058227539      Current accuracy:  9.0      Comp_time of one batch:  -0.8858935832977295\n",
            "Iteration:  20575      Current loss:  23.763383865356445      Current accuracy:  9.0      Comp_time of one batch:  -0.90972900390625\n",
            "Iteration:  20580      Current loss:  29.297021865844727      Current accuracy:  9.0      Comp_time of one batch:  -0.888479471206665\n",
            "Iteration:  20585      Current loss:  19.6686954498291      Current accuracy:  10.0      Comp_time of one batch:  -0.9000790119171143\n",
            "Iteration:  20590      Current loss:  26.87030601501465      Current accuracy:  10.0      Comp_time of one batch:  -0.9070711135864258\n",
            "Iteration:  20595      Current loss:  22.394832611083984      Current accuracy:  10.0      Comp_time of one batch:  -0.9058470726013184\n",
            "Iteration:  20600      Current loss:  19.431568145751953      Current accuracy:  11.0      Comp_time of one batch:  -0.89031982421875\n",
            "Iteration:  20605      Current loss:  27.853185653686523      Current accuracy:  11.0      Comp_time of one batch:  -0.9004418849945068\n",
            "Iteration:  20610      Current loss:  19.978839874267578      Current accuracy:  12.0      Comp_time of one batch:  -0.8807649612426758\n",
            "Iteration:  20615      Current loss:  31.44898223876953      Current accuracy:  12.0      Comp_time of one batch:  -0.8963301181793213\n",
            "Iteration:  20620      Current loss:  23.86127281188965      Current accuracy:  13.0      Comp_time of one batch:  -0.8960561752319336\n",
            "Iteration:  20625      Current loss:  25.857341766357422      Current accuracy:  13.0      Comp_time of one batch:  -0.9112932682037354\n",
            "Iteration:  20630      Current loss:  21.40579605102539      Current accuracy:  14.0      Comp_time of one batch:  -0.8865382671356201\n",
            "Iteration:  20635      Current loss:  23.128454208374023      Current accuracy:  15.0      Comp_time of one batch:  -0.8798027038574219\n",
            "Iteration:  20640      Current loss:  25.62308120727539      Current accuracy:  15.0      Comp_time of one batch:  -0.9084897041320801\n",
            "Iteration:  20645      Current loss:  28.42916488647461      Current accuracy:  15.0      Comp_time of one batch:  -0.8912074565887451\n",
            "Iteration:  20650      Current loss:  37.0797233581543      Current accuracy:  15.0      Comp_time of one batch:  -0.8883833885192871\n",
            "Iteration:  20655      Current loss:  17.53397560119629      Current accuracy:  16.0      Comp_time of one batch:  -0.8931901454925537\n",
            "Iteration:  20660      Current loss:  33.536354064941406      Current accuracy:  16.0      Comp_time of one batch:  -0.8827335834503174\n",
            "Iteration:  20665      Current loss:  22.281314849853516      Current accuracy:  17.0      Comp_time of one batch:  -0.8695704936981201\n",
            "Iteration:  20670      Current loss:  25.067049026489258      Current accuracy:  17.0      Comp_time of one batch:  -0.8777279853820801\n",
            "Iteration:  20675      Current loss:  21.619640350341797      Current accuracy:  18.0      Comp_time of one batch:  -0.8781466484069824\n",
            "Iteration:  20680      Current loss:  23.528831481933594      Current accuracy:  19.0      Comp_time of one batch:  -0.8821816444396973\n",
            "Iteration:  20685      Current loss:  21.015470504760742      Current accuracy:  20.0      Comp_time of one batch:  -0.8841629028320312\n",
            "Iteration:  20690      Current loss:  23.156219482421875      Current accuracy:  21.0      Comp_time of one batch:  -0.8938190937042236\n",
            "Iteration:  20695      Current loss:  21.794309616088867      Current accuracy:  22.0      Comp_time of one batch:  -0.9358701705932617\n",
            "Iteration:  20700      Current loss:  24.53183937072754      Current accuracy:  22.0      Comp_time of one batch:  -0.8883566856384277\n",
            "Iteration:  20705      Current loss:  30.607032775878906      Current accuracy:  22.0      Comp_time of one batch:  -0.878920316696167\n",
            "Iteration:  20710      Current loss:  31.059463500976562      Current accuracy:  22.0      Comp_time of one batch:  -0.8758621215820312\n",
            "Iteration:  20715      Current loss:  21.047496795654297      Current accuracy:  23.0      Comp_time of one batch:  -0.9018020629882812\n",
            "Iteration:  20720      Current loss:  20.660228729248047      Current accuracy:  24.0      Comp_time of one batch:  -0.8654835224151611\n",
            "Iteration:  20725      Current loss:  21.681198120117188      Current accuracy:  25.0      Comp_time of one batch:  -0.8773367404937744\n",
            "Iteration:  20730      Current loss:  23.33768653869629      Current accuracy:  26.0      Comp_time of one batch:  -0.9059951305389404\n",
            "Iteration:  20735      Current loss:  26.495267868041992      Current accuracy:  26.0      Comp_time of one batch:  -0.8955414295196533\n",
            "Iteration:  20740      Current loss:  18.271265029907227      Current accuracy:  27.0      Comp_time of one batch:  -0.8808333873748779\n",
            "Iteration:  20745      Current loss:  27.764728546142578      Current accuracy:  27.0      Comp_time of one batch:  -0.911482572555542\n",
            "Iteration:  20750      Current loss:  19.36100196838379      Current accuracy:  28.0      Comp_time of one batch:  -0.905925989151001\n",
            "Iteration:  20755      Current loss:  20.14201545715332      Current accuracy:  29.0      Comp_time of one batch:  -0.8885130882263184\n",
            "Iteration:  20760      Current loss:  22.570531845092773      Current accuracy:  30.0      Comp_time of one batch:  -0.883934497833252\n",
            "Iteration:  20765      Current loss:  28.16913604736328      Current accuracy:  30.0      Comp_time of one batch:  -0.8842554092407227\n",
            "Iteration:  20770      Current loss:  24.73792266845703      Current accuracy:  31.0      Comp_time of one batch:  -0.8724074363708496\n",
            "Iteration:  20775      Current loss:  21.686599731445312      Current accuracy:  32.0      Comp_time of one batch:  -0.918372631072998\n",
            "Iteration:  20780      Current loss:  26.98226547241211      Current accuracy:  32.0      Comp_time of one batch:  -0.8894422054290771\n",
            "Iteration:  20785      Current loss:  20.304779052734375      Current accuracy:  33.0      Comp_time of one batch:  -0.8943545818328857\n",
            "Iteration:  20790      Current loss:  17.34848403930664      Current accuracy:  34.0      Comp_time of one batch:  -0.9026029109954834\n",
            "Iteration:  20795      Current loss:  22.479957580566406      Current accuracy:  35.0      Comp_time of one batch:  -0.8776965141296387\n",
            "Iteration:  20800      Current loss:  27.422264099121094      Current accuracy:  35.0      Comp_time of one batch:  -0.8883941173553467\n",
            "Iteration:  20805      Current loss:  28.943410873413086      Current accuracy:  35.0      Comp_time of one batch:  -0.87888503074646\n",
            "Iteration:  20810      Current loss:  16.68505859375      Current accuracy:  36.0      Comp_time of one batch:  -0.8835277557373047\n",
            "Iteration:  20815      Current loss:  19.636180877685547      Current accuracy:  37.0      Comp_time of one batch:  -0.8871266841888428\n",
            "Iteration:  20820      Current loss:  42.814205169677734      Current accuracy:  37.0      Comp_time of one batch:  -0.9012553691864014\n",
            "Iteration:  20825      Current loss:  22.21843910217285      Current accuracy:  38.0      Comp_time of one batch:  -0.8904001712799072\n",
            "Iteration:  20830      Current loss:  25.41045570373535      Current accuracy:  38.0      Comp_time of one batch:  -0.8814456462860107\n",
            "Iteration:  20835      Current loss:  27.891355514526367      Current accuracy:  38.0      Comp_time of one batch:  -0.8839201927185059\n",
            "Iteration:  20840      Current loss:  37.1493034362793      Current accuracy:  38.0      Comp_time of one batch:  -0.8822073936462402\n",
            "Iteration:  20845      Current loss:  17.299419403076172      Current accuracy:  39.0      Comp_time of one batch:  -0.8955309391021729\n",
            "Iteration:  20850      Current loss:  21.445106506347656      Current accuracy:  40.0      Comp_time of one batch:  -0.887890100479126\n",
            "Iteration:  20855      Current loss:  21.895408630371094      Current accuracy:  41.0      Comp_time of one batch:  -0.8891172409057617\n",
            "Iteration:  20860      Current loss:  20.246583938598633      Current accuracy:  42.0      Comp_time of one batch:  -0.8862824440002441\n",
            "Iteration:  20865      Current loss:  16.460737228393555      Current accuracy:  43.0      Comp_time of one batch:  -0.8913900852203369\n",
            "Iteration:  20870      Current loss:  23.67294692993164      Current accuracy:  43.0      Comp_time of one batch:  -0.8904008865356445\n",
            "Iteration:  20875      Current loss:  24.70793914794922      Current accuracy:  43.0      Comp_time of one batch:  -0.8863668441772461\n",
            "Iteration:  20880      Current loss:  28.321422576904297      Current accuracy:  43.0      Comp_time of one batch:  -0.8913617134094238\n",
            "Iteration:  20885      Current loss:  16.303993225097656      Current accuracy:  44.0      Comp_time of one batch:  -0.884589433670044\n",
            "Iteration:  20890      Current loss:  25.995771408081055      Current accuracy:  44.0      Comp_time of one batch:  -0.887925386428833\n",
            "Iteration:  20895      Current loss:  27.35770606994629      Current accuracy:  44.0      Comp_time of one batch:  -0.8874170780181885\n",
            "Iteration:  20900      Current loss:  32.41974639892578      Current accuracy:  44.0      Comp_time of one batch:  -0.8906259536743164\n",
            "Iteration:  20905      Current loss:  31.505413055419922      Current accuracy:  44.0      Comp_time of one batch:  -0.887920618057251\n",
            "Iteration:  20910      Current loss:  22.3020076751709      Current accuracy:  44.0      Comp_time of one batch:  -0.8849117755889893\n",
            "Iteration:  20915      Current loss:  22.264089584350586      Current accuracy:  45.0      Comp_time of one batch:  -0.865339994430542\n",
            "Iteration:  20920      Current loss:  27.974992752075195      Current accuracy:  45.0      Comp_time of one batch:  -0.8868889808654785\n",
            "Iteration:  20925      Current loss:  24.603113174438477      Current accuracy:  46.0      Comp_time of one batch:  -0.8867065906524658\n",
            "Iteration:  20930      Current loss:  21.170930862426758      Current accuracy:  47.0      Comp_time of one batch:  -0.8813760280609131\n",
            "Iteration:  20935      Current loss:  21.27503776550293      Current accuracy:  48.0      Comp_time of one batch:  -0.890282154083252\n",
            "Iteration:  20940      Current loss:  28.07093048095703      Current accuracy:  48.0      Comp_time of one batch:  -0.888033390045166\n",
            "Iteration:  20945      Current loss:  23.095802307128906      Current accuracy:  48.0      Comp_time of one batch:  -0.8938846588134766\n",
            "Iteration:  20950      Current loss:  25.031967163085938      Current accuracy:  48.0      Comp_time of one batch:  -0.8879222869873047\n",
            "Iteration:  20955      Current loss:  17.188772201538086      Current accuracy:  49.0      Comp_time of one batch:  -0.8991055488586426\n",
            "Iteration:  20960      Current loss:  33.66516876220703      Current accuracy:  49.0      Comp_time of one batch:  -0.8890557289123535\n",
            "Iteration:  20965      Current loss:  30.127073287963867      Current accuracy:  49.0      Comp_time of one batch:  -0.8939976692199707\n",
            "Iteration:  20970      Current loss:  22.74125099182129      Current accuracy:  50.0      Comp_time of one batch:  -0.8798139095306396\n",
            "Iteration:  20975      Current loss:  25.56595802307129      Current accuracy:  50.0      Comp_time of one batch:  -0.8865392208099365\n",
            "Iteration:  20980      Current loss:  29.062095642089844      Current accuracy:  50.0      Comp_time of one batch:  -0.8849184513092041\n",
            "Iteration:  20985      Current loss:  17.190696716308594      Current accuracy:  51.0      Comp_time of one batch:  -0.9098362922668457\n",
            "Iteration:  20990      Current loss:  19.770898818969727      Current accuracy:  52.0      Comp_time of one batch:  -0.8933720588684082\n",
            "Iteration:  20995      Current loss:  19.707294464111328      Current accuracy:  53.0      Comp_time of one batch:  -0.8961260318756104\n",
            "Iteration:  21000      Current loss:  32.271568298339844      Current accuracy:  0.0      Comp_time of one batch:  -0.8971776962280273\n",
            "Iteration:  21005      Current loss:  20.908981323242188      Current accuracy:  1.0      Comp_time of one batch:  -0.8809554576873779\n",
            "Iteration:  21010      Current loss:  27.05727767944336      Current accuracy:  1.0      Comp_time of one batch:  -0.8926143646240234\n",
            "Iteration:  21015      Current loss:  22.52008819580078      Current accuracy:  2.0      Comp_time of one batch:  -0.903872013092041\n",
            "Iteration:  21020      Current loss:  35.424659729003906      Current accuracy:  2.0      Comp_time of one batch:  -0.9072034358978271\n",
            "Iteration:  21025      Current loss:  27.48091697692871      Current accuracy:  2.0      Comp_time of one batch:  -0.8994903564453125\n",
            "Iteration:  21030      Current loss:  21.79597282409668      Current accuracy:  3.0      Comp_time of one batch:  -0.9135510921478271\n",
            "Iteration:  21035      Current loss:  27.16971206665039      Current accuracy:  3.0      Comp_time of one batch:  -0.8921079635620117\n",
            "Iteration:  21040      Current loss:  22.752737045288086      Current accuracy:  3.0      Comp_time of one batch:  -0.9033212661743164\n",
            "Iteration:  21045      Current loss:  24.92616081237793      Current accuracy:  3.0      Comp_time of one batch:  -0.9227242469787598\n",
            "Iteration:  21050      Current loss:  31.692720413208008      Current accuracy:  3.0      Comp_time of one batch:  -0.8866937160491943\n",
            "Iteration:  21055      Current loss:  19.193950653076172      Current accuracy:  4.0      Comp_time of one batch:  -0.9008314609527588\n",
            "Iteration:  21060      Current loss:  23.593130111694336      Current accuracy:  4.0      Comp_time of one batch:  -0.8710098266601562\n",
            "Iteration:  21065      Current loss:  24.934219360351562      Current accuracy:  4.0      Comp_time of one batch:  -0.883836030960083\n",
            "Iteration:  21070      Current loss:  21.982481002807617      Current accuracy:  4.0      Comp_time of one batch:  -0.8928742408752441\n",
            "Iteration:  21075      Current loss:  23.740863800048828      Current accuracy:  4.0      Comp_time of one batch:  -0.8880505561828613\n",
            "Iteration:  21080      Current loss:  41.261741638183594      Current accuracy:  4.0      Comp_time of one batch:  -0.8913273811340332\n",
            "Iteration:  21085      Current loss:  20.836767196655273      Current accuracy:  5.0      Comp_time of one batch:  -0.9072198867797852\n",
            "Iteration:  21090      Current loss:  24.05279541015625      Current accuracy:  5.0      Comp_time of one batch:  -0.9116804599761963\n",
            "Iteration:  21095      Current loss:  39.75642395019531      Current accuracy:  5.0      Comp_time of one batch:  -0.8828961849212646\n",
            "Iteration:  21100      Current loss:  22.775203704833984      Current accuracy:  5.0      Comp_time of one batch:  -0.8885743618011475\n",
            "Iteration:  21105      Current loss:  33.83736801147461      Current accuracy:  5.0      Comp_time of one batch:  -0.8819797039031982\n",
            "Iteration:  21110      Current loss:  17.17195701599121      Current accuracy:  6.0      Comp_time of one batch:  -0.8980395793914795\n",
            "Iteration:  21115      Current loss:  25.047149658203125      Current accuracy:  6.0      Comp_time of one batch:  -0.890662670135498\n",
            "Iteration:  21120      Current loss:  21.410146713256836      Current accuracy:  7.0      Comp_time of one batch:  -0.8909692764282227\n",
            "Iteration:  21125      Current loss:  26.181411743164062      Current accuracy:  7.0      Comp_time of one batch:  -0.9050545692443848\n",
            "Iteration:  21130      Current loss:  18.46816062927246      Current accuracy:  8.0      Comp_time of one batch:  -0.8853662014007568\n",
            "Iteration:  21135      Current loss:  27.255887985229492      Current accuracy:  8.0      Comp_time of one batch:  -0.9027955532073975\n",
            "Iteration:  21140      Current loss:  19.458093643188477      Current accuracy:  9.0      Comp_time of one batch:  -0.8938446044921875\n",
            "Iteration:  21145      Current loss:  24.346904754638672      Current accuracy:  9.0      Comp_time of one batch:  -0.8808822631835938\n",
            "Iteration:  21150      Current loss:  19.883386611938477      Current accuracy:  10.0      Comp_time of one batch:  -0.8821907043457031\n",
            "Iteration:  21155      Current loss:  21.453510284423828      Current accuracy:  11.0      Comp_time of one batch:  -0.8909738063812256\n",
            "Iteration:  21160      Current loss:  32.37040328979492      Current accuracy:  11.0      Comp_time of one batch:  -0.8938608169555664\n",
            "Iteration:  21165      Current loss:  20.40792465209961      Current accuracy:  12.0      Comp_time of one batch:  -0.8913352489471436\n",
            "Iteration:  21170      Current loss:  20.703479766845703      Current accuracy:  13.0      Comp_time of one batch:  -0.9103109836578369\n",
            "Iteration:  21175      Current loss:  26.25909423828125      Current accuracy:  13.0      Comp_time of one batch:  -0.8847346305847168\n",
            "Iteration:  21180      Current loss:  19.736499786376953      Current accuracy:  14.0      Comp_time of one batch:  -0.889885425567627\n",
            "Iteration:  21185      Current loss:  26.514198303222656      Current accuracy:  15.0      Comp_time of one batch:  -0.9015047550201416\n",
            "Iteration:  21190      Current loss:  22.6563720703125      Current accuracy:  16.0      Comp_time of one batch:  -0.8814418315887451\n",
            "Iteration:  21195      Current loss:  25.997291564941406      Current accuracy:  16.0      Comp_time of one batch:  -0.8796119689941406\n",
            "Iteration:  21200      Current loss:  23.390033721923828      Current accuracy:  16.0      Comp_time of one batch:  -0.8734254837036133\n",
            "Iteration:  21205      Current loss:  22.989219665527344      Current accuracy:  17.0      Comp_time of one batch:  -0.8889122009277344\n",
            "Iteration:  21210      Current loss:  24.346101760864258      Current accuracy:  17.0      Comp_time of one batch:  -0.8854813575744629\n",
            "Iteration:  21215      Current loss:  20.387422561645508      Current accuracy:  18.0      Comp_time of one batch:  -0.8987851142883301\n",
            "Iteration:  21220      Current loss:  19.906721115112305      Current accuracy:  19.0      Comp_time of one batch:  -0.8968045711517334\n",
            "Iteration:  21225      Current loss:  19.291301727294922      Current accuracy:  20.0      Comp_time of one batch:  -0.883939266204834\n",
            "Iteration:  21230      Current loss:  25.133010864257812      Current accuracy:  20.0      Comp_time of one batch:  -0.9314608573913574\n",
            "Iteration:  21235      Current loss:  27.25380516052246      Current accuracy:  20.0      Comp_time of one batch:  -0.9047482013702393\n",
            "Iteration:  21240      Current loss:  18.9875431060791      Current accuracy:  21.0      Comp_time of one batch:  -0.9051589965820312\n",
            "Iteration:  21245      Current loss:  29.783037185668945      Current accuracy:  21.0      Comp_time of one batch:  -0.9260649681091309\n",
            "Iteration:  21250      Current loss:  36.782894134521484      Current accuracy:  21.0      Comp_time of one batch:  -0.8801965713500977\n",
            "Iteration:  21255      Current loss:  28.986160278320312      Current accuracy:  21.0      Comp_time of one batch:  -0.9160075187683105\n",
            "Iteration:  21260      Current loss:  26.07339096069336      Current accuracy:  21.0      Comp_time of one batch:  -0.8781075477600098\n",
            "Iteration:  21265      Current loss:  29.2724552154541      Current accuracy:  21.0      Comp_time of one batch:  -0.8845059871673584\n",
            "Iteration:  21270      Current loss:  23.343456268310547      Current accuracy:  22.0      Comp_time of one batch:  -0.9207906723022461\n",
            "Iteration:  21275      Current loss:  17.921789169311523      Current accuracy:  23.0      Comp_time of one batch:  -0.9153268337249756\n",
            "Iteration:  21280      Current loss:  34.58087158203125      Current accuracy:  23.0      Comp_time of one batch:  -0.8856477737426758\n",
            "Iteration:  21285      Current loss:  25.34327507019043      Current accuracy:  23.0      Comp_time of one batch:  -0.8861048221588135\n",
            "Iteration:  21290      Current loss:  26.77521514892578      Current accuracy:  23.0      Comp_time of one batch:  -0.8772130012512207\n",
            "Iteration:  21295      Current loss:  24.37837791442871      Current accuracy:  23.0      Comp_time of one batch:  -0.8784565925598145\n",
            "Iteration:  21300      Current loss:  33.476016998291016      Current accuracy:  23.0      Comp_time of one batch:  -0.8802490234375\n",
            "Iteration:  21305      Current loss:  36.023704528808594      Current accuracy:  23.0      Comp_time of one batch:  -0.8817319869995117\n",
            "Iteration:  21310      Current loss:  26.110239028930664      Current accuracy:  24.0      Comp_time of one batch:  -0.8797330856323242\n",
            "Iteration:  21315      Current loss:  25.23076629638672      Current accuracy:  24.0      Comp_time of one batch:  -0.8817837238311768\n",
            "Iteration:  21320      Current loss:  20.00035858154297      Current accuracy:  25.0      Comp_time of one batch:  -0.8826181888580322\n",
            "Iteration:  21325      Current loss:  20.614389419555664      Current accuracy:  26.0      Comp_time of one batch:  -0.8839008808135986\n",
            "Iteration:  21330      Current loss:  26.30892562866211      Current accuracy:  26.0      Comp_time of one batch:  -0.8820788860321045\n",
            "Iteration:  21335      Current loss:  21.954063415527344      Current accuracy:  27.0      Comp_time of one batch:  -0.8861911296844482\n",
            "Iteration:  21340      Current loss:  26.118541717529297      Current accuracy:  27.0      Comp_time of one batch:  -0.8843836784362793\n",
            "Iteration:  21345      Current loss:  31.446807861328125      Current accuracy:  27.0      Comp_time of one batch:  -0.8975968360900879\n",
            "Iteration:  21350      Current loss:  23.00986671447754      Current accuracy:  28.0      Comp_time of one batch:  -0.8847739696502686\n",
            "Iteration:  21355      Current loss:  22.931678771972656      Current accuracy:  28.0      Comp_time of one batch:  -0.8634729385375977\n",
            "Iteration:  21360      Current loss:  18.875938415527344      Current accuracy:  29.0      Comp_time of one batch:  -0.8868715763092041\n",
            "Iteration:  21365      Current loss:  37.55148696899414      Current accuracy:  29.0      Comp_time of one batch:  -0.9077508449554443\n",
            "Iteration:  21370      Current loss:  43.79457473754883      Current accuracy:  29.0      Comp_time of one batch:  -0.8897967338562012\n",
            "Iteration:  21375      Current loss:  21.132801055908203      Current accuracy:  30.0      Comp_time of one batch:  -0.9121394157409668\n",
            "Iteration:  21380      Current loss:  21.82821273803711      Current accuracy:  31.0      Comp_time of one batch:  -0.8935751914978027\n",
            "Iteration:  21385      Current loss:  25.479576110839844      Current accuracy:  32.0      Comp_time of one batch:  -0.8776190280914307\n",
            "Iteration:  21390      Current loss:  20.125450134277344      Current accuracy:  33.0      Comp_time of one batch:  -0.9320323467254639\n",
            "Iteration:  21395      Current loss:  17.238506317138672      Current accuracy:  34.0      Comp_time of one batch:  -0.8976142406463623\n",
            "Iteration:  21400      Current loss:  20.596837997436523      Current accuracy:  35.0      Comp_time of one batch:  -0.8884973526000977\n",
            "Iteration:  21405      Current loss:  26.67140769958496      Current accuracy:  35.0      Comp_time of one batch:  -0.879941463470459\n",
            "Iteration:  21410      Current loss:  27.143693923950195      Current accuracy:  35.0      Comp_time of one batch:  -0.8904678821563721\n",
            "Iteration:  21415      Current loss:  20.411787033081055      Current accuracy:  36.0      Comp_time of one batch:  -0.8982203006744385\n",
            "Iteration:  21420      Current loss:  31.073631286621094      Current accuracy:  36.0      Comp_time of one batch:  -0.8784029483795166\n",
            "Iteration:  21425      Current loss:  26.87130355834961      Current accuracy:  36.0      Comp_time of one batch:  -0.9189867973327637\n",
            "Iteration:  21430      Current loss:  23.30245590209961      Current accuracy:  37.0      Comp_time of one batch:  -0.9027066230773926\n",
            "Iteration:  21435      Current loss:  28.350189208984375      Current accuracy:  37.0      Comp_time of one batch:  -0.8950161933898926\n",
            "Iteration:  21440      Current loss:  24.220272064208984      Current accuracy:  37.0      Comp_time of one batch:  -0.8759119510650635\n",
            "Iteration:  21445      Current loss:  22.908525466918945      Current accuracy:  37.0      Comp_time of one batch:  -0.9022760391235352\n",
            "Iteration:  21450      Current loss:  30.19841957092285      Current accuracy:  37.0      Comp_time of one batch:  -0.8986554145812988\n",
            "Iteration:  21455      Current loss:  31.74408531188965      Current accuracy:  37.0      Comp_time of one batch:  -0.9083085060119629\n",
            "Iteration:  21460      Current loss:  19.563812255859375      Current accuracy:  38.0      Comp_time of one batch:  -0.8874924182891846\n",
            "Iteration:  21465      Current loss:  28.582515716552734      Current accuracy:  38.0      Comp_time of one batch:  -0.8991851806640625\n",
            "Iteration:  21470      Current loss:  24.542278289794922      Current accuracy:  38.0      Comp_time of one batch:  -0.9178602695465088\n",
            "Iteration:  21475      Current loss:  14.998697280883789      Current accuracy:  39.0      Comp_time of one batch:  -0.8765847682952881\n",
            "Iteration:  21480      Current loss:  24.40569305419922      Current accuracy:  39.0      Comp_time of one batch:  -0.8885335922241211\n",
            "Iteration:  21485      Current loss:  22.40951919555664      Current accuracy:  40.0      Comp_time of one batch:  -0.8760743141174316\n",
            "Iteration:  21490      Current loss:  21.026813507080078      Current accuracy:  41.0      Comp_time of one batch:  -0.8951647281646729\n",
            "Iteration:  21495      Current loss:  27.808696746826172      Current accuracy:  41.0      Comp_time of one batch:  -0.8736038208007812\n",
            "Iteration:  21500      Current loss:  22.14410400390625      Current accuracy:  0.0      Comp_time of one batch:  -0.8869118690490723\n",
            "Iteration:  21505      Current loss:  20.10091209411621      Current accuracy:  1.0      Comp_time of one batch:  -0.8815507888793945\n",
            "Iteration:  21510      Current loss:  20.913925170898438      Current accuracy:  2.0      Comp_time of one batch:  -0.8956143856048584\n",
            "Iteration:  21515      Current loss:  29.252084732055664      Current accuracy:  2.0      Comp_time of one batch:  -0.8898439407348633\n",
            "Iteration:  21520      Current loss:  26.651561737060547      Current accuracy:  2.0      Comp_time of one batch:  -0.9108030796051025\n",
            "Iteration:  21525      Current loss:  27.50310516357422      Current accuracy:  2.0      Comp_time of one batch:  -0.8925330638885498\n",
            "Iteration:  21530      Current loss:  18.77903938293457      Current accuracy:  3.0      Comp_time of one batch:  -0.8761253356933594\n",
            "Iteration:  21535      Current loss:  25.0511417388916      Current accuracy:  3.0      Comp_time of one batch:  -0.8999404907226562\n",
            "Iteration:  21540      Current loss:  31.041095733642578      Current accuracy:  3.0      Comp_time of one batch:  -0.8849778175354004\n",
            "Iteration:  21545      Current loss:  25.811857223510742      Current accuracy:  3.0      Comp_time of one batch:  -0.8926711082458496\n",
            "Iteration:  21550      Current loss:  28.53762435913086      Current accuracy:  3.0      Comp_time of one batch:  -0.9008946418762207\n",
            "Iteration:  21555      Current loss:  24.745319366455078      Current accuracy:  3.0      Comp_time of one batch:  -0.8911082744598389\n",
            "Iteration:  21560      Current loss:  25.83665657043457      Current accuracy:  3.0      Comp_time of one batch:  -0.9038326740264893\n",
            "Iteration:  21565      Current loss:  23.43996238708496      Current accuracy:  4.0      Comp_time of one batch:  -0.9248340129852295\n",
            "Iteration:  21570      Current loss:  25.98715591430664      Current accuracy:  4.0      Comp_time of one batch:  -0.8946859836578369\n",
            "Iteration:  21575      Current loss:  17.769357681274414      Current accuracy:  5.0      Comp_time of one batch:  -0.9061553478240967\n",
            "Iteration:  21580      Current loss:  25.002864837646484      Current accuracy:  6.0      Comp_time of one batch:  -0.90267014503479\n",
            "Iteration:  21585      Current loss:  30.408918380737305      Current accuracy:  6.0      Comp_time of one batch:  -0.8910908699035645\n",
            "Iteration:  21590      Current loss:  24.749353408813477      Current accuracy:  6.0      Comp_time of one batch:  -0.8718409538269043\n",
            "Iteration:  21595      Current loss:  20.847190856933594      Current accuracy:  7.0      Comp_time of one batch:  -0.8872380256652832\n",
            "Iteration:  21600      Current loss:  22.723297119140625      Current accuracy:  8.0      Comp_time of one batch:  -0.9088366031646729\n",
            "Iteration:  21605      Current loss:  37.95024490356445      Current accuracy:  8.0      Comp_time of one batch:  -0.9075663089752197\n",
            "Iteration:  21610      Current loss:  24.28387451171875      Current accuracy:  8.0      Comp_time of one batch:  -0.8973448276519775\n",
            "Iteration:  21615      Current loss:  28.23624610900879      Current accuracy:  8.0      Comp_time of one batch:  -0.8818237781524658\n",
            "Iteration:  21620      Current loss:  25.71558952331543      Current accuracy:  8.0      Comp_time of one batch:  -0.9034624099731445\n",
            "Iteration:  21625      Current loss:  24.67340850830078      Current accuracy:  8.0      Comp_time of one batch:  -0.8894741535186768\n",
            "Iteration:  21630      Current loss:  20.275056838989258      Current accuracy:  9.0      Comp_time of one batch:  -0.8931059837341309\n",
            "Iteration:  21635      Current loss:  25.399538040161133      Current accuracy:  9.0      Comp_time of one batch:  -0.8984353542327881\n",
            "Iteration:  21640      Current loss:  20.389734268188477      Current accuracy:  10.0      Comp_time of one batch:  -0.8932785987854004\n",
            "Iteration:  21645      Current loss:  20.12631607055664      Current accuracy:  11.0      Comp_time of one batch:  -0.8855304718017578\n",
            "Iteration:  21650      Current loss:  26.489465713500977      Current accuracy:  11.0      Comp_time of one batch:  -0.8939383029937744\n",
            "Iteration:  21655      Current loss:  33.24565124511719      Current accuracy:  11.0      Comp_time of one batch:  -0.8892166614532471\n",
            "Iteration:  21660      Current loss:  23.278301239013672      Current accuracy:  12.0      Comp_time of one batch:  -0.8752999305725098\n",
            "Iteration:  21665      Current loss:  20.11519432067871      Current accuracy:  13.0      Comp_time of one batch:  -0.8947896957397461\n",
            "Iteration:  21670      Current loss:  24.323808670043945      Current accuracy:  13.0      Comp_time of one batch:  -0.8892772197723389\n",
            "Iteration:  21675      Current loss:  20.92230224609375      Current accuracy:  14.0      Comp_time of one batch:  -0.9125983715057373\n",
            "Iteration:  21680      Current loss:  21.982126235961914      Current accuracy:  15.0      Comp_time of one batch:  -0.9182403087615967\n",
            "Iteration:  21685      Current loss:  30.676742553710938      Current accuracy:  15.0      Comp_time of one batch:  -0.8925571441650391\n",
            "Iteration:  21690      Current loss:  21.145986557006836      Current accuracy:  16.0      Comp_time of one batch:  -0.8893020153045654\n",
            "Iteration:  21695      Current loss:  29.36664581298828      Current accuracy:  16.0      Comp_time of one batch:  -0.8871738910675049\n",
            "Iteration:  21700      Current loss:  30.63612174987793      Current accuracy:  16.0      Comp_time of one batch:  -0.8723134994506836\n",
            "Iteration:  21705      Current loss:  19.96115493774414      Current accuracy:  17.0      Comp_time of one batch:  -0.8933188915252686\n",
            "Iteration:  21710      Current loss:  23.58710479736328      Current accuracy:  17.0      Comp_time of one batch:  -0.8913743495941162\n",
            "Iteration:  21715      Current loss:  24.346174240112305      Current accuracy:  17.0      Comp_time of one batch:  -0.8829474449157715\n",
            "Iteration:  21720      Current loss:  26.714550018310547      Current accuracy:  17.0      Comp_time of one batch:  -0.9236257076263428\n",
            "Iteration:  21725      Current loss:  24.253808975219727      Current accuracy:  17.0      Comp_time of one batch:  -0.8863821029663086\n",
            "Iteration:  21730      Current loss:  18.438125610351562      Current accuracy:  18.0      Comp_time of one batch:  -0.8845860958099365\n",
            "Iteration:  21735      Current loss:  26.07918930053711      Current accuracy:  18.0      Comp_time of one batch:  -0.8956916332244873\n",
            "Iteration:  21740      Current loss:  22.26072120666504      Current accuracy:  19.0      Comp_time of one batch:  -0.8808903694152832\n",
            "Iteration:  21745      Current loss:  20.92742156982422      Current accuracy:  20.0      Comp_time of one batch:  -0.8914918899536133\n",
            "Iteration:  21750      Current loss:  23.420486450195312      Current accuracy:  21.0      Comp_time of one batch:  -0.8835785388946533\n",
            "Iteration:  21755      Current loss:  22.504114151000977      Current accuracy:  22.0      Comp_time of one batch:  -0.8795955181121826\n",
            "Iteration:  21760      Current loss:  18.623828887939453      Current accuracy:  23.0      Comp_time of one batch:  -0.9033050537109375\n",
            "Iteration:  21765      Current loss:  34.408084869384766      Current accuracy:  23.0      Comp_time of one batch:  -0.908416748046875\n",
            "Iteration:  21770      Current loss:  29.458011627197266      Current accuracy:  23.0      Comp_time of one batch:  -0.8785214424133301\n",
            "Iteration:  21775      Current loss:  26.342084884643555      Current accuracy:  23.0      Comp_time of one batch:  -0.8871068954467773\n",
            "Iteration:  21780      Current loss:  22.278255462646484      Current accuracy:  23.0      Comp_time of one batch:  -0.9055240154266357\n",
            "Iteration:  21785      Current loss:  24.67377281188965      Current accuracy:  23.0      Comp_time of one batch:  -0.9005098342895508\n",
            "Iteration:  21790      Current loss:  21.443023681640625      Current accuracy:  24.0      Comp_time of one batch:  -0.8824832439422607\n",
            "Iteration:  21795      Current loss:  26.597030639648438      Current accuracy:  24.0      Comp_time of one batch:  -0.902346134185791\n",
            "Iteration:  21800      Current loss:  25.10546875      Current accuracy:  24.0      Comp_time of one batch:  -0.8914697170257568\n",
            "Iteration:  21805      Current loss:  29.131912231445312      Current accuracy:  24.0      Comp_time of one batch:  -0.9040594100952148\n",
            "Iteration:  21810      Current loss:  22.24241065979004      Current accuracy:  25.0      Comp_time of one batch:  -0.876211404800415\n",
            "Iteration:  21815      Current loss:  18.468584060668945      Current accuracy:  26.0      Comp_time of one batch:  -0.9064619541168213\n",
            "Iteration:  21820      Current loss:  25.852922439575195      Current accuracy:  26.0      Comp_time of one batch:  -0.9159607887268066\n",
            "Iteration:  21825      Current loss:  25.058116912841797      Current accuracy:  26.0      Comp_time of one batch:  -0.9152474403381348\n",
            "Iteration:  21830      Current loss:  19.555334091186523      Current accuracy:  27.0      Comp_time of one batch:  -0.9055178165435791\n",
            "Iteration:  21835      Current loss:  29.5324764251709      Current accuracy:  27.0      Comp_time of one batch:  -0.881035566329956\n",
            "Iteration:  21840      Current loss:  25.25769805908203      Current accuracy:  27.0      Comp_time of one batch:  -0.9076721668243408\n",
            "Iteration:  21845      Current loss:  32.53679275512695      Current accuracy:  27.0      Comp_time of one batch:  -0.8808326721191406\n",
            "Iteration:  21850      Current loss:  25.160621643066406      Current accuracy:  28.0      Comp_time of one batch:  -0.8982887268066406\n",
            "Iteration:  21855      Current loss:  30.84766387939453      Current accuracy:  28.0      Comp_time of one batch:  -0.889129638671875\n",
            "Iteration:  21860      Current loss:  22.294557571411133      Current accuracy:  29.0      Comp_time of one batch:  -0.8888332843780518\n",
            "Iteration:  21865      Current loss:  23.574003219604492      Current accuracy:  29.0      Comp_time of one batch:  -0.9065206050872803\n",
            "Iteration:  21870      Current loss:  20.184263229370117      Current accuracy:  30.0      Comp_time of one batch:  -0.8787670135498047\n",
            "Iteration:  21875      Current loss:  25.95335578918457      Current accuracy:  30.0      Comp_time of one batch:  -0.8935132026672363\n",
            "Iteration:  21880      Current loss:  26.4079647064209      Current accuracy:  30.0      Comp_time of one batch:  -0.8796787261962891\n",
            "Iteration:  21885      Current loss:  22.18882179260254      Current accuracy:  31.0      Comp_time of one batch:  -0.8778548240661621\n",
            "Iteration:  21890      Current loss:  21.21861457824707      Current accuracy:  32.0      Comp_time of one batch:  -0.8984811305999756\n",
            "Iteration:  21895      Current loss:  27.93378257751465      Current accuracy:  32.0      Comp_time of one batch:  -0.880744457244873\n",
            "Iteration:  21900      Current loss:  18.710952758789062      Current accuracy:  33.0      Comp_time of one batch:  -0.9014816284179688\n",
            "Iteration:  21905      Current loss:  22.011159896850586      Current accuracy:  34.0      Comp_time of one batch:  -0.8923823833465576\n",
            "Iteration:  21910      Current loss:  27.590795516967773      Current accuracy:  34.0      Comp_time of one batch:  -0.8855516910552979\n",
            "Iteration:  21915      Current loss:  21.501008987426758      Current accuracy:  35.0      Comp_time of one batch:  -0.9034717082977295\n",
            "Iteration:  21920      Current loss:  19.706790924072266      Current accuracy:  36.0      Comp_time of one batch:  -0.8983845710754395\n",
            "Iteration:  21925      Current loss:  20.924999237060547      Current accuracy:  37.0      Comp_time of one batch:  -0.8710675239562988\n",
            "Iteration:  21930      Current loss:  26.422636032104492      Current accuracy:  37.0      Comp_time of one batch:  -0.8792414665222168\n",
            "Iteration:  21935      Current loss:  19.318729400634766      Current accuracy:  38.0      Comp_time of one batch:  -0.9089827537536621\n",
            "Iteration:  21940      Current loss:  22.059213638305664      Current accuracy:  39.0      Comp_time of one batch:  -0.884955644607544\n",
            "Iteration:  21945      Current loss:  22.53150177001953      Current accuracy:  40.0      Comp_time of one batch:  -0.8905117511749268\n",
            "Iteration:  21950      Current loss:  22.10103988647461      Current accuracy:  41.0      Comp_time of one batch:  -0.902381181716919\n",
            "Iteration:  21955      Current loss:  22.258018493652344      Current accuracy:  42.0      Comp_time of one batch:  -0.88995361328125\n",
            "Iteration:  21960      Current loss:  23.16107749938965      Current accuracy:  43.0      Comp_time of one batch:  -0.8830444812774658\n",
            "Iteration:  21965      Current loss:  17.547046661376953      Current accuracy:  44.0      Comp_time of one batch:  -0.9042859077453613\n",
            "Iteration:  21970      Current loss:  21.203414916992188      Current accuracy:  45.0      Comp_time of one batch:  -0.8784387111663818\n",
            "Iteration:  21975      Current loss:  26.81830406188965      Current accuracy:  45.0      Comp_time of one batch:  -0.8794982433319092\n",
            "Iteration:  21980      Current loss:  27.882366180419922      Current accuracy:  45.0      Comp_time of one batch:  -0.9080197811126709\n",
            "Iteration:  21985      Current loss:  26.15983772277832      Current accuracy:  46.0      Comp_time of one batch:  -0.9076070785522461\n",
            "Iteration:  21990      Current loss:  22.157663345336914      Current accuracy:  47.0      Comp_time of one batch:  -0.898956298828125\n",
            "Iteration:  21995      Current loss:  29.06633758544922      Current accuracy:  47.0      Comp_time of one batch:  -0.9045751094818115\n",
            "Iteration:  22000      Current loss:  18.01201820373535      Current accuracy:  0.0      Comp_time of one batch:  -0.8843650817871094\n",
            "Iteration:  22005      Current loss:  25.33653450012207      Current accuracy:  0.0      Comp_time of one batch:  -0.8821384906768799\n",
            "Iteration:  22010      Current loss:  31.988439559936523      Current accuracy:  0.0      Comp_time of one batch:  -0.8913896083831787\n",
            "Iteration:  22015      Current loss:  24.8554630279541      Current accuracy:  0.0      Comp_time of one batch:  -0.8783202171325684\n",
            "Iteration:  22020      Current loss:  20.54621124267578      Current accuracy:  1.0      Comp_time of one batch:  -0.9030060768127441\n",
            "Iteration:  22025      Current loss:  31.240585327148438      Current accuracy:  1.0      Comp_time of one batch:  -0.8933041095733643\n",
            "Iteration:  22030      Current loss:  17.47067642211914      Current accuracy:  2.0      Comp_time of one batch:  -0.8862946033477783\n",
            "Iteration:  22035      Current loss:  23.045608520507812      Current accuracy:  2.0      Comp_time of one batch:  -0.9033620357513428\n",
            "Iteration:  22040      Current loss:  29.39655876159668      Current accuracy:  2.0      Comp_time of one batch:  -0.8908708095550537\n",
            "Iteration:  22045      Current loss:  33.073726654052734      Current accuracy:  2.0      Comp_time of one batch:  -0.9028713703155518\n",
            "Iteration:  22050      Current loss:  27.859676361083984      Current accuracy:  2.0      Comp_time of one batch:  -0.907602071762085\n",
            "Iteration:  22055      Current loss:  21.363143920898438      Current accuracy:  3.0      Comp_time of one batch:  -0.894650936126709\n",
            "Iteration:  22060      Current loss:  22.8406925201416      Current accuracy:  4.0      Comp_time of one batch:  -0.8926539421081543\n",
            "Iteration:  22065      Current loss:  19.792194366455078      Current accuracy:  5.0      Comp_time of one batch:  -0.896237850189209\n",
            "Iteration:  22070      Current loss:  26.65610694885254      Current accuracy:  5.0      Comp_time of one batch:  -0.8752467632293701\n",
            "Iteration:  22075      Current loss:  24.37769317626953      Current accuracy:  5.0      Comp_time of one batch:  -0.8905112743377686\n",
            "Iteration:  22080      Current loss:  22.186546325683594      Current accuracy:  6.0      Comp_time of one batch:  -0.8911292552947998\n",
            "Iteration:  22085      Current loss:  26.35590362548828      Current accuracy:  6.0      Comp_time of one batch:  -0.9051780700683594\n",
            "Iteration:  22090      Current loss:  22.541841506958008      Current accuracy:  7.0      Comp_time of one batch:  -0.897026777267456\n",
            "Iteration:  22095      Current loss:  21.7854061126709      Current accuracy:  8.0      Comp_time of one batch:  -0.9068996906280518\n",
            "Iteration:  22100      Current loss:  21.76409149169922      Current accuracy:  9.0      Comp_time of one batch:  -0.9013915061950684\n",
            "Iteration:  22105      Current loss:  26.52594757080078      Current accuracy:  9.0      Comp_time of one batch:  -0.8771731853485107\n",
            "Iteration:  22110      Current loss:  17.613567352294922      Current accuracy:  10.0      Comp_time of one batch:  -0.8846619129180908\n",
            "Iteration:  22115      Current loss:  32.50224304199219      Current accuracy:  10.0      Comp_time of one batch:  -0.8818533420562744\n",
            "Iteration:  22120      Current loss:  22.23998260498047      Current accuracy:  11.0      Comp_time of one batch:  -0.911240816116333\n",
            "Iteration:  22125      Current loss:  19.58026695251465      Current accuracy:  12.0      Comp_time of one batch:  -0.8925158977508545\n",
            "Iteration:  22130      Current loss:  37.073246002197266      Current accuracy:  12.0      Comp_time of one batch:  -0.8989911079406738\n",
            "Iteration:  22135      Current loss:  20.886001586914062      Current accuracy:  13.0      Comp_time of one batch:  -0.8871831893920898\n",
            "Iteration:  22140      Current loss:  27.369243621826172      Current accuracy:  13.0      Comp_time of one batch:  -0.8905868530273438\n",
            "Iteration:  22145      Current loss:  25.914016723632812      Current accuracy:  14.0      Comp_time of one batch:  -0.8890619277954102\n",
            "Iteration:  22150      Current loss:  27.11203956604004      Current accuracy:  14.0      Comp_time of one batch:  -0.8800454139709473\n",
            "Iteration:  22155      Current loss:  19.978057861328125      Current accuracy:  15.0      Comp_time of one batch:  -0.922877311706543\n",
            "Iteration:  22160      Current loss:  26.431753158569336      Current accuracy:  15.0      Comp_time of one batch:  -0.8814830780029297\n",
            "Iteration:  22165      Current loss:  26.184965133666992      Current accuracy:  15.0      Comp_time of one batch:  -0.9004726409912109\n",
            "Iteration:  22170      Current loss:  30.421781539916992      Current accuracy:  15.0      Comp_time of one batch:  -0.8973073959350586\n",
            "Iteration:  22175      Current loss:  22.092798233032227      Current accuracy:  16.0      Comp_time of one batch:  -0.8946208953857422\n",
            "Iteration:  22180      Current loss:  24.743732452392578      Current accuracy:  16.0      Comp_time of one batch:  -0.8927929401397705\n",
            "Iteration:  22185      Current loss:  20.54899787902832      Current accuracy:  17.0      Comp_time of one batch:  -0.891939640045166\n",
            "Iteration:  22190      Current loss:  20.375160217285156      Current accuracy:  18.0      Comp_time of one batch:  -0.883753776550293\n",
            "Iteration:  22195      Current loss:  17.151838302612305      Current accuracy:  19.0      Comp_time of one batch:  -0.8787062168121338\n",
            "Iteration:  22200      Current loss:  24.429241180419922      Current accuracy:  20.0      Comp_time of one batch:  -0.8914532661437988\n",
            "Iteration:  22205      Current loss:  26.102703094482422      Current accuracy:  20.0      Comp_time of one batch:  -0.8947873115539551\n",
            "Iteration:  22210      Current loss:  18.70684051513672      Current accuracy:  21.0      Comp_time of one batch:  -0.9064750671386719\n",
            "Iteration:  22215      Current loss:  29.686508178710938      Current accuracy:  21.0      Comp_time of one batch:  -0.9167635440826416\n",
            "Iteration:  22220      Current loss:  19.932958602905273      Current accuracy:  22.0      Comp_time of one batch:  -0.8810291290283203\n",
            "Iteration:  22225      Current loss:  30.734113693237305      Current accuracy:  22.0      Comp_time of one batch:  -0.8761076927185059\n",
            "Iteration:  22230      Current loss:  19.851415634155273      Current accuracy:  23.0      Comp_time of one batch:  -0.8921315670013428\n",
            "Iteration:  22235      Current loss:  16.548980712890625      Current accuracy:  24.0      Comp_time of one batch:  -0.897437572479248\n",
            "Iteration:  22240      Current loss:  25.172882080078125      Current accuracy:  24.0      Comp_time of one batch:  -0.8740196228027344\n",
            "Iteration:  22245      Current loss:  29.326221466064453      Current accuracy:  24.0      Comp_time of one batch:  -0.8906331062316895\n",
            "Iteration:  22250      Current loss:  22.296140670776367      Current accuracy:  25.0      Comp_time of one batch:  -0.8913803100585938\n",
            "Iteration:  22255      Current loss:  27.37779426574707      Current accuracy:  25.0      Comp_time of one batch:  -0.898817777633667\n",
            "Iteration:  22260      Current loss:  20.590045928955078      Current accuracy:  26.0      Comp_time of one batch:  -0.8868374824523926\n",
            "Iteration:  22265      Current loss:  20.399877548217773      Current accuracy:  27.0      Comp_time of one batch:  -0.8875207901000977\n",
            "Iteration:  22270      Current loss:  25.78346824645996      Current accuracy:  27.0      Comp_time of one batch:  -0.8877773284912109\n",
            "Iteration:  22275      Current loss:  16.876523971557617      Current accuracy:  28.0      Comp_time of one batch:  -0.8810784816741943\n",
            "Iteration:  22280      Current loss:  21.524354934692383      Current accuracy:  29.0      Comp_time of one batch:  -0.9052627086639404\n",
            "Iteration:  22285      Current loss:  27.796607971191406      Current accuracy:  29.0      Comp_time of one batch:  -0.8720753192901611\n",
            "Iteration:  22290      Current loss:  24.31431770324707      Current accuracy:  29.0      Comp_time of one batch:  -0.9076395034790039\n",
            "Iteration:  22295      Current loss:  25.473880767822266      Current accuracy:  29.0      Comp_time of one batch:  -0.8874902725219727\n",
            "Iteration:  22300      Current loss:  17.925657272338867      Current accuracy:  30.0      Comp_time of one batch:  -0.8927569389343262\n",
            "Iteration:  22305      Current loss:  24.41731071472168      Current accuracy:  30.0      Comp_time of one batch:  -0.9018149375915527\n",
            "Iteration:  22310      Current loss:  23.291797637939453      Current accuracy:  30.0      Comp_time of one batch:  -0.9017899036407471\n",
            "Iteration:  22315      Current loss:  28.98170280456543      Current accuracy:  30.0      Comp_time of one batch:  -0.884868860244751\n",
            "Iteration:  22320      Current loss:  21.831743240356445      Current accuracy:  31.0      Comp_time of one batch:  -0.9062614440917969\n",
            "Iteration:  22325      Current loss:  26.72049331665039      Current accuracy:  31.0      Comp_time of one batch:  -0.889209508895874\n",
            "Iteration:  22330      Current loss:  35.35195541381836      Current accuracy:  31.0      Comp_time of one batch:  -0.874610424041748\n",
            "Iteration:  22335      Current loss:  24.60650062561035      Current accuracy:  31.0      Comp_time of one batch:  -0.8985097408294678\n",
            "Iteration:  22340      Current loss:  26.81134605407715      Current accuracy:  32.0      Comp_time of one batch:  -0.8842892646789551\n",
            "Iteration:  22345      Current loss:  27.960559844970703      Current accuracy:  32.0      Comp_time of one batch:  -0.9052009582519531\n",
            "Iteration:  22350      Current loss:  20.949419021606445      Current accuracy:  33.0      Comp_time of one batch:  -0.907567024230957\n",
            "Iteration:  22355      Current loss:  26.84796714782715      Current accuracy:  33.0      Comp_time of one batch:  -0.8992974758148193\n",
            "Iteration:  22360      Current loss:  24.98539161682129      Current accuracy:  33.0      Comp_time of one batch:  -0.885469913482666\n",
            "Iteration:  22365      Current loss:  20.57108497619629      Current accuracy:  34.0      Comp_time of one batch:  -0.9034924507141113\n",
            "Iteration:  22370      Current loss:  24.22483253479004      Current accuracy:  35.0      Comp_time of one batch:  -0.8861720561981201\n",
            "Iteration:  22375      Current loss:  27.014698028564453      Current accuracy:  35.0      Comp_time of one batch:  -0.8811125755310059\n",
            "Iteration:  22380      Current loss:  25.543725967407227      Current accuracy:  35.0      Comp_time of one batch:  -0.8938367366790771\n",
            "Iteration:  22385      Current loss:  18.275432586669922      Current accuracy:  36.0      Comp_time of one batch:  -0.8857467174530029\n",
            "Iteration:  22390      Current loss:  19.021928787231445      Current accuracy:  37.0      Comp_time of one batch:  -0.9007036685943604\n",
            "Iteration:  22395      Current loss:  23.836469650268555      Current accuracy:  37.0      Comp_time of one batch:  -0.8846943378448486\n",
            "Iteration:  22400      Current loss:  25.89278221130371      Current accuracy:  37.0      Comp_time of one batch:  -0.9168152809143066\n",
            "Iteration:  22405      Current loss:  19.481218338012695      Current accuracy:  38.0      Comp_time of one batch:  -0.8953092098236084\n",
            "Iteration:  22410      Current loss:  21.30155372619629      Current accuracy:  39.0      Comp_time of one batch:  -0.9120297431945801\n",
            "Iteration:  22415      Current loss:  21.967029571533203      Current accuracy:  40.0      Comp_time of one batch:  -0.905893087387085\n",
            "Iteration:  22420      Current loss:  24.981199264526367      Current accuracy:  40.0      Comp_time of one batch:  -0.8910984992980957\n",
            "Iteration:  22425      Current loss:  26.928197860717773      Current accuracy:  40.0      Comp_time of one batch:  -0.8810687065124512\n",
            "Iteration:  22430      Current loss:  25.116687774658203      Current accuracy:  40.0      Comp_time of one batch:  -0.9089713096618652\n",
            "Iteration:  22435      Current loss:  28.21063995361328      Current accuracy:  40.0      Comp_time of one batch:  -0.8922054767608643\n",
            "Iteration:  22440      Current loss:  35.179046630859375      Current accuracy:  40.0      Comp_time of one batch:  -0.8936154842376709\n",
            "Iteration:  22445      Current loss:  24.805038452148438      Current accuracy:  41.0      Comp_time of one batch:  -0.9020841121673584\n",
            "Iteration:  22450      Current loss:  19.11664390563965      Current accuracy:  42.0      Comp_time of one batch:  -0.8893346786499023\n",
            "Iteration:  22455      Current loss:  22.890209197998047      Current accuracy:  42.0      Comp_time of one batch:  -0.8820366859436035\n",
            "Iteration:  22460      Current loss:  35.47842788696289      Current accuracy:  42.0      Comp_time of one batch:  -0.9220678806304932\n",
            "Iteration:  22465      Current loss:  25.02436637878418      Current accuracy:  42.0      Comp_time of one batch:  -0.8958632946014404\n",
            "Iteration:  22470      Current loss:  23.592531204223633      Current accuracy:  42.0      Comp_time of one batch:  -0.8870117664337158\n",
            "Iteration:  22475      Current loss:  25.290447235107422      Current accuracy:  42.0      Comp_time of one batch:  -0.917656660079956\n",
            "Iteration:  22480      Current loss:  30.92032241821289      Current accuracy:  42.0      Comp_time of one batch:  -0.8835465908050537\n",
            "Iteration:  22485      Current loss:  21.30503273010254      Current accuracy:  43.0      Comp_time of one batch:  -0.8836565017700195\n",
            "Iteration:  22490      Current loss:  17.174030303955078      Current accuracy:  44.0      Comp_time of one batch:  -0.9130511283874512\n",
            "Iteration:  22495      Current loss:  26.40167999267578      Current accuracy:  44.0      Comp_time of one batch:  -0.8859553337097168\n",
            "Iteration:  22500      Current loss:  21.1290340423584      Current accuracy:  0.0      Comp_time of one batch:  -0.8822710514068604\n",
            "Iteration:  22505      Current loss:  20.13730812072754      Current accuracy:  1.0      Comp_time of one batch:  -0.894026517868042\n",
            "Iteration:  22510      Current loss:  24.524642944335938      Current accuracy:  2.0      Comp_time of one batch:  -0.8964014053344727\n",
            "Iteration:  22515      Current loss:  23.07305335998535      Current accuracy:  3.0      Comp_time of one batch:  -0.8893027305603027\n",
            "Iteration:  22520      Current loss:  21.532352447509766      Current accuracy:  4.0      Comp_time of one batch:  -0.8903508186340332\n",
            "Iteration:  22525      Current loss:  28.255517959594727      Current accuracy:  4.0      Comp_time of one batch:  -0.9009783267974854\n",
            "Iteration:  22530      Current loss:  27.191730499267578      Current accuracy:  4.0      Comp_time of one batch:  -0.8874590396881104\n",
            "Iteration:  22535      Current loss:  22.85015106201172      Current accuracy:  4.0      Comp_time of one batch:  -0.8843624591827393\n",
            "Iteration:  22540      Current loss:  22.389545440673828      Current accuracy:  5.0      Comp_time of one batch:  -0.90144944190979\n",
            "Iteration:  22545      Current loss:  33.52023696899414      Current accuracy:  5.0      Comp_time of one batch:  -0.8979349136352539\n",
            "Iteration:  22550      Current loss:  33.537376403808594      Current accuracy:  5.0      Comp_time of one batch:  -0.8923132419586182\n",
            "Iteration:  22555      Current loss:  24.983795166015625      Current accuracy:  5.0      Comp_time of one batch:  -0.8851416110992432\n",
            "Iteration:  22560      Current loss:  23.12708282470703      Current accuracy:  6.0      Comp_time of one batch:  -0.8922443389892578\n",
            "Iteration:  22565      Current loss:  26.927770614624023      Current accuracy:  6.0      Comp_time of one batch:  -0.8825595378875732\n",
            "Iteration:  22570      Current loss:  27.345685958862305      Current accuracy:  6.0      Comp_time of one batch:  -0.8821189403533936\n",
            "Iteration:  22575      Current loss:  25.768009185791016      Current accuracy:  6.0      Comp_time of one batch:  -0.9044620990753174\n",
            "Iteration:  22580      Current loss:  19.41350746154785      Current accuracy:  7.0      Comp_time of one batch:  -0.8931338787078857\n",
            "Iteration:  22585      Current loss:  26.641328811645508      Current accuracy:  7.0      Comp_time of one batch:  -0.8853089809417725\n",
            "Iteration:  22590      Current loss:  26.971515655517578      Current accuracy:  7.0      Comp_time of one batch:  -0.8818724155426025\n",
            "Iteration:  22595      Current loss:  30.635059356689453      Current accuracy:  7.0      Comp_time of one batch:  -0.8811125755310059\n",
            "Iteration:  22600      Current loss:  26.052396774291992      Current accuracy:  7.0      Comp_time of one batch:  -0.9013657569885254\n",
            "Iteration:  22605      Current loss:  24.274961471557617      Current accuracy:  7.0      Comp_time of one batch:  -0.8808784484863281\n",
            "Iteration:  22610      Current loss:  24.799177169799805      Current accuracy:  7.0      Comp_time of one batch:  -0.9117259979248047\n",
            "Iteration:  22615      Current loss:  27.91751480102539      Current accuracy:  7.0      Comp_time of one batch:  -0.8987352848052979\n",
            "Iteration:  22620      Current loss:  31.030948638916016      Current accuracy:  7.0      Comp_time of one batch:  -0.9366300106048584\n",
            "Iteration:  22625      Current loss:  18.04922866821289      Current accuracy:  8.0      Comp_time of one batch:  -0.8941552639007568\n",
            "Iteration:  22630      Current loss:  23.259138107299805      Current accuracy:  9.0      Comp_time of one batch:  -0.8806607723236084\n",
            "Iteration:  22635      Current loss:  28.27755355834961      Current accuracy:  9.0      Comp_time of one batch:  -0.9034345149993896\n",
            "Iteration:  22640      Current loss:  23.73704719543457      Current accuracy:  10.0      Comp_time of one batch:  -0.8853988647460938\n",
            "Iteration:  22645      Current loss:  27.660356521606445      Current accuracy:  10.0      Comp_time of one batch:  -0.8881194591522217\n",
            "Iteration:  22650      Current loss:  39.267921447753906      Current accuracy:  10.0      Comp_time of one batch:  -0.8889076709747314\n",
            "Iteration:  22655      Current loss:  22.97504234313965      Current accuracy:  10.0      Comp_time of one batch:  -0.8833134174346924\n",
            "Iteration:  22660      Current loss:  19.626876831054688      Current accuracy:  11.0      Comp_time of one batch:  -0.8923225402832031\n",
            "Iteration:  22665      Current loss:  16.75351905822754      Current accuracy:  12.0      Comp_time of one batch:  -0.8805365562438965\n",
            "Iteration:  22670      Current loss:  19.18000030517578      Current accuracy:  13.0      Comp_time of one batch:  -0.8786673545837402\n",
            "Iteration:  22675      Current loss:  32.58805465698242      Current accuracy:  13.0      Comp_time of one batch:  -0.8832104206085205\n",
            "Iteration:  22680      Current loss:  19.55596923828125      Current accuracy:  14.0      Comp_time of one batch:  -0.8806266784667969\n",
            "Iteration:  22685      Current loss:  17.310407638549805      Current accuracy:  15.0      Comp_time of one batch:  -0.8815982341766357\n",
            "Iteration:  22690      Current loss:  20.779220581054688      Current accuracy:  16.0      Comp_time of one batch:  -0.8962268829345703\n",
            "Iteration:  22695      Current loss:  20.19953155517578      Current accuracy:  17.0      Comp_time of one batch:  -0.8836772441864014\n",
            "Iteration:  22700      Current loss:  19.204832077026367      Current accuracy:  18.0      Comp_time of one batch:  -0.882455587387085\n",
            "Iteration:  22705      Current loss:  27.77052116394043      Current accuracy:  18.0      Comp_time of one batch:  -0.8883264064788818\n",
            "Iteration:  22710      Current loss:  25.112510681152344      Current accuracy:  18.0      Comp_time of one batch:  -0.8997671604156494\n",
            "Iteration:  22715      Current loss:  33.687294006347656      Current accuracy:  18.0      Comp_time of one batch:  -0.8990569114685059\n",
            "Iteration:  22720      Current loss:  24.25335693359375      Current accuracy:  19.0      Comp_time of one batch:  -0.9113445281982422\n",
            "Iteration:  22725      Current loss:  23.0173397064209      Current accuracy:  19.0      Comp_time of one batch:  -0.8766987323760986\n",
            "Iteration:  22730      Current loss:  21.381267547607422      Current accuracy:  20.0      Comp_time of one batch:  -0.8835694789886475\n",
            "Iteration:  22735      Current loss:  20.027727127075195      Current accuracy:  21.0      Comp_time of one batch:  -0.8929429054260254\n",
            "Iteration:  22740      Current loss:  20.83283042907715      Current accuracy:  22.0      Comp_time of one batch:  -0.8879444599151611\n",
            "Iteration:  22745      Current loss:  28.857242584228516      Current accuracy:  22.0      Comp_time of one batch:  -0.8920555114746094\n",
            "Iteration:  22750      Current loss:  30.764902114868164      Current accuracy:  22.0      Comp_time of one batch:  -0.9014203548431396\n",
            "Iteration:  22755      Current loss:  17.774612426757812      Current accuracy:  23.0      Comp_time of one batch:  -0.88388991355896\n",
            "Iteration:  22760      Current loss:  20.83675193786621      Current accuracy:  24.0      Comp_time of one batch:  -0.8813629150390625\n",
            "Iteration:  22765      Current loss:  21.45455551147461      Current accuracy:  25.0      Comp_time of one batch:  -0.8999779224395752\n",
            "Iteration:  22770      Current loss:  21.45789909362793      Current accuracy:  26.0      Comp_time of one batch:  -0.8867597579956055\n",
            "Iteration:  22775      Current loss:  20.20249366760254      Current accuracy:  27.0      Comp_time of one batch:  -0.8847317695617676\n",
            "Iteration:  22780      Current loss:  30.502044677734375      Current accuracy:  27.0      Comp_time of one batch:  -0.8836395740509033\n",
            "Iteration:  22785      Current loss:  23.146873474121094      Current accuracy:  28.0      Comp_time of one batch:  -0.9074029922485352\n",
            "Iteration:  22790      Current loss:  25.95884895324707      Current accuracy:  28.0      Comp_time of one batch:  -0.8801789283752441\n",
            "Iteration:  22795      Current loss:  26.561294555664062      Current accuracy:  28.0      Comp_time of one batch:  -0.8866293430328369\n",
            "Iteration:  22800      Current loss:  36.025230407714844      Current accuracy:  28.0      Comp_time of one batch:  -0.898745059967041\n",
            "Iteration:  22805      Current loss:  20.842235565185547      Current accuracy:  29.0      Comp_time of one batch:  -0.8874967098236084\n",
            "Iteration:  22810      Current loss:  19.093965530395508      Current accuracy:  30.0      Comp_time of one batch:  -0.8876214027404785\n",
            "Iteration:  22815      Current loss:  18.407690048217773      Current accuracy:  31.0      Comp_time of one batch:  -0.8846004009246826\n",
            "Iteration:  22820      Current loss:  26.91806411743164      Current accuracy:  31.0      Comp_time of one batch:  -0.8846163749694824\n",
            "Iteration:  22825      Current loss:  21.17992401123047      Current accuracy:  32.0      Comp_time of one batch:  -0.9003729820251465\n",
            "Iteration:  22830      Current loss:  18.875192642211914      Current accuracy:  33.0      Comp_time of one batch:  -0.885382890701294\n",
            "Iteration:  22835      Current loss:  26.27704620361328      Current accuracy:  34.0      Comp_time of one batch:  -0.9277539253234863\n",
            "Iteration:  22840      Current loss:  24.457143783569336      Current accuracy:  34.0      Comp_time of one batch:  -0.891826868057251\n",
            "Iteration:  22845      Current loss:  20.78026580810547      Current accuracy:  35.0      Comp_time of one batch:  -0.8916983604431152\n",
            "Iteration:  22850      Current loss:  19.989276885986328      Current accuracy:  36.0      Comp_time of one batch:  -0.9009823799133301\n",
            "Iteration:  22855      Current loss:  22.543132781982422      Current accuracy:  36.0      Comp_time of one batch:  -0.9130101203918457\n",
            "Iteration:  22860      Current loss:  18.986835479736328      Current accuracy:  37.0      Comp_time of one batch:  -0.8934431076049805\n",
            "Iteration:  22865      Current loss:  25.58499526977539      Current accuracy:  37.0      Comp_time of one batch:  -0.9237456321716309\n",
            "Iteration:  22870      Current loss:  19.664968490600586      Current accuracy:  38.0      Comp_time of one batch:  -0.8787937164306641\n",
            "Iteration:  22875      Current loss:  27.121185302734375      Current accuracy:  38.0      Comp_time of one batch:  -0.9148707389831543\n",
            "Iteration:  22880      Current loss:  23.794967651367188      Current accuracy:  38.0      Comp_time of one batch:  -0.9210610389709473\n",
            "Iteration:  22885      Current loss:  19.038677215576172      Current accuracy:  39.0      Comp_time of one batch:  -0.8926420211791992\n",
            "Iteration:  22890      Current loss:  27.77694320678711      Current accuracy:  39.0      Comp_time of one batch:  -0.9032144546508789\n",
            "Iteration:  22895      Current loss:  25.066789627075195      Current accuracy:  40.0      Comp_time of one batch:  -0.8862621784210205\n",
            "Iteration:  22900      Current loss:  23.50066566467285      Current accuracy:  40.0      Comp_time of one batch:  -0.9076576232910156\n",
            "Iteration:  22905      Current loss:  29.278087615966797      Current accuracy:  40.0      Comp_time of one batch:  -0.8797802925109863\n",
            "Iteration:  22910      Current loss:  22.712352752685547      Current accuracy:  41.0      Comp_time of one batch:  -0.894270658493042\n",
            "Iteration:  22915      Current loss:  20.366945266723633      Current accuracy:  42.0      Comp_time of one batch:  -0.8858895301818848\n",
            "Iteration:  22920      Current loss:  20.60593032836914      Current accuracy:  43.0      Comp_time of one batch:  -0.9198863506317139\n",
            "Iteration:  22925      Current loss:  22.17285919189453      Current accuracy:  44.0      Comp_time of one batch:  -0.9369750022888184\n",
            "Iteration:  22930      Current loss:  19.090042114257812      Current accuracy:  45.0      Comp_time of one batch:  -0.8838462829589844\n",
            "Iteration:  22935      Current loss:  34.23280334472656      Current accuracy:  45.0      Comp_time of one batch:  -0.8781020641326904\n",
            "Iteration:  22940      Current loss:  18.53030014038086      Current accuracy:  46.0      Comp_time of one batch:  -0.8800747394561768\n",
            "Iteration:  22945      Current loss:  24.44721794128418      Current accuracy:  46.0      Comp_time of one batch:  -0.8820888996124268\n",
            "Iteration:  22950      Current loss:  20.685503005981445      Current accuracy:  47.0      Comp_time of one batch:  -0.8928954601287842\n",
            "Iteration:  22955      Current loss:  22.655776977539062      Current accuracy:  48.0      Comp_time of one batch:  -0.8837931156158447\n",
            "Iteration:  22960      Current loss:  19.98541259765625      Current accuracy:  49.0      Comp_time of one batch:  -0.8760867118835449\n",
            "Iteration:  22965      Current loss:  25.577259063720703      Current accuracy:  49.0      Comp_time of one batch:  -0.9073693752288818\n",
            "Iteration:  22970      Current loss:  20.858308792114258      Current accuracy:  50.0      Comp_time of one batch:  -0.8882324695587158\n",
            "Iteration:  22975      Current loss:  18.159942626953125      Current accuracy:  51.0      Comp_time of one batch:  -0.8863339424133301\n",
            "Iteration:  22980      Current loss:  18.683195114135742      Current accuracy:  52.0      Comp_time of one batch:  -0.8861346244812012\n",
            "Iteration:  22985      Current loss:  21.790481567382812      Current accuracy:  53.0      Comp_time of one batch:  -0.8822653293609619\n",
            "Iteration:  22990      Current loss:  20.155580520629883      Current accuracy:  54.0      Comp_time of one batch:  -0.8868122100830078\n",
            "Iteration:  22995      Current loss:  25.660669326782227      Current accuracy:  54.0      Comp_time of one batch:  -0.8895037174224854\n",
            "Iteration:  23000      Current loss:  26.745689392089844      Current accuracy:  0.0      Comp_time of one batch:  -0.896007776260376\n",
            "Iteration:  23005      Current loss:  24.80095863342285      Current accuracy:  0.0      Comp_time of one batch:  -0.9113657474517822\n",
            "Iteration:  23010      Current loss:  26.236387252807617      Current accuracy:  1.0      Comp_time of one batch:  -0.9043521881103516\n",
            "Iteration:  23015      Current loss:  28.80733871459961      Current accuracy:  1.0      Comp_time of one batch:  -0.8902182579040527\n",
            "Iteration:  23020      Current loss:  20.778837203979492      Current accuracy:  2.0      Comp_time of one batch:  -0.8913438320159912\n",
            "Iteration:  23025      Current loss:  17.150068283081055      Current accuracy:  3.0      Comp_time of one batch:  -0.9029097557067871\n",
            "Iteration:  23030      Current loss:  31.94428825378418      Current accuracy:  3.0      Comp_time of one batch:  -0.8855748176574707\n",
            "Iteration:  23035      Current loss:  29.914926528930664      Current accuracy:  3.0      Comp_time of one batch:  -0.8813858032226562\n",
            "Iteration:  23040      Current loss:  26.136699676513672      Current accuracy:  3.0      Comp_time of one batch:  -0.8901855945587158\n",
            "Iteration:  23045      Current loss:  20.220064163208008      Current accuracy:  4.0      Comp_time of one batch:  -0.9338173866271973\n",
            "Iteration:  23050      Current loss:  25.163869857788086      Current accuracy:  4.0      Comp_time of one batch:  -0.9141645431518555\n",
            "Iteration:  23055      Current loss:  32.12480163574219      Current accuracy:  4.0      Comp_time of one batch:  -0.9194691181182861\n",
            "Iteration:  23060      Current loss:  24.23021125793457      Current accuracy:  4.0      Comp_time of one batch:  -0.9035813808441162\n",
            "Iteration:  23065      Current loss:  23.198026657104492      Current accuracy:  5.0      Comp_time of one batch:  -0.9037234783172607\n",
            "Iteration:  23070      Current loss:  25.04493522644043      Current accuracy:  5.0      Comp_time of one batch:  -0.8952672481536865\n",
            "Iteration:  23075      Current loss:  23.73858070373535      Current accuracy:  6.0      Comp_time of one batch:  -0.8762295246124268\n",
            "Iteration:  23080      Current loss:  35.83348083496094      Current accuracy:  6.0      Comp_time of one batch:  -0.8859274387359619\n",
            "Iteration:  23085      Current loss:  26.001571655273438      Current accuracy:  6.0      Comp_time of one batch:  -0.9156169891357422\n",
            "Iteration:  23090      Current loss:  23.600765228271484      Current accuracy:  6.0      Comp_time of one batch:  -0.8989224433898926\n",
            "Iteration:  23095      Current loss:  33.37434387207031      Current accuracy:  6.0      Comp_time of one batch:  -0.8918185234069824\n",
            "Iteration:  23100      Current loss:  24.66512107849121      Current accuracy:  7.0      Comp_time of one batch:  -0.8887410163879395\n",
            "Iteration:  23105      Current loss:  28.170228958129883      Current accuracy:  7.0      Comp_time of one batch:  -0.8991143703460693\n",
            "Iteration:  23110      Current loss:  25.850378036499023      Current accuracy:  7.0      Comp_time of one batch:  -0.8876535892486572\n",
            "Iteration:  23115      Current loss:  22.518770217895508      Current accuracy:  8.0      Comp_time of one batch:  -0.8845272064208984\n",
            "Iteration:  23120      Current loss:  24.37464714050293      Current accuracy:  8.0      Comp_time of one batch:  -0.8994960784912109\n",
            "Iteration:  23125      Current loss:  26.85173988342285      Current accuracy:  8.0      Comp_time of one batch:  -0.866504430770874\n",
            "Iteration:  23130      Current loss:  22.406822204589844      Current accuracy:  9.0      Comp_time of one batch:  -0.8997287750244141\n",
            "Iteration:  23135      Current loss:  35.99700164794922      Current accuracy:  9.0      Comp_time of one batch:  -0.9122838973999023\n",
            "Iteration:  23140      Current loss:  32.20586013793945      Current accuracy:  9.0      Comp_time of one batch:  -0.8867828845977783\n",
            "Iteration:  23145      Current loss:  24.936508178710938      Current accuracy:  9.0      Comp_time of one batch:  -0.8818051815032959\n",
            "Iteration:  23150      Current loss:  28.51292610168457      Current accuracy:  9.0      Comp_time of one batch:  -0.8810703754425049\n",
            "Iteration:  23155      Current loss:  20.851356506347656      Current accuracy:  10.0      Comp_time of one batch:  -0.9047420024871826\n",
            "Iteration:  23160      Current loss:  19.76192855834961      Current accuracy:  11.0      Comp_time of one batch:  -0.8807697296142578\n",
            "Iteration:  23165      Current loss:  21.483428955078125      Current accuracy:  12.0      Comp_time of one batch:  -0.8824560642242432\n",
            "Iteration:  23170      Current loss:  23.63876724243164      Current accuracy:  12.0      Comp_time of one batch:  -0.8971381187438965\n",
            "Iteration:  23175      Current loss:  27.417266845703125      Current accuracy:  12.0      Comp_time of one batch:  -0.8957781791687012\n",
            "Iteration:  23180      Current loss:  24.60253143310547      Current accuracy:  13.0      Comp_time of one batch:  -0.8794822692871094\n",
            "Iteration:  23185      Current loss:  20.138916015625      Current accuracy:  14.0      Comp_time of one batch:  -0.8947117328643799\n",
            "Iteration:  23190      Current loss:  28.785694122314453      Current accuracy:  14.0      Comp_time of one batch:  -0.8754079341888428\n",
            "Iteration:  23195      Current loss:  33.21072769165039      Current accuracy:  14.0      Comp_time of one batch:  -0.9090158939361572\n",
            "Iteration:  23200      Current loss:  31.76458168029785      Current accuracy:  14.0      Comp_time of one batch:  -0.8865697383880615\n",
            "Iteration:  23205      Current loss:  26.280223846435547      Current accuracy:  14.0      Comp_time of one batch:  -0.8957171440124512\n",
            "Iteration:  23210      Current loss:  26.73055648803711      Current accuracy:  14.0      Comp_time of one batch:  -0.9236259460449219\n",
            "Iteration:  23215      Current loss:  25.60898780822754      Current accuracy:  14.0      Comp_time of one batch:  -0.8917174339294434\n",
            "Iteration:  23220      Current loss:  36.571075439453125      Current accuracy:  14.0      Comp_time of one batch:  -0.9025290012359619\n",
            "Iteration:  23225      Current loss:  17.727558135986328      Current accuracy:  15.0      Comp_time of one batch:  -0.8981258869171143\n",
            "Iteration:  23230      Current loss:  23.449777603149414      Current accuracy:  15.0      Comp_time of one batch:  -0.8793392181396484\n",
            "Iteration:  23235      Current loss:  21.897165298461914      Current accuracy:  16.0      Comp_time of one batch:  -0.9032690525054932\n",
            "Iteration:  23240      Current loss:  35.04245376586914      Current accuracy:  16.0      Comp_time of one batch:  -0.8759686946868896\n",
            "Iteration:  23245      Current loss:  25.62758445739746      Current accuracy:  16.0      Comp_time of one batch:  -0.8796372413635254\n",
            "Iteration:  23250      Current loss:  21.174823760986328      Current accuracy:  17.0      Comp_time of one batch:  -0.8820803165435791\n",
            "Iteration:  23255      Current loss:  23.765926361083984      Current accuracy:  17.0      Comp_time of one batch:  -0.9017670154571533\n",
            "Iteration:  23260      Current loss:  17.26169204711914      Current accuracy:  18.0      Comp_time of one batch:  -0.9018349647521973\n",
            "Iteration:  23265      Current loss:  20.242250442504883      Current accuracy:  19.0      Comp_time of one batch:  -0.9137568473815918\n",
            "Iteration:  23270      Current loss:  20.827131271362305      Current accuracy:  20.0      Comp_time of one batch:  -0.8931090831756592\n",
            "Iteration:  23275      Current loss:  22.1566162109375      Current accuracy:  21.0      Comp_time of one batch:  -0.8775947093963623\n",
            "Iteration:  23280      Current loss:  18.721920013427734      Current accuracy:  22.0      Comp_time of one batch:  -0.8889408111572266\n",
            "Iteration:  23285      Current loss:  18.421354293823242      Current accuracy:  23.0      Comp_time of one batch:  -0.8745334148406982\n",
            "Iteration:  23290      Current loss:  19.615814208984375      Current accuracy:  24.0      Comp_time of one batch:  -0.8854167461395264\n",
            "Iteration:  23295      Current loss:  24.936403274536133      Current accuracy:  24.0      Comp_time of one batch:  -0.8833627700805664\n",
            "Iteration:  23300      Current loss:  33.285552978515625      Current accuracy:  24.0      Comp_time of one batch:  -0.8759546279907227\n",
            "Iteration:  23305      Current loss:  19.690073013305664      Current accuracy:  25.0      Comp_time of one batch:  -0.8868672847747803\n",
            "Iteration:  23310      Current loss:  22.309335708618164      Current accuracy:  26.0      Comp_time of one batch:  -0.9100606441497803\n",
            "Iteration:  23315      Current loss:  16.907258987426758      Current accuracy:  27.0      Comp_time of one batch:  -0.8884015083312988\n",
            "Iteration:  23320      Current loss:  18.509435653686523      Current accuracy:  28.0      Comp_time of one batch:  -0.9046902656555176\n",
            "Iteration:  23325      Current loss:  20.125699996948242      Current accuracy:  29.0      Comp_time of one batch:  -0.9065403938293457\n",
            "Iteration:  23330      Current loss:  21.963045120239258      Current accuracy:  30.0      Comp_time of one batch:  -0.884575605392456\n",
            "Iteration:  23335      Current loss:  18.547588348388672      Current accuracy:  31.0      Comp_time of one batch:  -0.9023270606994629\n",
            "Iteration:  23340      Current loss:  16.400720596313477      Current accuracy:  32.0      Comp_time of one batch:  -0.8852951526641846\n",
            "Iteration:  23345      Current loss:  29.182323455810547      Current accuracy:  32.0      Comp_time of one batch:  -0.9178462028503418\n",
            "Iteration:  23350      Current loss:  21.48816680908203      Current accuracy:  33.0      Comp_time of one batch:  -0.8942902088165283\n",
            "Iteration:  23355      Current loss:  24.887351989746094      Current accuracy:  33.0      Comp_time of one batch:  -0.8807640075683594\n",
            "Iteration:  23360      Current loss:  23.460294723510742      Current accuracy:  33.0      Comp_time of one batch:  -0.8857142925262451\n",
            "Iteration:  23365      Current loss:  18.360841751098633      Current accuracy:  34.0      Comp_time of one batch:  -0.8785240650177002\n",
            "Iteration:  23370      Current loss:  18.385787963867188      Current accuracy:  35.0      Comp_time of one batch:  -0.8800063133239746\n",
            "Iteration:  23375      Current loss:  26.099172592163086      Current accuracy:  35.0      Comp_time of one batch:  -0.9186320304870605\n",
            "Iteration:  23380      Current loss:  28.251859664916992      Current accuracy:  35.0      Comp_time of one batch:  -0.9063510894775391\n",
            "Iteration:  23385      Current loss:  25.863759994506836      Current accuracy:  35.0      Comp_time of one batch:  -0.8799638748168945\n",
            "Iteration:  23390      Current loss:  28.336856842041016      Current accuracy:  35.0      Comp_time of one batch:  -0.8753635883331299\n",
            "Iteration:  23395      Current loss:  21.800098419189453      Current accuracy:  36.0      Comp_time of one batch:  -0.8912112712860107\n",
            "Iteration:  23400      Current loss:  21.404136657714844      Current accuracy:  37.0      Comp_time of one batch:  -0.9082560539245605\n",
            "Iteration:  23405      Current loss:  17.77789306640625      Current accuracy:  38.0      Comp_time of one batch:  -0.9230489730834961\n",
            "Iteration:  23410      Current loss:  16.151809692382812      Current accuracy:  39.0      Comp_time of one batch:  -0.8898782730102539\n",
            "Iteration:  23415      Current loss:  18.156461715698242      Current accuracy:  40.0      Comp_time of one batch:  -0.9011490345001221\n",
            "Iteration:  23420      Current loss:  19.740264892578125      Current accuracy:  41.0      Comp_time of one batch:  -0.8788974285125732\n",
            "Iteration:  23425      Current loss:  21.464872360229492      Current accuracy:  42.0      Comp_time of one batch:  -0.901214599609375\n",
            "Iteration:  23430      Current loss:  24.860273361206055      Current accuracy:  42.0      Comp_time of one batch:  -0.897000789642334\n",
            "Iteration:  23435      Current loss:  17.873899459838867      Current accuracy:  43.0      Comp_time of one batch:  -0.90692138671875\n",
            "Iteration:  23440      Current loss:  21.834735870361328      Current accuracy:  44.0      Comp_time of one batch:  -0.9027960300445557\n",
            "Iteration:  23445      Current loss:  21.223033905029297      Current accuracy:  45.0      Comp_time of one batch:  -0.9397125244140625\n",
            "Iteration:  23450      Current loss:  23.665252685546875      Current accuracy:  46.0      Comp_time of one batch:  -0.9129254817962646\n",
            "Iteration:  23455      Current loss:  23.623445510864258      Current accuracy:  46.0      Comp_time of one batch:  -0.8829071521759033\n",
            "Iteration:  23460      Current loss:  18.96831512451172      Current accuracy:  47.0      Comp_time of one batch:  -0.8810839653015137\n",
            "Iteration:  23465      Current loss:  22.30044174194336      Current accuracy:  48.0      Comp_time of one batch:  -0.8793826103210449\n",
            "Iteration:  23470      Current loss:  24.564077377319336      Current accuracy:  48.0      Comp_time of one batch:  -0.8839261531829834\n",
            "Iteration:  23475      Current loss:  19.35908317565918      Current accuracy:  49.0      Comp_time of one batch:  -0.8994190692901611\n",
            "Iteration:  23480      Current loss:  23.150062561035156      Current accuracy:  49.0      Comp_time of one batch:  -0.8767886161804199\n",
            "Iteration:  23485      Current loss:  19.43169593811035      Current accuracy:  50.0      Comp_time of one batch:  -0.8920457363128662\n",
            "Iteration:  23490      Current loss:  28.38119888305664      Current accuracy:  50.0      Comp_time of one batch:  -0.9165971279144287\n",
            "Iteration:  23495      Current loss:  24.727890014648438      Current accuracy:  50.0      Comp_time of one batch:  -0.8899405002593994\n",
            "Iteration:  23500      Current loss:  20.518680572509766      Current accuracy:  0.0      Comp_time of one batch:  -0.9028019905090332\n",
            "Iteration:  23505      Current loss:  22.287668228149414      Current accuracy:  0.0      Comp_time of one batch:  -0.8979735374450684\n",
            "Iteration:  23510      Current loss:  21.999475479125977      Current accuracy:  1.0      Comp_time of one batch:  -0.8970201015472412\n",
            "Iteration:  23515      Current loss:  32.51601791381836      Current accuracy:  1.0      Comp_time of one batch:  -0.8832612037658691\n",
            "Iteration:  23520      Current loss:  23.22562599182129      Current accuracy:  2.0      Comp_time of one batch:  -0.8818058967590332\n",
            "Iteration:  23525      Current loss:  24.68528938293457      Current accuracy:  3.0      Comp_time of one batch:  -0.8946676254272461\n",
            "Iteration:  23530      Current loss:  25.98697853088379      Current accuracy:  3.0      Comp_time of one batch:  -0.887446403503418\n",
            "Iteration:  23535      Current loss:  28.13954734802246      Current accuracy:  3.0      Comp_time of one batch:  -0.8751983642578125\n",
            "Iteration:  23540      Current loss:  29.2775936126709      Current accuracy:  3.0      Comp_time of one batch:  -0.8755285739898682\n",
            "Iteration:  23545      Current loss:  22.63625144958496      Current accuracy:  4.0      Comp_time of one batch:  -0.886868953704834\n",
            "Iteration:  23550      Current loss:  36.25674819946289      Current accuracy:  4.0      Comp_time of one batch:  -0.8873584270477295\n",
            "Iteration:  23555      Current loss:  24.76187515258789      Current accuracy:  4.0      Comp_time of one batch:  -0.8803682327270508\n",
            "Iteration:  23560      Current loss:  24.424184799194336      Current accuracy:  5.0      Comp_time of one batch:  -0.882251501083374\n",
            "Iteration:  23565      Current loss:  18.41337776184082      Current accuracy:  6.0      Comp_time of one batch:  -0.891958475112915\n",
            "Iteration:  23570      Current loss:  25.33721923828125      Current accuracy:  6.0      Comp_time of one batch:  -0.8849642276763916\n",
            "Iteration:  23575      Current loss:  18.12804412841797      Current accuracy:  7.0      Comp_time of one batch:  -0.8824617862701416\n",
            "Iteration:  23580      Current loss:  21.008451461791992      Current accuracy:  8.0      Comp_time of one batch:  -0.8770651817321777\n",
            "Iteration:  23585      Current loss:  23.105796813964844      Current accuracy:  9.0      Comp_time of one batch:  -0.8833560943603516\n",
            "Iteration:  23590      Current loss:  21.491519927978516      Current accuracy:  10.0      Comp_time of one batch:  -0.9255986213684082\n",
            "Iteration:  23595      Current loss:  23.94600486755371      Current accuracy:  10.0      Comp_time of one batch:  -0.8931052684783936\n",
            "Iteration:  23600      Current loss:  19.839815139770508      Current accuracy:  11.0      Comp_time of one batch:  -0.9113879203796387\n",
            "Iteration:  23605      Current loss:  35.33426284790039      Current accuracy:  11.0      Comp_time of one batch:  -0.9019453525543213\n",
            "Iteration:  23610      Current loss:  20.525169372558594      Current accuracy:  12.0      Comp_time of one batch:  -0.8899898529052734\n",
            "Iteration:  23615      Current loss:  29.79433822631836      Current accuracy:  12.0      Comp_time of one batch:  -0.888843297958374\n",
            "Iteration:  23620      Current loss:  37.227867126464844      Current accuracy:  12.0      Comp_time of one batch:  -0.8886504173278809\n",
            "Iteration:  23625      Current loss:  20.837270736694336      Current accuracy:  13.0      Comp_time of one batch:  -0.8962628841400146\n",
            "Iteration:  23630      Current loss:  23.84666633605957      Current accuracy:  13.0      Comp_time of one batch:  -0.9073634147644043\n",
            "Iteration:  23635      Current loss:  21.67978286743164      Current accuracy:  14.0      Comp_time of one batch:  -0.8804175853729248\n",
            "Iteration:  23640      Current loss:  24.53179168701172      Current accuracy:  14.0      Comp_time of one batch:  -0.8764073848724365\n",
            "Iteration:  23645      Current loss:  26.129831314086914      Current accuracy:  14.0      Comp_time of one batch:  -0.8832919597625732\n",
            "Iteration:  23650      Current loss:  20.9364013671875      Current accuracy:  15.0      Comp_time of one batch:  -0.9143719673156738\n",
            "Iteration:  23655      Current loss:  20.116622924804688      Current accuracy:  16.0      Comp_time of one batch:  -0.9320495128631592\n",
            "Iteration:  23660      Current loss:  31.447540283203125      Current accuracy:  16.0      Comp_time of one batch:  -0.9075186252593994\n",
            "Iteration:  23665      Current loss:  26.66960906982422      Current accuracy:  16.0      Comp_time of one batch:  -0.9032371044158936\n",
            "Iteration:  23670      Current loss:  26.20805549621582      Current accuracy:  16.0      Comp_time of one batch:  -0.8715713024139404\n",
            "Iteration:  23675      Current loss:  21.86598777770996      Current accuracy:  17.0      Comp_time of one batch:  -0.870255708694458\n",
            "Iteration:  23680      Current loss:  24.371129989624023      Current accuracy:  17.0      Comp_time of one batch:  -0.8827481269836426\n",
            "Iteration:  23685      Current loss:  25.354467391967773      Current accuracy:  17.0      Comp_time of one batch:  -0.8890125751495361\n",
            "Iteration:  23690      Current loss:  18.795381546020508      Current accuracy:  18.0      Comp_time of one batch:  -0.8859755992889404\n",
            "Iteration:  23695      Current loss:  23.20513916015625      Current accuracy:  18.0      Comp_time of one batch:  -0.9179410934448242\n",
            "Iteration:  23700      Current loss:  17.268396377563477      Current accuracy:  19.0      Comp_time of one batch:  -0.8935363292694092\n",
            "Iteration:  23705      Current loss:  18.625465393066406      Current accuracy:  20.0      Comp_time of one batch:  -0.8791646957397461\n",
            "Iteration:  23710      Current loss:  21.71231460571289      Current accuracy:  21.0      Comp_time of one batch:  -0.8842008113861084\n",
            "Iteration:  23715      Current loss:  17.334068298339844      Current accuracy:  22.0      Comp_time of one batch:  -0.8856754302978516\n",
            "Iteration:  23720      Current loss:  34.786216735839844      Current accuracy:  22.0      Comp_time of one batch:  -0.8967704772949219\n",
            "Iteration:  23725      Current loss:  22.40777015686035      Current accuracy:  23.0      Comp_time of one batch:  -0.8883395195007324\n",
            "Iteration:  23730      Current loss:  21.984952926635742      Current accuracy:  24.0      Comp_time of one batch:  -0.883155107498169\n",
            "Iteration:  23735      Current loss:  33.486629486083984      Current accuracy:  24.0      Comp_time of one batch:  -0.9281966686248779\n",
            "Iteration:  23740      Current loss:  31.30050277709961      Current accuracy:  24.0      Comp_time of one batch:  -0.8908116817474365\n",
            "Iteration:  23745      Current loss:  20.287477493286133      Current accuracy:  25.0      Comp_time of one batch:  -0.8871817588806152\n",
            "Iteration:  23750      Current loss:  27.54717254638672      Current accuracy:  25.0      Comp_time of one batch:  -0.917813777923584\n",
            "Iteration:  23755      Current loss:  23.605676651000977      Current accuracy:  25.0      Comp_time of one batch:  -0.9235219955444336\n",
            "Iteration:  23760      Current loss:  19.510862350463867      Current accuracy:  26.0      Comp_time of one batch:  -0.8886971473693848\n",
            "Iteration:  23765      Current loss:  35.0845832824707      Current accuracy:  26.0      Comp_time of one batch:  -0.8930330276489258\n",
            "Iteration:  23770      Current loss:  26.663427352905273      Current accuracy:  26.0      Comp_time of one batch:  -0.8868138790130615\n",
            "Iteration:  23775      Current loss:  24.34993553161621      Current accuracy:  26.0      Comp_time of one batch:  -0.9089441299438477\n",
            "Iteration:  23780      Current loss:  21.293991088867188      Current accuracy:  27.0      Comp_time of one batch:  -0.8800525665283203\n",
            "Iteration:  23785      Current loss:  19.961145401000977      Current accuracy:  28.0      Comp_time of one batch:  -0.8989877700805664\n",
            "Iteration:  23790      Current loss:  21.898643493652344      Current accuracy:  29.0      Comp_time of one batch:  -0.8885824680328369\n",
            "Iteration:  23795      Current loss:  20.71569061279297      Current accuracy:  30.0      Comp_time of one batch:  -0.9000897407531738\n",
            "Iteration:  23800      Current loss:  21.16972541809082      Current accuracy:  31.0      Comp_time of one batch:  -0.8904075622558594\n",
            "Iteration:  23805      Current loss:  16.5457763671875      Current accuracy:  32.0      Comp_time of one batch:  -0.9059360027313232\n",
            "Iteration:  23810      Current loss:  18.4939022064209      Current accuracy:  33.0      Comp_time of one batch:  -0.9192965030670166\n",
            "Iteration:  23815      Current loss:  28.50575065612793      Current accuracy:  33.0      Comp_time of one batch:  -0.88755202293396\n",
            "Iteration:  23820      Current loss:  23.232175827026367      Current accuracy:  33.0      Comp_time of one batch:  -0.8885931968688965\n",
            "Iteration:  23825      Current loss:  20.878376007080078      Current accuracy:  34.0      Comp_time of one batch:  -0.897540807723999\n",
            "Iteration:  23830      Current loss:  17.93511199951172      Current accuracy:  35.0      Comp_time of one batch:  -0.8930089473724365\n",
            "Iteration:  23835      Current loss:  23.888883590698242      Current accuracy:  35.0      Comp_time of one batch:  -0.8842949867248535\n",
            "Iteration:  23840      Current loss:  22.74921417236328      Current accuracy:  35.0      Comp_time of one batch:  -0.8741364479064941\n",
            "Iteration:  23845      Current loss:  31.54534339904785      Current accuracy:  35.0      Comp_time of one batch:  -0.8804531097412109\n",
            "Iteration:  23850      Current loss:  25.583547592163086      Current accuracy:  35.0      Comp_time of one batch:  -0.9029831886291504\n",
            "Iteration:  23855      Current loss:  26.20123863220215      Current accuracy:  35.0      Comp_time of one batch:  -0.8917028903961182\n",
            "Iteration:  23860      Current loss:  20.531946182250977      Current accuracy:  36.0      Comp_time of one batch:  -0.8760640621185303\n",
            "Iteration:  23865      Current loss:  36.71284866333008      Current accuracy:  36.0      Comp_time of one batch:  -0.9056832790374756\n",
            "Iteration:  23870      Current loss:  24.720321655273438      Current accuracy:  36.0      Comp_time of one batch:  -0.893385648727417\n",
            "Iteration:  23875      Current loss:  27.635282516479492      Current accuracy:  36.0      Comp_time of one batch:  -0.8980987071990967\n",
            "Iteration:  23880      Current loss:  24.4520320892334      Current accuracy:  36.0      Comp_time of one batch:  -0.9134652614593506\n",
            "Iteration:  23885      Current loss:  25.104293823242188      Current accuracy:  36.0      Comp_time of one batch:  -0.8782069683074951\n",
            "Iteration:  23890      Current loss:  23.070837020874023      Current accuracy:  36.0      Comp_time of one batch:  -0.8928976058959961\n",
            "Iteration:  23895      Current loss:  20.439050674438477      Current accuracy:  37.0      Comp_time of one batch:  -0.8856568336486816\n",
            "Iteration:  23900      Current loss:  26.417417526245117      Current accuracy:  37.0      Comp_time of one batch:  -0.9096543788909912\n",
            "Iteration:  23905      Current loss:  24.26461410522461      Current accuracy:  37.0      Comp_time of one batch:  -0.8843863010406494\n",
            "Iteration:  23910      Current loss:  24.53567886352539      Current accuracy:  37.0      Comp_time of one batch:  -0.882570743560791\n",
            "Iteration:  23915      Current loss:  29.533748626708984      Current accuracy:  37.0      Comp_time of one batch:  -0.9195253849029541\n",
            "Iteration:  23920      Current loss:  21.092985153198242      Current accuracy:  38.0      Comp_time of one batch:  -0.8844666481018066\n",
            "Iteration:  23925      Current loss:  38.2509880065918      Current accuracy:  38.0      Comp_time of one batch:  -0.8893086910247803\n",
            "Iteration:  23930      Current loss:  21.63191032409668      Current accuracy:  39.0      Comp_time of one batch:  -0.94303297996521\n",
            "Iteration:  23935      Current loss:  20.983108520507812      Current accuracy:  40.0      Comp_time of one batch:  -0.8744392395019531\n",
            "Iteration:  23940      Current loss:  30.479965209960938      Current accuracy:  40.0      Comp_time of one batch:  -0.9041445255279541\n",
            "Iteration:  23945      Current loss:  18.165857315063477      Current accuracy:  41.0      Comp_time of one batch:  -0.9043302536010742\n",
            "Iteration:  23950      Current loss:  23.160985946655273      Current accuracy:  41.0      Comp_time of one batch:  -0.8804223537445068\n",
            "Iteration:  23955      Current loss:  21.255779266357422      Current accuracy:  42.0      Comp_time of one batch:  -0.9056997299194336\n",
            "Iteration:  23960      Current loss:  26.585203170776367      Current accuracy:  43.0      Comp_time of one batch:  -0.8756911754608154\n",
            "Iteration:  23965      Current loss:  19.450820922851562      Current accuracy:  44.0      Comp_time of one batch:  -0.8849353790283203\n",
            "Iteration:  23970      Current loss:  16.54758644104004      Current accuracy:  45.0      Comp_time of one batch:  -0.8883790969848633\n",
            "Iteration:  23975      Current loss:  22.30988121032715      Current accuracy:  46.0      Comp_time of one batch:  -0.8831522464752197\n",
            "Iteration:  23980      Current loss:  28.321557998657227      Current accuracy:  46.0      Comp_time of one batch:  -0.8952999114990234\n",
            "Iteration:  23985      Current loss:  25.477672576904297      Current accuracy:  46.0      Comp_time of one batch:  -0.8840110301971436\n",
            "Iteration:  23990      Current loss:  23.767044067382812      Current accuracy:  46.0      Comp_time of one batch:  -0.8818933963775635\n",
            "Iteration:  23995      Current loss:  23.312196731567383      Current accuracy:  46.0      Comp_time of one batch:  -0.8758964538574219\n",
            "Iteration:  24000      Current loss:  19.403915405273438      Current accuracy:  0.0      Comp_time of one batch:  -0.8890049457550049\n",
            "Iteration:  24005      Current loss:  20.374526977539062      Current accuracy:  1.0      Comp_time of one batch:  -0.9066016674041748\n",
            "Iteration:  24010      Current loss:  20.616825103759766      Current accuracy:  2.0      Comp_time of one batch:  -0.8886308670043945\n",
            "Iteration:  24015      Current loss:  21.85941505432129      Current accuracy:  3.0      Comp_time of one batch:  -0.879533052444458\n",
            "Iteration:  24020      Current loss:  30.279029846191406      Current accuracy:  3.0      Comp_time of one batch:  -0.9090383052825928\n",
            "Iteration:  24025      Current loss:  37.139854431152344      Current accuracy:  3.0      Comp_time of one batch:  -0.8856513500213623\n",
            "Iteration:  24030      Current loss:  24.76304054260254      Current accuracy:  3.0      Comp_time of one batch:  -0.8920159339904785\n",
            "Iteration:  24035      Current loss:  19.21316909790039      Current accuracy:  4.0      Comp_time of one batch:  -0.8990249633789062\n",
            "Iteration:  24040      Current loss:  31.2802677154541      Current accuracy:  4.0      Comp_time of one batch:  -0.8957054615020752\n",
            "Iteration:  24045      Current loss:  24.306060791015625      Current accuracy:  4.0      Comp_time of one batch:  -0.8913404941558838\n",
            "Iteration:  24050      Current loss:  26.418701171875      Current accuracy:  4.0      Comp_time of one batch:  -0.9027767181396484\n",
            "Iteration:  24055      Current loss:  21.13426971435547      Current accuracy:  5.0      Comp_time of one batch:  -0.9014689922332764\n",
            "Iteration:  24060      Current loss:  32.85043716430664      Current accuracy:  5.0      Comp_time of one batch:  -0.8885486125946045\n",
            "Iteration:  24065      Current loss:  23.31662940979004      Current accuracy:  6.0      Comp_time of one batch:  -0.9086771011352539\n",
            "Iteration:  24070      Current loss:  20.294200897216797      Current accuracy:  7.0      Comp_time of one batch:  -0.8731107711791992\n",
            "Iteration:  24075      Current loss:  18.564863204956055      Current accuracy:  8.0      Comp_time of one batch:  -0.8875610828399658\n",
            "Iteration:  24080      Current loss:  24.039045333862305      Current accuracy:  8.0      Comp_time of one batch:  -0.905179500579834\n",
            "Iteration:  24085      Current loss:  30.794647216796875      Current accuracy:  8.0      Comp_time of one batch:  -0.9089024066925049\n",
            "Iteration:  24090      Current loss:  22.2005615234375      Current accuracy:  9.0      Comp_time of one batch:  -0.9183642864227295\n",
            "Iteration:  24095      Current loss:  28.732568740844727      Current accuracy:  9.0      Comp_time of one batch:  -0.9106893539428711\n",
            "Iteration:  24100      Current loss:  24.603164672851562      Current accuracy:  9.0      Comp_time of one batch:  -0.9246253967285156\n",
            "Iteration:  24105      Current loss:  19.058143615722656      Current accuracy:  10.0      Comp_time of one batch:  -0.9048953056335449\n",
            "Iteration:  24110      Current loss:  20.411231994628906      Current accuracy:  11.0      Comp_time of one batch:  -0.9109468460083008\n",
            "Iteration:  24115      Current loss:  29.118663787841797      Current accuracy:  11.0      Comp_time of one batch:  -0.8876383304595947\n",
            "Iteration:  24120      Current loss:  29.116777420043945      Current accuracy:  11.0      Comp_time of one batch:  -0.9208762645721436\n",
            "Iteration:  24125      Current loss:  25.509368896484375      Current accuracy:  11.0      Comp_time of one batch:  -0.908111572265625\n",
            "Iteration:  24130      Current loss:  18.184005737304688      Current accuracy:  12.0      Comp_time of one batch:  -0.8850593566894531\n",
            "Iteration:  24135      Current loss:  19.523820877075195      Current accuracy:  13.0      Comp_time of one batch:  -0.8760519027709961\n",
            "Iteration:  24140      Current loss:  25.13705825805664      Current accuracy:  13.0      Comp_time of one batch:  -0.8731167316436768\n",
            "Iteration:  24145      Current loss:  19.676513671875      Current accuracy:  14.0      Comp_time of one batch:  -0.9011585712432861\n",
            "Iteration:  24150      Current loss:  22.523523330688477      Current accuracy:  14.0      Comp_time of one batch:  -0.875051736831665\n",
            "Iteration:  24155      Current loss:  17.906476974487305      Current accuracy:  15.0      Comp_time of one batch:  -0.9096560478210449\n",
            "Iteration:  24160      Current loss:  22.537961959838867      Current accuracy:  16.0      Comp_time of one batch:  -0.8763635158538818\n",
            "Iteration:  24165      Current loss:  36.36069869995117      Current accuracy:  16.0      Comp_time of one batch:  -0.8817553520202637\n",
            "Iteration:  24170      Current loss:  20.640535354614258      Current accuracy:  17.0      Comp_time of one batch:  -0.9029085636138916\n",
            "Iteration:  24175      Current loss:  31.357641220092773      Current accuracy:  17.0      Comp_time of one batch:  -0.9116923809051514\n",
            "Iteration:  24180      Current loss:  23.82470703125      Current accuracy:  17.0      Comp_time of one batch:  -0.9009838104248047\n",
            "Iteration:  24185      Current loss:  23.07750129699707      Current accuracy:  17.0      Comp_time of one batch:  -0.877342700958252\n",
            "Iteration:  24190      Current loss:  26.7055721282959      Current accuracy:  17.0      Comp_time of one batch:  -0.8907797336578369\n",
            "Iteration:  24195      Current loss:  29.259618759155273      Current accuracy:  17.0      Comp_time of one batch:  -0.9050467014312744\n",
            "Iteration:  24200      Current loss:  22.722368240356445      Current accuracy:  18.0      Comp_time of one batch:  -0.8765878677368164\n",
            "Iteration:  24205      Current loss:  28.918779373168945      Current accuracy:  18.0      Comp_time of one batch:  -0.8929393291473389\n",
            "Iteration:  24210      Current loss:  23.848709106445312      Current accuracy:  18.0      Comp_time of one batch:  -0.9365730285644531\n",
            "Iteration:  24215      Current loss:  26.156774520874023      Current accuracy:  18.0      Comp_time of one batch:  -0.9080991744995117\n",
            "Iteration:  24220      Current loss:  21.03073501586914      Current accuracy:  19.0      Comp_time of one batch:  -0.9368007183074951\n",
            "Iteration:  24225      Current loss:  28.58614730834961      Current accuracy:  19.0      Comp_time of one batch:  -0.8830995559692383\n",
            "Iteration:  24230      Current loss:  20.73232650756836      Current accuracy:  20.0      Comp_time of one batch:  -0.9105100631713867\n",
            "Iteration:  24235      Current loss:  16.51787567138672      Current accuracy:  21.0      Comp_time of one batch:  -0.888331413269043\n",
            "Iteration:  24240      Current loss:  18.471860885620117      Current accuracy:  22.0      Comp_time of one batch:  -0.937502384185791\n",
            "Iteration:  24245      Current loss:  28.25796890258789      Current accuracy:  22.0      Comp_time of one batch:  -0.8864166736602783\n",
            "Iteration:  24250      Current loss:  23.291593551635742      Current accuracy:  22.0      Comp_time of one batch:  -0.8988449573516846\n",
            "Iteration:  24255      Current loss:  25.658992767333984      Current accuracy:  22.0      Comp_time of one batch:  -0.8817150592803955\n",
            "Iteration:  24260      Current loss:  24.928104400634766      Current accuracy:  22.0      Comp_time of one batch:  -0.8827874660491943\n",
            "Iteration:  24265      Current loss:  27.603546142578125      Current accuracy:  22.0      Comp_time of one batch:  -0.923429012298584\n",
            "Iteration:  24270      Current loss:  21.891061782836914      Current accuracy:  23.0      Comp_time of one batch:  -0.8934273719787598\n",
            "Iteration:  24275      Current loss:  29.701547622680664      Current accuracy:  23.0      Comp_time of one batch:  -0.8983266353607178\n",
            "Iteration:  24280      Current loss:  21.46965217590332      Current accuracy:  24.0      Comp_time of one batch:  -0.8849749565124512\n",
            "Iteration:  24285      Current loss:  25.026840209960938      Current accuracy:  24.0      Comp_time of one batch:  -0.8921077251434326\n",
            "Iteration:  24290      Current loss:  20.92854881286621      Current accuracy:  25.0      Comp_time of one batch:  -0.8988833427429199\n",
            "Iteration:  24295      Current loss:  27.48087501525879      Current accuracy:  25.0      Comp_time of one batch:  -0.8814151287078857\n",
            "Iteration:  24300      Current loss:  19.323348999023438      Current accuracy:  26.0      Comp_time of one batch:  -0.8797962665557861\n",
            "Iteration:  24305      Current loss:  29.808565139770508      Current accuracy:  26.0      Comp_time of one batch:  -0.903878927230835\n",
            "Iteration:  24310      Current loss:  29.332496643066406      Current accuracy:  26.0      Comp_time of one batch:  -0.8954048156738281\n",
            "Iteration:  24315      Current loss:  23.50943946838379      Current accuracy:  26.0      Comp_time of one batch:  -0.8854200839996338\n",
            "Iteration:  24320      Current loss:  20.999624252319336      Current accuracy:  27.0      Comp_time of one batch:  -0.8870716094970703\n",
            "Iteration:  24325      Current loss:  23.65228843688965      Current accuracy:  27.0      Comp_time of one batch:  -0.9083304405212402\n",
            "Iteration:  24330      Current loss:  28.806703567504883      Current accuracy:  27.0      Comp_time of one batch:  -0.882143497467041\n",
            "Iteration:  24335      Current loss:  28.420162200927734      Current accuracy:  27.0      Comp_time of one batch:  -0.9073295593261719\n",
            "Iteration:  24340      Current loss:  19.374494552612305      Current accuracy:  28.0      Comp_time of one batch:  -0.8906877040863037\n",
            "Iteration:  24345      Current loss:  21.31040382385254      Current accuracy:  29.0      Comp_time of one batch:  -0.8914592266082764\n",
            "Iteration:  24350      Current loss:  20.70121192932129      Current accuracy:  30.0      Comp_time of one batch:  -0.8864414691925049\n",
            "Iteration:  24355      Current loss:  24.27062225341797      Current accuracy:  30.0      Comp_time of one batch:  -0.8754422664642334\n",
            "Iteration:  24360      Current loss:  25.42102813720703      Current accuracy:  30.0      Comp_time of one batch:  -0.895493745803833\n",
            "Iteration:  24365      Current loss:  31.54466438293457      Current accuracy:  30.0      Comp_time of one batch:  -0.8859553337097168\n",
            "Iteration:  24370      Current loss:  31.76378631591797      Current accuracy:  30.0      Comp_time of one batch:  -0.9028229713439941\n",
            "Iteration:  24375      Current loss:  28.624183654785156      Current accuracy:  30.0      Comp_time of one batch:  -0.8906927108764648\n",
            "Iteration:  24380      Current loss:  28.019001007080078      Current accuracy:  30.0      Comp_time of one batch:  -0.8781924247741699\n",
            "Iteration:  24385      Current loss:  29.760448455810547      Current accuracy:  30.0      Comp_time of one batch:  -0.899937629699707\n",
            "Iteration:  24390      Current loss:  20.370498657226562      Current accuracy:  31.0      Comp_time of one batch:  -0.896073579788208\n",
            "Iteration:  24395      Current loss:  24.83591079711914      Current accuracy:  31.0      Comp_time of one batch:  -0.8824317455291748\n",
            "Iteration:  24400      Current loss:  25.375703811645508      Current accuracy:  32.0      Comp_time of one batch:  -0.8986513614654541\n",
            "Iteration:  24405      Current loss:  27.548969268798828      Current accuracy:  32.0      Comp_time of one batch:  -0.9054222106933594\n",
            "Iteration:  24410      Current loss:  17.05167007446289      Current accuracy:  33.0      Comp_time of one batch:  -0.8880085945129395\n",
            "Iteration:  24415      Current loss:  38.03997802734375      Current accuracy:  33.0      Comp_time of one batch:  -0.9035995006561279\n",
            "Iteration:  24420      Current loss:  20.20686149597168      Current accuracy:  34.0      Comp_time of one batch:  -0.8738293647766113\n",
            "Iteration:  24425      Current loss:  22.830583572387695      Current accuracy:  34.0      Comp_time of one batch:  -0.9022390842437744\n",
            "Iteration:  24430      Current loss:  21.887109756469727      Current accuracy:  35.0      Comp_time of one batch:  -0.8850922584533691\n",
            "Iteration:  24435      Current loss:  20.54665756225586      Current accuracy:  36.0      Comp_time of one batch:  -0.8937587738037109\n",
            "Iteration:  24440      Current loss:  20.96418571472168      Current accuracy:  37.0      Comp_time of one batch:  -0.8918790817260742\n",
            "Iteration:  24445      Current loss:  21.458484649658203      Current accuracy:  38.0      Comp_time of one batch:  -0.8944897651672363\n",
            "Iteration:  24450      Current loss:  29.063751220703125      Current accuracy:  38.0      Comp_time of one batch:  -0.9086456298828125\n",
            "Iteration:  24455      Current loss:  24.40928840637207      Current accuracy:  38.0      Comp_time of one batch:  -0.8929665088653564\n",
            "Iteration:  24460      Current loss:  22.34358787536621      Current accuracy:  39.0      Comp_time of one batch:  -0.8901114463806152\n",
            "Iteration:  24465      Current loss:  24.546897888183594      Current accuracy:  39.0      Comp_time of one batch:  -0.8997321128845215\n",
            "Iteration:  24470      Current loss:  19.14716148376465      Current accuracy:  40.0      Comp_time of one batch:  -0.8928344249725342\n",
            "Iteration:  24475      Current loss:  30.770626068115234      Current accuracy:  40.0      Comp_time of one batch:  -0.9039089679718018\n",
            "Iteration:  24480      Current loss:  24.492008209228516      Current accuracy:  40.0      Comp_time of one batch:  -0.9019169807434082\n",
            "Iteration:  24485      Current loss:  29.601057052612305      Current accuracy:  40.0      Comp_time of one batch:  -0.8961338996887207\n",
            "Iteration:  24490      Current loss:  19.72977066040039      Current accuracy:  41.0      Comp_time of one batch:  -0.8725032806396484\n",
            "Iteration:  24495      Current loss:  28.014936447143555      Current accuracy:  41.0      Comp_time of one batch:  -0.9041893482208252\n",
            "Iteration:  24500      Current loss:  21.982627868652344      Current accuracy:  0.0      Comp_time of one batch:  -0.8991048336029053\n",
            "Iteration:  24505      Current loss:  35.24003219604492      Current accuracy:  0.0      Comp_time of one batch:  -0.883167028427124\n",
            "Iteration:  24510      Current loss:  19.667518615722656      Current accuracy:  1.0      Comp_time of one batch:  -0.9046003818511963\n",
            "Iteration:  24515      Current loss:  28.73484992980957      Current accuracy:  1.0      Comp_time of one batch:  -0.9351935386657715\n",
            "Iteration:  24520      Current loss:  23.006683349609375      Current accuracy:  2.0      Comp_time of one batch:  -0.901085376739502\n",
            "Iteration:  24525      Current loss:  21.813308715820312      Current accuracy:  3.0      Comp_time of one batch:  -0.8796827793121338\n",
            "Iteration:  24530      Current loss:  25.044893264770508      Current accuracy:  3.0      Comp_time of one batch:  -0.896099328994751\n",
            "Iteration:  24535      Current loss:  24.67612648010254      Current accuracy:  4.0      Comp_time of one batch:  -0.8776078224182129\n",
            "Iteration:  24540      Current loss:  24.568986892700195      Current accuracy:  4.0      Comp_time of one batch:  -0.8956325054168701\n",
            "Iteration:  24545      Current loss:  21.053892135620117      Current accuracy:  5.0      Comp_time of one batch:  -0.880272388458252\n",
            "Iteration:  24550      Current loss:  20.79972267150879      Current accuracy:  6.0      Comp_time of one batch:  -0.9053926467895508\n",
            "Iteration:  24555      Current loss:  21.504419326782227      Current accuracy:  7.0      Comp_time of one batch:  -0.923886775970459\n",
            "Iteration:  24560      Current loss:  21.148395538330078      Current accuracy:  8.0      Comp_time of one batch:  -0.8849494457244873\n",
            "Iteration:  24565      Current loss:  19.566621780395508      Current accuracy:  9.0      Comp_time of one batch:  -0.9066755771636963\n",
            "Iteration:  24570      Current loss:  18.57255744934082      Current accuracy:  10.0      Comp_time of one batch:  -0.8934440612792969\n",
            "Iteration:  24575      Current loss:  29.59372329711914      Current accuracy:  10.0      Comp_time of one batch:  -0.8906300067901611\n",
            "Iteration:  24580      Current loss:  21.847681045532227      Current accuracy:  11.0      Comp_time of one batch:  -0.9016945362091064\n",
            "Iteration:  24585      Current loss:  32.37075424194336      Current accuracy:  11.0      Comp_time of one batch:  -0.8935959339141846\n",
            "Iteration:  24590      Current loss:  23.43642807006836      Current accuracy:  11.0      Comp_time of one batch:  -0.8954095840454102\n",
            "Iteration:  24595      Current loss:  29.581165313720703      Current accuracy:  11.0      Comp_time of one batch:  -0.8990058898925781\n",
            "Iteration:  24600      Current loss:  18.709922790527344      Current accuracy:  12.0      Comp_time of one batch:  -0.8819899559020996\n",
            "Iteration:  24605      Current loss:  24.403736114501953      Current accuracy:  12.0      Comp_time of one batch:  -0.913576602935791\n",
            "Iteration:  24610      Current loss:  24.685001373291016      Current accuracy:  12.0      Comp_time of one batch:  -0.9195444583892822\n",
            "Iteration:  24615      Current loss:  22.579914093017578      Current accuracy:  13.0      Comp_time of one batch:  -0.9033083915710449\n",
            "Iteration:  24620      Current loss:  43.543861389160156      Current accuracy:  13.0      Comp_time of one batch:  -0.8800334930419922\n",
            "Iteration:  24625      Current loss:  29.062204360961914      Current accuracy:  13.0      Comp_time of one batch:  -0.9103946685791016\n",
            "Iteration:  24630      Current loss:  20.120079040527344      Current accuracy:  14.0      Comp_time of one batch:  -0.9099202156066895\n",
            "Iteration:  24635      Current loss:  22.039569854736328      Current accuracy:  15.0      Comp_time of one batch:  -0.8926846981048584\n",
            "Iteration:  24640      Current loss:  26.642244338989258      Current accuracy:  15.0      Comp_time of one batch:  -0.8794326782226562\n",
            "Iteration:  24645      Current loss:  20.93742561340332      Current accuracy:  16.0      Comp_time of one batch:  -0.918471097946167\n",
            "Iteration:  24650      Current loss:  23.5502986907959      Current accuracy:  17.0      Comp_time of one batch:  -0.8807530403137207\n",
            "Iteration:  24655      Current loss:  19.815410614013672      Current accuracy:  18.0      Comp_time of one batch:  -0.8966405391693115\n",
            "Iteration:  24660      Current loss:  18.55332374572754      Current accuracy:  19.0      Comp_time of one batch:  -0.8967809677124023\n",
            "Iteration:  24665      Current loss:  24.936725616455078      Current accuracy:  19.0      Comp_time of one batch:  -0.8876791000366211\n",
            "Iteration:  24670      Current loss:  22.901491165161133      Current accuracy:  20.0      Comp_time of one batch:  -0.8767237663269043\n",
            "Iteration:  24675      Current loss:  28.199047088623047      Current accuracy:  20.0      Comp_time of one batch:  -0.9005186557769775\n",
            "Iteration:  24680      Current loss:  25.07784652709961      Current accuracy:  20.0      Comp_time of one batch:  -0.8793754577636719\n",
            "Iteration:  24685      Current loss:  28.542495727539062      Current accuracy:  20.0      Comp_time of one batch:  -0.8786156177520752\n",
            "Iteration:  24690      Current loss:  28.725807189941406      Current accuracy:  20.0      Comp_time of one batch:  -0.9150185585021973\n",
            "Iteration:  24695      Current loss:  27.2808837890625      Current accuracy:  20.0      Comp_time of one batch:  -0.8927698135375977\n",
            "Iteration:  24700      Current loss:  24.915128707885742      Current accuracy:  20.0      Comp_time of one batch:  -0.9036669731140137\n",
            "Iteration:  24705      Current loss:  31.058317184448242      Current accuracy:  20.0      Comp_time of one batch:  -0.8876714706420898\n",
            "Iteration:  24710      Current loss:  16.603090286254883      Current accuracy:  21.0      Comp_time of one batch:  -0.898728609085083\n",
            "Iteration:  24715      Current loss:  20.552976608276367      Current accuracy:  22.0      Comp_time of one batch:  -0.899287223815918\n",
            "Iteration:  24720      Current loss:  21.56208038330078      Current accuracy:  23.0      Comp_time of one batch:  -0.9122319221496582\n",
            "Iteration:  24725      Current loss:  20.405241012573242      Current accuracy:  24.0      Comp_time of one batch:  -0.8814985752105713\n",
            "Iteration:  24730      Current loss:  19.312681198120117      Current accuracy:  25.0      Comp_time of one batch:  -0.8932960033416748\n",
            "Iteration:  24735      Current loss:  27.714797973632812      Current accuracy:  25.0      Comp_time of one batch:  -0.9102437496185303\n",
            "Iteration:  24740      Current loss:  35.157257080078125      Current accuracy:  25.0      Comp_time of one batch:  -0.8875405788421631\n",
            "Iteration:  24745      Current loss:  24.39751625061035      Current accuracy:  25.0      Comp_time of one batch:  -0.8971223831176758\n",
            "Iteration:  24750      Current loss:  17.918642044067383      Current accuracy:  26.0      Comp_time of one batch:  -0.8873047828674316\n",
            "Iteration:  24755      Current loss:  24.98897933959961      Current accuracy:  26.0      Comp_time of one batch:  -0.9034030437469482\n",
            "Iteration:  24760      Current loss:  25.18441390991211      Current accuracy:  26.0      Comp_time of one batch:  -0.9161579608917236\n",
            "Iteration:  24765      Current loss:  22.75115203857422      Current accuracy:  27.0      Comp_time of one batch:  -0.9274337291717529\n",
            "Iteration:  24770      Current loss:  24.56312370300293      Current accuracy:  27.0      Comp_time of one batch:  -0.9323406219482422\n",
            "Iteration:  24775      Current loss:  30.776487350463867      Current accuracy:  27.0      Comp_time of one batch:  -0.9137530326843262\n",
            "Iteration:  24780      Current loss:  34.869815826416016      Current accuracy:  27.0      Comp_time of one batch:  -0.8930730819702148\n",
            "Iteration:  24785      Current loss:  22.373762130737305      Current accuracy:  28.0      Comp_time of one batch:  -0.9164245128631592\n",
            "Iteration:  24790      Current loss:  25.072402954101562      Current accuracy:  28.0      Comp_time of one batch:  -0.9190471172332764\n",
            "Iteration:  24795      Current loss:  18.267576217651367      Current accuracy:  29.0      Comp_time of one batch:  -0.8735060691833496\n",
            "Iteration:  24800      Current loss:  21.9977970123291      Current accuracy:  30.0      Comp_time of one batch:  -0.8988378047943115\n",
            "Iteration:  24805      Current loss:  27.975433349609375      Current accuracy:  30.0      Comp_time of one batch:  -0.9009442329406738\n",
            "Iteration:  24810      Current loss:  21.39918327331543      Current accuracy:  31.0      Comp_time of one batch:  -0.9005815982818604\n",
            "Iteration:  24815      Current loss:  19.561349868774414      Current accuracy:  32.0      Comp_time of one batch:  -0.9011664390563965\n",
            "Iteration:  24820      Current loss:  20.277009963989258      Current accuracy:  33.0      Comp_time of one batch:  -0.9117803573608398\n",
            "Iteration:  24825      Current loss:  27.1959228515625      Current accuracy:  33.0      Comp_time of one batch:  -0.8962607383728027\n",
            "Iteration:  24830      Current loss:  29.508197784423828      Current accuracy:  33.0      Comp_time of one batch:  -0.9030478000640869\n",
            "Iteration:  24835      Current loss:  16.669353485107422      Current accuracy:  34.0      Comp_time of one batch:  -0.8774023056030273\n",
            "Iteration:  24840      Current loss:  40.83685302734375      Current accuracy:  34.0      Comp_time of one batch:  -0.8865935802459717\n",
            "Iteration:  24845      Current loss:  22.233903884887695      Current accuracy:  35.0      Comp_time of one batch:  -0.8718156814575195\n",
            "Iteration:  24850      Current loss:  20.19764518737793      Current accuracy:  36.0      Comp_time of one batch:  -0.9100005626678467\n",
            "Iteration:  24855      Current loss:  18.359249114990234      Current accuracy:  37.0      Comp_time of one batch:  -0.8812248706817627\n",
            "Iteration:  24860      Current loss:  19.566158294677734      Current accuracy:  38.0      Comp_time of one batch:  -0.9108998775482178\n",
            "Iteration:  24865      Current loss:  26.496421813964844      Current accuracy:  38.0      Comp_time of one batch:  -0.8874907493591309\n",
            "Iteration:  24870      Current loss:  19.93853187561035      Current accuracy:  39.0      Comp_time of one batch:  -0.8859772682189941\n",
            "Iteration:  24875      Current loss:  21.184125900268555      Current accuracy:  40.0      Comp_time of one batch:  -0.8794534206390381\n",
            "Iteration:  24880      Current loss:  16.636402130126953      Current accuracy:  41.0      Comp_time of one batch:  -0.8817451000213623\n",
            "Iteration:  24885      Current loss:  18.919336318969727      Current accuracy:  42.0      Comp_time of one batch:  -0.8857100009918213\n",
            "Iteration:  24890      Current loss:  25.0147647857666      Current accuracy:  43.0      Comp_time of one batch:  -0.9156508445739746\n",
            "Iteration:  24895      Current loss:  22.909902572631836      Current accuracy:  44.0      Comp_time of one batch:  -0.9090468883514404\n",
            "Iteration:  24900      Current loss:  23.25818634033203      Current accuracy:  45.0      Comp_time of one batch:  -0.8985755443572998\n",
            "Iteration:  24905      Current loss:  25.428543090820312      Current accuracy:  45.0      Comp_time of one batch:  -0.9043838977813721\n",
            "Iteration:  24910      Current loss:  22.134971618652344      Current accuracy:  46.0      Comp_time of one batch:  -0.8849503993988037\n",
            "Iteration:  24915      Current loss:  17.225055694580078      Current accuracy:  47.0      Comp_time of one batch:  -0.8884057998657227\n",
            "Iteration:  24920      Current loss:  24.22231101989746      Current accuracy:  48.0      Comp_time of one batch:  -0.8819975852966309\n",
            "Iteration:  24925      Current loss:  25.266647338867188      Current accuracy:  48.0      Comp_time of one batch:  -0.879671573638916\n",
            "Iteration:  24930      Current loss:  22.455486297607422      Current accuracy:  49.0      Comp_time of one batch:  -0.891787052154541\n",
            "Iteration:  24935      Current loss:  22.82060432434082      Current accuracy:  49.0      Comp_time of one batch:  -0.8920502662658691\n",
            "Iteration:  24940      Current loss:  24.070253372192383      Current accuracy:  49.0      Comp_time of one batch:  -0.8775689601898193\n",
            "Iteration:  24945      Current loss:  17.193710327148438      Current accuracy:  50.0      Comp_time of one batch:  -0.9027848243713379\n",
            "Iteration:  24950      Current loss:  21.752931594848633      Current accuracy:  51.0      Comp_time of one batch:  -0.8871293067932129\n",
            "Iteration:  24955      Current loss:  28.877870559692383      Current accuracy:  51.0      Comp_time of one batch:  -0.9091238975524902\n",
            "Iteration:  24960      Current loss:  23.754655838012695      Current accuracy:  52.0      Comp_time of one batch:  -0.8899812698364258\n",
            "Iteration:  24965      Current loss:  22.88307762145996      Current accuracy:  53.0      Comp_time of one batch:  -0.9016313552856445\n",
            "Iteration:  24970      Current loss:  22.015913009643555      Current accuracy:  54.0      Comp_time of one batch:  -0.8936727046966553\n",
            "Iteration:  24975      Current loss:  19.268234252929688      Current accuracy:  55.0      Comp_time of one batch:  -0.9011259078979492\n",
            "Iteration:  24980      Current loss:  27.756526947021484      Current accuracy:  55.0      Comp_time of one batch:  -0.8773510456085205\n",
            "Iteration:  24985      Current loss:  34.33164978027344      Current accuracy:  55.0      Comp_time of one batch:  -0.8731784820556641\n",
            "Iteration:  24990      Current loss:  29.835899353027344      Current accuracy:  55.0      Comp_time of one batch:  -0.8831624984741211\n",
            "Iteration:  24995      Current loss:  24.307462692260742      Current accuracy:  55.0      Comp_time of one batch:  -0.8883528709411621\n",
            "Iteration:  25000      Current loss:  25.557373046875      Current accuracy:  0.0      Comp_time of one batch:  -0.9044890403747559\n",
            "Iteration:  25005      Current loss:  20.72096824645996      Current accuracy:  1.0      Comp_time of one batch:  -0.8843960762023926\n",
            "Iteration:  25010      Current loss:  22.744293212890625      Current accuracy:  1.0      Comp_time of one batch:  -0.9119083881378174\n",
            "Iteration:  25015      Current loss:  34.60780334472656      Current accuracy:  1.0      Comp_time of one batch:  -0.8772575855255127\n",
            "Iteration:  25020      Current loss:  31.05073356628418      Current accuracy:  1.0      Comp_time of one batch:  -0.8936004638671875\n",
            "Iteration:  25025      Current loss:  24.86383628845215      Current accuracy:  1.0      Comp_time of one batch:  -0.8829679489135742\n",
            "Iteration:  25030      Current loss:  21.462068557739258      Current accuracy:  2.0      Comp_time of one batch:  -0.90144944190979\n",
            "Iteration:  25035      Current loss:  16.345966339111328      Current accuracy:  3.0      Comp_time of one batch:  -0.9199819564819336\n",
            "Iteration:  25040      Current loss:  24.307119369506836      Current accuracy:  3.0      Comp_time of one batch:  -0.8891420364379883\n",
            "Iteration:  25045      Current loss:  31.80022621154785      Current accuracy:  3.0      Comp_time of one batch:  -0.9164125919342041\n",
            "Iteration:  25050      Current loss:  23.82343292236328      Current accuracy:  3.0      Comp_time of one batch:  -0.9048483371734619\n",
            "Iteration:  25055      Current loss:  39.79884719848633      Current accuracy:  3.0      Comp_time of one batch:  -0.8973250389099121\n",
            "Iteration:  25060      Current loss:  26.06197738647461      Current accuracy:  3.0      Comp_time of one batch:  -0.8837676048278809\n",
            "Iteration:  25065      Current loss:  25.444486618041992      Current accuracy:  3.0      Comp_time of one batch:  -0.8965747356414795\n",
            "Iteration:  25070      Current loss:  19.19199562072754      Current accuracy:  4.0      Comp_time of one batch:  -0.8830254077911377\n",
            "Iteration:  25075      Current loss:  22.07451057434082      Current accuracy:  5.0      Comp_time of one batch:  -0.8829154968261719\n",
            "Iteration:  25080      Current loss:  18.38552474975586      Current accuracy:  6.0      Comp_time of one batch:  -0.8985531330108643\n",
            "Iteration:  25085      Current loss:  23.982940673828125      Current accuracy:  7.0      Comp_time of one batch:  -0.8913061618804932\n",
            "Iteration:  25090      Current loss:  21.127370834350586      Current accuracy:  8.0      Comp_time of one batch:  -0.8841109275817871\n",
            "Iteration:  25095      Current loss:  23.12966537475586      Current accuracy:  9.0      Comp_time of one batch:  -0.8774397373199463\n",
            "Iteration:  25100      Current loss:  19.738954544067383      Current accuracy:  10.0      Comp_time of one batch:  -0.8897781372070312\n",
            "Iteration:  25105      Current loss:  22.079362869262695      Current accuracy:  11.0      Comp_time of one batch:  -0.8819332122802734\n",
            "Iteration:  25110      Current loss:  32.47151565551758      Current accuracy:  11.0      Comp_time of one batch:  -0.8828938007354736\n",
            "Iteration:  25115      Current loss:  23.749664306640625      Current accuracy:  12.0      Comp_time of one batch:  -0.8926680088043213\n",
            "Iteration:  25120      Current loss:  29.28623390197754      Current accuracy:  12.0      Comp_time of one batch:  -0.9238712787628174\n",
            "Iteration:  25125      Current loss:  24.684322357177734      Current accuracy:  12.0      Comp_time of one batch:  -0.913421630859375\n",
            "Iteration:  25130      Current loss:  32.8182487487793      Current accuracy:  12.0      Comp_time of one batch:  -0.9012632369995117\n",
            "Iteration:  25135      Current loss:  20.871671676635742      Current accuracy:  13.0      Comp_time of one batch:  -0.887488842010498\n",
            "Iteration:  25140      Current loss:  27.18998146057129      Current accuracy:  13.0      Comp_time of one batch:  -0.897463321685791\n",
            "Iteration:  25145      Current loss:  28.471181869506836      Current accuracy:  13.0      Comp_time of one batch:  -0.906003475189209\n",
            "Iteration:  25150      Current loss:  21.445547103881836      Current accuracy:  14.0      Comp_time of one batch:  -0.9169735908508301\n",
            "Iteration:  25155      Current loss:  20.480194091796875      Current accuracy:  15.0      Comp_time of one batch:  -0.9416534900665283\n",
            "Iteration:  25160      Current loss:  29.581119537353516      Current accuracy:  15.0      Comp_time of one batch:  -0.8856899738311768\n",
            "Iteration:  25165      Current loss:  30.950956344604492      Current accuracy:  15.0      Comp_time of one batch:  -0.8860056400299072\n",
            "Iteration:  25170      Current loss:  17.395530700683594      Current accuracy:  16.0      Comp_time of one batch:  -0.9041574001312256\n",
            "Iteration:  25175      Current loss:  22.12472152709961      Current accuracy:  17.0      Comp_time of one batch:  -0.8911905288696289\n",
            "Iteration:  25180      Current loss:  26.743934631347656      Current accuracy:  17.0      Comp_time of one batch:  -0.8800289630889893\n",
            "Iteration:  25185      Current loss:  19.97959327697754      Current accuracy:  18.0      Comp_time of one batch:  -0.9143235683441162\n",
            "Iteration:  25190      Current loss:  32.552696228027344      Current accuracy:  18.0      Comp_time of one batch:  -0.8729124069213867\n",
            "Iteration:  25195      Current loss:  33.321067810058594      Current accuracy:  18.0      Comp_time of one batch:  -0.8970675468444824\n",
            "Iteration:  25200      Current loss:  22.14636993408203      Current accuracy:  19.0      Comp_time of one batch:  -0.904144287109375\n",
            "Iteration:  25205      Current loss:  22.494401931762695      Current accuracy:  20.0      Comp_time of one batch:  -0.9066641330718994\n",
            "Iteration:  25210      Current loss:  28.83527374267578      Current accuracy:  20.0      Comp_time of one batch:  -0.8902018070220947\n",
            "Iteration:  25215      Current loss:  20.299724578857422      Current accuracy:  21.0      Comp_time of one batch:  -0.8969836235046387\n",
            "Iteration:  25220      Current loss:  37.15648651123047      Current accuracy:  21.0      Comp_time of one batch:  -0.8764994144439697\n",
            "Iteration:  25225      Current loss:  23.40839195251465      Current accuracy:  22.0      Comp_time of one batch:  -0.8837506771087646\n",
            "Iteration:  25230      Current loss:  20.147396087646484      Current accuracy:  23.0      Comp_time of one batch:  -0.8713116645812988\n",
            "Iteration:  25235      Current loss:  21.823827743530273      Current accuracy:  24.0      Comp_time of one batch:  -0.8816182613372803\n",
            "Iteration:  25240      Current loss:  29.939777374267578      Current accuracy:  24.0      Comp_time of one batch:  -0.9034457206726074\n",
            "Iteration:  25245      Current loss:  28.929527282714844      Current accuracy:  24.0      Comp_time of one batch:  -0.9052069187164307\n",
            "Iteration:  25250      Current loss:  26.351024627685547      Current accuracy:  24.0      Comp_time of one batch:  -0.877561092376709\n",
            "Iteration:  25255      Current loss:  25.947498321533203      Current accuracy:  24.0      Comp_time of one batch:  -0.8955397605895996\n",
            "Iteration:  25260      Current loss:  20.28583335876465      Current accuracy:  25.0      Comp_time of one batch:  -0.8835659027099609\n",
            "Iteration:  25265      Current loss:  24.334165573120117      Current accuracy:  26.0      Comp_time of one batch:  -0.9180562496185303\n",
            "Iteration:  25270      Current loss:  28.888879776000977      Current accuracy:  26.0      Comp_time of one batch:  -0.8920938968658447\n",
            "Iteration:  25275      Current loss:  27.36323356628418      Current accuracy:  26.0      Comp_time of one batch:  -0.9321327209472656\n",
            "Iteration:  25280      Current loss:  19.75904083251953      Current accuracy:  27.0      Comp_time of one batch:  -0.8917689323425293\n",
            "Iteration:  25285      Current loss:  29.476285934448242      Current accuracy:  27.0      Comp_time of one batch:  -0.9000957012176514\n",
            "Iteration:  25290      Current loss:  20.30661964416504      Current accuracy:  28.0      Comp_time of one batch:  -0.8867843151092529\n",
            "Iteration:  25295      Current loss:  31.63510513305664      Current accuracy:  28.0      Comp_time of one batch:  -0.8696615695953369\n",
            "Iteration:  25300      Current loss:  19.784452438354492      Current accuracy:  29.0      Comp_time of one batch:  -0.882082462310791\n",
            "Iteration:  25305      Current loss:  23.490083694458008      Current accuracy:  30.0      Comp_time of one batch:  -0.9094095230102539\n",
            "Iteration:  25310      Current loss:  23.31905174255371      Current accuracy:  30.0      Comp_time of one batch:  -0.9049806594848633\n",
            "Iteration:  25315      Current loss:  21.606958389282227      Current accuracy:  31.0      Comp_time of one batch:  -0.8893871307373047\n",
            "Iteration:  25320      Current loss:  25.821794509887695      Current accuracy:  31.0      Comp_time of one batch:  -0.9127392768859863\n",
            "Iteration:  25325      Current loss:  19.62620735168457      Current accuracy:  32.0      Comp_time of one batch:  -0.8831746578216553\n",
            "Iteration:  25330      Current loss:  25.98906898498535      Current accuracy:  32.0      Comp_time of one batch:  -0.9045851230621338\n",
            "Iteration:  25335      Current loss:  25.61774253845215      Current accuracy:  32.0      Comp_time of one batch:  -0.8962631225585938\n",
            "Iteration:  25340      Current loss:  30.369781494140625      Current accuracy:  32.0      Comp_time of one batch:  -0.8859004974365234\n",
            "Iteration:  25345      Current loss:  19.926376342773438      Current accuracy:  33.0      Comp_time of one batch:  -0.8907773494720459\n",
            "Iteration:  25350      Current loss:  17.434255599975586      Current accuracy:  34.0      Comp_time of one batch:  -0.884376049041748\n",
            "Iteration:  25355      Current loss:  30.834049224853516      Current accuracy:  34.0      Comp_time of one batch:  -0.8702857494354248\n",
            "Iteration:  25360      Current loss:  16.2260799407959      Current accuracy:  35.0      Comp_time of one batch:  -0.8970420360565186\n",
            "Iteration:  25365      Current loss:  23.534467697143555      Current accuracy:  35.0      Comp_time of one batch:  -0.8772046566009521\n",
            "Iteration:  25370      Current loss:  31.3351993560791      Current accuracy:  35.0      Comp_time of one batch:  -0.8814411163330078\n",
            "Iteration:  25375      Current loss:  29.846538543701172      Current accuracy:  35.0      Comp_time of one batch:  -0.9078302383422852\n",
            "Iteration:  25380      Current loss:  18.814598083496094      Current accuracy:  36.0      Comp_time of one batch:  -0.8858752250671387\n",
            "Iteration:  25385      Current loss:  17.25554847717285      Current accuracy:  37.0      Comp_time of one batch:  -0.871539831161499\n",
            "Iteration:  25390      Current loss:  31.981874465942383      Current accuracy:  37.0      Comp_time of one batch:  -0.8864164352416992\n",
            "Iteration:  25395      Current loss:  20.575469970703125      Current accuracy:  38.0      Comp_time of one batch:  -0.8773150444030762\n",
            "Iteration:  25400      Current loss:  26.968488693237305      Current accuracy:  38.0      Comp_time of one batch:  -0.9040019512176514\n",
            "Iteration:  25405      Current loss:  26.06754493713379      Current accuracy:  38.0      Comp_time of one batch:  -0.9042208194732666\n",
            "Iteration:  25410      Current loss:  27.747583389282227      Current accuracy:  38.0      Comp_time of one batch:  -0.900327205657959\n",
            "Iteration:  25415      Current loss:  22.001873016357422      Current accuracy:  39.0      Comp_time of one batch:  -0.9010500907897949\n",
            "Iteration:  25420      Current loss:  23.110769271850586      Current accuracy:  39.0      Comp_time of one batch:  -0.9133090972900391\n",
            "Iteration:  25425      Current loss:  24.092140197753906      Current accuracy:  39.0      Comp_time of one batch:  -0.8883142471313477\n",
            "Iteration:  25430      Current loss:  19.506263732910156      Current accuracy:  40.0      Comp_time of one batch:  -0.904017448425293\n",
            "Iteration:  25435      Current loss:  23.991968154907227      Current accuracy:  41.0      Comp_time of one batch:  -0.9142293930053711\n",
            "Iteration:  25440      Current loss:  29.41676902770996      Current accuracy:  41.0      Comp_time of one batch:  -0.9249885082244873\n",
            "Iteration:  25445      Current loss:  20.74418830871582      Current accuracy:  42.0      Comp_time of one batch:  -0.8833410739898682\n",
            "Iteration:  25450      Current loss:  21.136552810668945      Current accuracy:  43.0      Comp_time of one batch:  -0.894364595413208\n",
            "Iteration:  25455      Current loss:  32.50090408325195      Current accuracy:  43.0      Comp_time of one batch:  -0.8821210861206055\n",
            "Iteration:  25460      Current loss:  18.142560958862305      Current accuracy:  44.0      Comp_time of one batch:  -0.89133620262146\n",
            "Iteration:  25465      Current loss:  19.777231216430664      Current accuracy:  45.0      Comp_time of one batch:  -0.9121818542480469\n",
            "Iteration:  25470      Current loss:  21.44186019897461      Current accuracy:  46.0      Comp_time of one batch:  -0.8971717357635498\n",
            "Iteration:  25475      Current loss:  20.714309692382812      Current accuracy:  47.0      Comp_time of one batch:  -0.8884673118591309\n",
            "Iteration:  25480      Current loss:  24.032190322875977      Current accuracy:  47.0      Comp_time of one batch:  -0.8901457786560059\n",
            "Iteration:  25485      Current loss:  26.41481590270996      Current accuracy:  47.0      Comp_time of one batch:  -0.8932147026062012\n",
            "Iteration:  25490      Current loss:  21.283493041992188      Current accuracy:  48.0      Comp_time of one batch:  -0.9205427169799805\n",
            "Iteration:  25495      Current loss:  21.857866287231445      Current accuracy:  49.0      Comp_time of one batch:  -0.9187846183776855\n",
            "Iteration:  25500      Current loss:  21.859458923339844      Current accuracy:  0.0      Comp_time of one batch:  -0.9184346199035645\n",
            "Iteration:  25505      Current loss:  20.317041397094727      Current accuracy:  1.0      Comp_time of one batch:  -0.8792610168457031\n",
            "Iteration:  25510      Current loss:  19.515602111816406      Current accuracy:  2.0      Comp_time of one batch:  -0.9210927486419678\n",
            "Iteration:  25515      Current loss:  22.595050811767578      Current accuracy:  3.0      Comp_time of one batch:  -0.88883376121521\n",
            "Iteration:  25520      Current loss:  15.794583320617676      Current accuracy:  4.0      Comp_time of one batch:  -0.9027533531188965\n",
            "Iteration:  25525      Current loss:  25.225839614868164      Current accuracy:  4.0      Comp_time of one batch:  -0.8921325206756592\n",
            "Iteration:  25530      Current loss:  22.97064971923828      Current accuracy:  5.0      Comp_time of one batch:  -0.8998763561248779\n",
            "Iteration:  25535      Current loss:  19.969186782836914      Current accuracy:  6.0      Comp_time of one batch:  -0.8889782428741455\n",
            "Iteration:  25540      Current loss:  18.241397857666016      Current accuracy:  7.0      Comp_time of one batch:  -0.8986973762512207\n",
            "Iteration:  25545      Current loss:  31.54666519165039      Current accuracy:  7.0      Comp_time of one batch:  -0.8718311786651611\n",
            "Iteration:  25550      Current loss:  17.172618865966797      Current accuracy:  8.0      Comp_time of one batch:  -0.9103498458862305\n",
            "Iteration:  25555      Current loss:  26.695919036865234      Current accuracy:  8.0      Comp_time of one batch:  -0.9004933834075928\n",
            "Iteration:  25560      Current loss:  21.6247615814209      Current accuracy:  9.0      Comp_time of one batch:  -0.8687357902526855\n",
            "Iteration:  25565      Current loss:  27.71983528137207      Current accuracy:  9.0      Comp_time of one batch:  -0.9435842037200928\n",
            "Iteration:  25570      Current loss:  25.22990608215332      Current accuracy:  9.0      Comp_time of one batch:  -0.9109282493591309\n",
            "Iteration:  25575      Current loss:  17.25757598876953      Current accuracy:  10.0      Comp_time of one batch:  -0.8853273391723633\n",
            "Iteration:  25580      Current loss:  20.5491886138916      Current accuracy:  11.0      Comp_time of one batch:  -0.8894691467285156\n",
            "Iteration:  25585      Current loss:  33.858768463134766      Current accuracy:  11.0      Comp_time of one batch:  -0.9048497676849365\n",
            "Iteration:  25590      Current loss:  24.644210815429688      Current accuracy:  11.0      Comp_time of one batch:  -0.8806016445159912\n",
            "Iteration:  25595      Current loss:  32.0791130065918      Current accuracy:  11.0      Comp_time of one batch:  -0.9157941341400146\n",
            "Iteration:  25600      Current loss:  25.10628890991211      Current accuracy:  11.0      Comp_time of one batch:  -0.8941340446472168\n",
            "Iteration:  25605      Current loss:  23.432662963867188      Current accuracy:  11.0      Comp_time of one batch:  -0.9086503982543945\n",
            "Iteration:  25610      Current loss:  25.10547637939453      Current accuracy:  11.0      Comp_time of one batch:  -0.8821926116943359\n",
            "Iteration:  25615      Current loss:  32.92790985107422      Current accuracy:  11.0      Comp_time of one batch:  -0.8978312015533447\n",
            "Iteration:  25620      Current loss:  27.683090209960938      Current accuracy:  11.0      Comp_time of one batch:  -0.9217262268066406\n",
            "Iteration:  25625      Current loss:  26.309494018554688      Current accuracy:  11.0      Comp_time of one batch:  -0.8975846767425537\n",
            "Iteration:  25630      Current loss:  28.354578018188477      Current accuracy:  11.0      Comp_time of one batch:  -0.8926620483398438\n",
            "Iteration:  25635      Current loss:  27.726289749145508      Current accuracy:  11.0      Comp_time of one batch:  -0.8909869194030762\n",
            "Iteration:  25640      Current loss:  21.433958053588867      Current accuracy:  12.0      Comp_time of one batch:  -0.8867311477661133\n",
            "Iteration:  25645      Current loss:  22.762657165527344      Current accuracy:  13.0      Comp_time of one batch:  -0.9110763072967529\n",
            "Iteration:  25650      Current loss:  16.4397029876709      Current accuracy:  14.0      Comp_time of one batch:  -0.887554407119751\n",
            "Iteration:  25655      Current loss:  38.053192138671875      Current accuracy:  14.0      Comp_time of one batch:  -0.9115478992462158\n",
            "Iteration:  25660      Current loss:  24.518657684326172      Current accuracy:  14.0      Comp_time of one batch:  -0.8700246810913086\n",
            "Iteration:  25665      Current loss:  23.61514663696289      Current accuracy:  14.0      Comp_time of one batch:  -0.9056334495544434\n",
            "Iteration:  25670      Current loss:  20.822473526000977      Current accuracy:  15.0      Comp_time of one batch:  -0.891167402267456\n",
            "Iteration:  25675      Current loss:  27.30011558532715      Current accuracy:  15.0      Comp_time of one batch:  -0.9046032428741455\n",
            "Iteration:  25680      Current loss:  27.15220832824707      Current accuracy:  15.0      Comp_time of one batch:  -0.9252214431762695\n",
            "Iteration:  25685      Current loss:  25.338987350463867      Current accuracy:  15.0      Comp_time of one batch:  -0.9226911067962646\n",
            "Iteration:  25690      Current loss:  26.694690704345703      Current accuracy:  15.0      Comp_time of one batch:  -0.9377713203430176\n",
            "Iteration:  25695      Current loss:  22.905231475830078      Current accuracy:  15.0      Comp_time of one batch:  -0.9102377891540527\n",
            "Iteration:  25700      Current loss:  19.9531307220459      Current accuracy:  16.0      Comp_time of one batch:  -0.8869748115539551\n",
            "Iteration:  25705      Current loss:  27.419559478759766      Current accuracy:  16.0      Comp_time of one batch:  -0.8807773590087891\n",
            "Iteration:  25710      Current loss:  32.123538970947266      Current accuracy:  16.0      Comp_time of one batch:  -0.9049053192138672\n",
            "Iteration:  25715      Current loss:  25.640705108642578      Current accuracy:  16.0      Comp_time of one batch:  -0.9020264148712158\n",
            "Iteration:  25720      Current loss:  24.45037078857422      Current accuracy:  16.0      Comp_time of one batch:  -0.8772580623626709\n",
            "Iteration:  25725      Current loss:  21.663759231567383      Current accuracy:  17.0      Comp_time of one batch:  -0.8991274833679199\n",
            "Iteration:  25730      Current loss:  22.780414581298828      Current accuracy:  18.0      Comp_time of one batch:  -0.9023029804229736\n",
            "Iteration:  25735      Current loss:  29.032997131347656      Current accuracy:  18.0      Comp_time of one batch:  -0.8742802143096924\n",
            "Iteration:  25740      Current loss:  22.974445343017578      Current accuracy:  19.0      Comp_time of one batch:  -0.9075186252593994\n",
            "Iteration:  25745      Current loss:  26.672382354736328      Current accuracy:  19.0      Comp_time of one batch:  -0.91251540184021\n",
            "Iteration:  25750      Current loss:  29.278549194335938      Current accuracy:  19.0      Comp_time of one batch:  -0.872758150100708\n",
            "Iteration:  25755      Current loss:  17.204397201538086      Current accuracy:  20.0      Comp_time of one batch:  -0.8869872093200684\n",
            "Iteration:  25760      Current loss:  23.122875213623047      Current accuracy:  20.0      Comp_time of one batch:  -0.8869943618774414\n",
            "Iteration:  25765      Current loss:  26.030698776245117      Current accuracy:  20.0      Comp_time of one batch:  -0.8663301467895508\n",
            "Iteration:  25770      Current loss:  18.602542877197266      Current accuracy:  21.0      Comp_time of one batch:  -0.9052879810333252\n",
            "Iteration:  25775      Current loss:  21.50825309753418      Current accuracy:  22.0      Comp_time of one batch:  -0.8721904754638672\n",
            "Iteration:  25780      Current loss:  19.324359893798828      Current accuracy:  23.0      Comp_time of one batch:  -0.8824894428253174\n",
            "Iteration:  25785      Current loss:  24.301822662353516      Current accuracy:  23.0      Comp_time of one batch:  -0.8813369274139404\n",
            "Iteration:  25790      Current loss:  20.29218864440918      Current accuracy:  24.0      Comp_time of one batch:  -0.8830888271331787\n",
            "Iteration:  25795      Current loss:  24.353225708007812      Current accuracy:  25.0      Comp_time of one batch:  -0.8916373252868652\n",
            "Iteration:  25800      Current loss:  24.809377670288086      Current accuracy:  25.0      Comp_time of one batch:  -0.8994243144989014\n",
            "Iteration:  25805      Current loss:  20.469083786010742      Current accuracy:  26.0      Comp_time of one batch:  -0.8918919563293457\n",
            "Iteration:  25810      Current loss:  24.431594848632812      Current accuracy:  26.0      Comp_time of one batch:  -0.9087162017822266\n",
            "Iteration:  25815      Current loss:  21.993501663208008      Current accuracy:  27.0      Comp_time of one batch:  -0.909221887588501\n",
            "Iteration:  25820      Current loss:  21.924755096435547      Current accuracy:  28.0      Comp_time of one batch:  -0.8876874446868896\n",
            "Iteration:  25825      Current loss:  26.42035484313965      Current accuracy:  28.0      Comp_time of one batch:  -0.8836715221405029\n",
            "Iteration:  25830      Current loss:  22.95595359802246      Current accuracy:  29.0      Comp_time of one batch:  -0.8923206329345703\n",
            "Iteration:  25835      Current loss:  20.666759490966797      Current accuracy:  30.0      Comp_time of one batch:  -0.9013011455535889\n",
            "Iteration:  25840      Current loss:  27.007301330566406      Current accuracy:  30.0      Comp_time of one batch:  -0.9167056083679199\n",
            "Iteration:  25845      Current loss:  26.9974308013916      Current accuracy:  30.0      Comp_time of one batch:  -0.8937747478485107\n",
            "Iteration:  25850      Current loss:  16.32836151123047      Current accuracy:  31.0      Comp_time of one batch:  -0.8828823566436768\n",
            "Iteration:  25855      Current loss:  20.897245407104492      Current accuracy:  32.0      Comp_time of one batch:  -0.9092550277709961\n",
            "Iteration:  25860      Current loss:  27.70486831665039      Current accuracy:  32.0      Comp_time of one batch:  -0.874675989151001\n",
            "Iteration:  25865      Current loss:  16.231481552124023      Current accuracy:  33.0      Comp_time of one batch:  -0.9244139194488525\n",
            "Iteration:  25870      Current loss:  34.69196319580078      Current accuracy:  33.0      Comp_time of one batch:  -0.8845527172088623\n",
            "Iteration:  25875      Current loss:  18.99910545349121      Current accuracy:  34.0      Comp_time of one batch:  -0.8780272006988525\n",
            "Iteration:  25880      Current loss:  26.061050415039062      Current accuracy:  34.0      Comp_time of one batch:  -0.9076833724975586\n",
            "Iteration:  25885      Current loss:  30.695941925048828      Current accuracy:  34.0      Comp_time of one batch:  -0.9166560173034668\n",
            "Iteration:  25890      Current loss:  23.449434280395508      Current accuracy:  34.0      Comp_time of one batch:  -0.8798925876617432\n",
            "Iteration:  25895      Current loss:  25.160188674926758      Current accuracy:  35.0      Comp_time of one batch:  -0.8934967517852783\n",
            "Iteration:  25900      Current loss:  18.79279899597168      Current accuracy:  36.0      Comp_time of one batch:  -0.8767910003662109\n",
            "Iteration:  25905      Current loss:  20.687122344970703      Current accuracy:  37.0      Comp_time of one batch:  -0.9224388599395752\n",
            "Iteration:  25910      Current loss:  26.662870407104492      Current accuracy:  37.0      Comp_time of one batch:  -0.8764855861663818\n",
            "Iteration:  25915      Current loss:  25.02420425415039      Current accuracy:  37.0      Comp_time of one batch:  -0.8771770000457764\n",
            "Iteration:  25920      Current loss:  20.925413131713867      Current accuracy:  38.0      Comp_time of one batch:  -0.904200553894043\n",
            "Iteration:  25925      Current loss:  29.776851654052734      Current accuracy:  38.0      Comp_time of one batch:  -0.8983261585235596\n",
            "Iteration:  25930      Current loss:  23.314416885375977      Current accuracy:  39.0      Comp_time of one batch:  -0.8997836112976074\n",
            "Iteration:  25935      Current loss:  27.58845329284668      Current accuracy:  39.0      Comp_time of one batch:  -0.8827524185180664\n",
            "Iteration:  25940      Current loss:  37.96470260620117      Current accuracy:  39.0      Comp_time of one batch:  -0.9004402160644531\n",
            "Iteration:  25945      Current loss:  19.654884338378906      Current accuracy:  40.0      Comp_time of one batch:  -0.8769629001617432\n",
            "Iteration:  25950      Current loss:  20.53788185119629      Current accuracy:  41.0      Comp_time of one batch:  -0.8985626697540283\n",
            "Iteration:  25955      Current loss:  24.750219345092773      Current accuracy:  41.0      Comp_time of one batch:  -0.8997082710266113\n",
            "Iteration:  25960      Current loss:  28.2091121673584      Current accuracy:  41.0      Comp_time of one batch:  -0.8926231861114502\n",
            "Iteration:  25965      Current loss:  20.391231536865234      Current accuracy:  42.0      Comp_time of one batch:  -0.8831026554107666\n",
            "Iteration:  25970      Current loss:  19.432273864746094      Current accuracy:  43.0      Comp_time of one batch:  -0.8790473937988281\n",
            "Iteration:  25975      Current loss:  19.20697021484375      Current accuracy:  44.0      Comp_time of one batch:  -0.9030783176422119\n",
            "Iteration:  25980      Current loss:  21.420278549194336      Current accuracy:  45.0      Comp_time of one batch:  -0.8878624439239502\n",
            "Iteration:  25985      Current loss:  23.609411239624023      Current accuracy:  45.0      Comp_time of one batch:  -0.9220743179321289\n",
            "Iteration:  25990      Current loss:  27.337751388549805      Current accuracy:  46.0      Comp_time of one batch:  -0.8879506587982178\n",
            "Iteration:  25995      Current loss:  18.821603775024414      Current accuracy:  47.0      Comp_time of one batch:  -0.9111480712890625\n",
            "Iteration:  26000      Current loss:  23.861356735229492      Current accuracy:  0.0      Comp_time of one batch:  -0.8952786922454834\n",
            "Iteration:  26005      Current loss:  21.083158493041992      Current accuracy:  1.0      Comp_time of one batch:  -0.8805074691772461\n",
            "Iteration:  26010      Current loss:  36.816688537597656      Current accuracy:  1.0      Comp_time of one batch:  -0.9034802913665771\n",
            "Iteration:  26015      Current loss:  24.49709701538086      Current accuracy:  1.0      Comp_time of one batch:  -0.9047136306762695\n",
            "Iteration:  26020      Current loss:  26.954511642456055      Current accuracy:  1.0      Comp_time of one batch:  -0.8823018074035645\n",
            "Iteration:  26025      Current loss:  29.341150283813477      Current accuracy:  1.0      Comp_time of one batch:  -0.8970479965209961\n",
            "Iteration:  26030      Current loss:  35.67216873168945      Current accuracy:  1.0      Comp_time of one batch:  -0.9178709983825684\n",
            "Iteration:  26035      Current loss:  22.660173416137695      Current accuracy:  2.0      Comp_time of one batch:  -0.8803756237030029\n",
            "Iteration:  26040      Current loss:  17.737817764282227      Current accuracy:  3.0      Comp_time of one batch:  -0.9254293441772461\n",
            "Iteration:  26045      Current loss:  26.077293395996094      Current accuracy:  3.0      Comp_time of one batch:  -0.8696777820587158\n",
            "Iteration:  26050      Current loss:  26.875070571899414      Current accuracy:  3.0      Comp_time of one batch:  -0.8958711624145508\n",
            "Iteration:  26055      Current loss:  34.7861328125      Current accuracy:  3.0      Comp_time of one batch:  -0.9090638160705566\n",
            "Iteration:  26060      Current loss:  23.569225311279297      Current accuracy:  4.0      Comp_time of one batch:  -0.8827321529388428\n",
            "Iteration:  26065      Current loss:  27.157100677490234      Current accuracy:  4.0      Comp_time of one batch:  -0.8880395889282227\n",
            "Iteration:  26070      Current loss:  33.45922088623047      Current accuracy:  4.0      Comp_time of one batch:  -0.8839643001556396\n",
            "Iteration:  26075      Current loss:  25.37240219116211      Current accuracy:  4.0      Comp_time of one batch:  -0.8715813159942627\n",
            "Iteration:  26080      Current loss:  27.14739990234375      Current accuracy:  4.0      Comp_time of one batch:  -0.8819639682769775\n",
            "Iteration:  26085      Current loss:  19.972450256347656      Current accuracy:  5.0      Comp_time of one batch:  -0.9350275993347168\n",
            "Iteration:  26090      Current loss:  23.543521881103516      Current accuracy:  5.0      Comp_time of one batch:  -0.9141311645507812\n",
            "Iteration:  26095      Current loss:  18.63182258605957      Current accuracy:  6.0      Comp_time of one batch:  -0.8917732238769531\n",
            "Iteration:  26100      Current loss:  18.926908493041992      Current accuracy:  7.0      Comp_time of one batch:  -0.8715114593505859\n",
            "Iteration:  26105      Current loss:  19.827024459838867      Current accuracy:  8.0      Comp_time of one batch:  -0.8873093128204346\n",
            "Iteration:  26110      Current loss:  24.415586471557617      Current accuracy:  8.0      Comp_time of one batch:  -0.8714418411254883\n",
            "Iteration:  26115      Current loss:  32.648685455322266      Current accuracy:  8.0      Comp_time of one batch:  -0.8959815502166748\n",
            "Iteration:  26120      Current loss:  23.925457000732422      Current accuracy:  8.0      Comp_time of one batch:  -0.8847842216491699\n",
            "Iteration:  26125      Current loss:  19.950708389282227      Current accuracy:  9.0      Comp_time of one batch:  -0.893378734588623\n",
            "Iteration:  26130      Current loss:  24.855226516723633      Current accuracy:  9.0      Comp_time of one batch:  -0.8901898860931396\n",
            "Iteration:  26135      Current loss:  17.08944320678711      Current accuracy:  10.0      Comp_time of one batch:  -0.8831655979156494\n",
            "Iteration:  26140      Current loss:  24.064725875854492      Current accuracy:  10.0      Comp_time of one batch:  -0.9058094024658203\n",
            "Iteration:  26145      Current loss:  24.965518951416016      Current accuracy:  11.0      Comp_time of one batch:  -0.9095678329467773\n",
            "Iteration:  26150      Current loss:  29.60474395751953      Current accuracy:  11.0      Comp_time of one batch:  -0.8810503482818604\n",
            "Iteration:  26155      Current loss:  23.994775772094727      Current accuracy:  11.0      Comp_time of one batch:  -0.8918461799621582\n",
            "Iteration:  26160      Current loss:  24.46531867980957      Current accuracy:  11.0      Comp_time of one batch:  -0.8846108913421631\n",
            "Iteration:  26165      Current loss:  23.909072875976562      Current accuracy:  12.0      Comp_time of one batch:  -0.8840372562408447\n",
            "Iteration:  26170      Current loss:  26.499279022216797      Current accuracy:  12.0      Comp_time of one batch:  -0.890265703201294\n",
            "Iteration:  26175      Current loss:  26.881519317626953      Current accuracy:  12.0      Comp_time of one batch:  -0.9043169021606445\n",
            "Iteration:  26180      Current loss:  28.41694450378418      Current accuracy:  12.0      Comp_time of one batch:  -0.8855197429656982\n",
            "Iteration:  26185      Current loss:  27.76021385192871      Current accuracy:  12.0      Comp_time of one batch:  -0.883091926574707\n",
            "Iteration:  26190      Current loss:  22.044172286987305      Current accuracy:  13.0      Comp_time of one batch:  -0.8892412185668945\n",
            "Iteration:  26195      Current loss:  19.588857650756836      Current accuracy:  14.0      Comp_time of one batch:  -0.8888988494873047\n",
            "Iteration:  26200      Current loss:  23.042091369628906      Current accuracy:  15.0      Comp_time of one batch:  -0.8852808475494385\n",
            "Iteration:  26205      Current loss:  21.92617416381836      Current accuracy:  16.0      Comp_time of one batch:  -0.8847174644470215\n",
            "Iteration:  26210      Current loss:  24.009065628051758      Current accuracy:  17.0      Comp_time of one batch:  -0.8952453136444092\n",
            "Iteration:  26215      Current loss:  21.548717498779297      Current accuracy:  18.0      Comp_time of one batch:  -0.8828220367431641\n",
            "Iteration:  26220      Current loss:  24.584436416625977      Current accuracy:  18.0      Comp_time of one batch:  -0.8827643394470215\n",
            "Iteration:  26225      Current loss:  26.49618148803711      Current accuracy:  18.0      Comp_time of one batch:  -0.8828730583190918\n",
            "Iteration:  26230      Current loss:  17.719684600830078      Current accuracy:  19.0      Comp_time of one batch:  -0.9039955139160156\n",
            "Iteration:  26235      Current loss:  26.647991180419922      Current accuracy:  19.0      Comp_time of one batch:  -0.8904414176940918\n",
            "Iteration:  26240      Current loss:  21.316503524780273      Current accuracy:  20.0      Comp_time of one batch:  -0.8785688877105713\n",
            "Iteration:  26245      Current loss:  35.65109634399414      Current accuracy:  20.0      Comp_time of one batch:  -0.8945512771606445\n",
            "Iteration:  26250      Current loss:  22.949378967285156      Current accuracy:  21.0      Comp_time of one batch:  -0.9101266860961914\n",
            "Iteration:  26255      Current loss:  22.281118392944336      Current accuracy:  21.0      Comp_time of one batch:  -0.8798086643218994\n",
            "Iteration:  26260      Current loss:  23.135034561157227      Current accuracy:  22.0      Comp_time of one batch:  -0.8774325847625732\n",
            "Iteration:  26265      Current loss:  29.04222869873047      Current accuracy:  22.0      Comp_time of one batch:  -0.8795511722564697\n",
            "Iteration:  26270      Current loss:  16.18454360961914      Current accuracy:  23.0      Comp_time of one batch:  -0.8838334083557129\n",
            "Iteration:  26275      Current loss:  21.44386100769043      Current accuracy:  24.0      Comp_time of one batch:  -0.873955488204956\n",
            "Iteration:  26280      Current loss:  27.10365104675293      Current accuracy:  24.0      Comp_time of one batch:  -0.8864364624023438\n",
            "Iteration:  26285      Current loss:  26.98373031616211      Current accuracy:  24.0      Comp_time of one batch:  -0.8903446197509766\n",
            "Iteration:  26290      Current loss:  23.734434127807617      Current accuracy:  25.0      Comp_time of one batch:  -0.8829388618469238\n",
            "Iteration:  26295      Current loss:  33.899635314941406      Current accuracy:  25.0      Comp_time of one batch:  -0.8781721591949463\n",
            "Iteration:  26300      Current loss:  24.414731979370117      Current accuracy:  25.0      Comp_time of one batch:  -0.8815174102783203\n",
            "Iteration:  26305      Current loss:  19.679109573364258      Current accuracy:  26.0      Comp_time of one batch:  -0.8859972953796387\n",
            "Iteration:  26310      Current loss:  18.489696502685547      Current accuracy:  27.0      Comp_time of one batch:  -0.881023645401001\n",
            "Iteration:  26315      Current loss:  18.496034622192383      Current accuracy:  28.0      Comp_time of one batch:  -0.8793601989746094\n",
            "Iteration:  26320      Current loss:  37.14175796508789      Current accuracy:  28.0      Comp_time of one batch:  -0.8961825370788574\n",
            "Iteration:  26325      Current loss:  18.15058708190918      Current accuracy:  29.0      Comp_time of one batch:  -0.8848836421966553\n",
            "Iteration:  26330      Current loss:  20.573762893676758      Current accuracy:  30.0      Comp_time of one batch:  -0.9075467586517334\n",
            "Iteration:  26335      Current loss:  23.758628845214844      Current accuracy:  31.0      Comp_time of one batch:  -0.8865158557891846\n",
            "Iteration:  26340      Current loss:  25.852697372436523      Current accuracy:  31.0      Comp_time of one batch:  -0.8810639381408691\n",
            "Iteration:  26345      Current loss:  17.967147827148438      Current accuracy:  32.0      Comp_time of one batch:  -0.8928720951080322\n",
            "Iteration:  26350      Current loss:  23.965599060058594      Current accuracy:  32.0      Comp_time of one batch:  -0.90313720703125\n",
            "Iteration:  26355      Current loss:  17.125215530395508      Current accuracy:  33.0      Comp_time of one batch:  -0.898749589920044\n",
            "Iteration:  26360      Current loss:  25.015336990356445      Current accuracy:  33.0      Comp_time of one batch:  -0.9181289672851562\n",
            "Iteration:  26365      Current loss:  31.50444984436035      Current accuracy:  33.0      Comp_time of one batch:  -0.89601731300354\n",
            "Iteration:  26370      Current loss:  17.28533172607422      Current accuracy:  34.0      Comp_time of one batch:  -0.8962092399597168\n",
            "Iteration:  26375      Current loss:  19.686674118041992      Current accuracy:  35.0      Comp_time of one batch:  -0.8937966823577881\n",
            "Iteration:  26380      Current loss:  23.09856414794922      Current accuracy:  36.0      Comp_time of one batch:  -0.880251407623291\n",
            "Iteration:  26385      Current loss:  32.72802734375      Current accuracy:  36.0      Comp_time of one batch:  -0.9022026062011719\n",
            "Iteration:  26390      Current loss:  41.6381721496582      Current accuracy:  36.0      Comp_time of one batch:  -0.8795716762542725\n",
            "Iteration:  26395      Current loss:  20.116283416748047      Current accuracy:  37.0      Comp_time of one batch:  -0.8872363567352295\n",
            "Iteration:  26400      Current loss:  30.378150939941406      Current accuracy:  37.0      Comp_time of one batch:  -0.9056618213653564\n",
            "Iteration:  26405      Current loss:  24.627050399780273      Current accuracy:  37.0      Comp_time of one batch:  -0.8813984394073486\n",
            "Iteration:  26410      Current loss:  18.634445190429688      Current accuracy:  38.0      Comp_time of one batch:  -0.8799622058868408\n",
            "Iteration:  26415      Current loss:  16.987979888916016      Current accuracy:  39.0      Comp_time of one batch:  -0.884894609451294\n",
            "Iteration:  26420      Current loss:  24.53022003173828      Current accuracy:  39.0      Comp_time of one batch:  -0.9014692306518555\n",
            "Iteration:  26425      Current loss:  23.855878829956055      Current accuracy:  39.0      Comp_time of one batch:  -0.9011547565460205\n",
            "Iteration:  26430      Current loss:  22.95598602294922      Current accuracy:  39.0      Comp_time of one batch:  -0.9320206642150879\n",
            "Iteration:  26435      Current loss:  19.426570892333984      Current accuracy:  40.0      Comp_time of one batch:  -0.8806283473968506\n",
            "Iteration:  26440      Current loss:  21.840789794921875      Current accuracy:  41.0      Comp_time of one batch:  -0.8978357315063477\n",
            "Iteration:  26445      Current loss:  32.388458251953125      Current accuracy:  41.0      Comp_time of one batch:  -0.8806552886962891\n",
            "Iteration:  26450      Current loss:  20.560260772705078      Current accuracy:  42.0      Comp_time of one batch:  -0.8810415267944336\n",
            "Iteration:  26455      Current loss:  22.37450408935547      Current accuracy:  43.0      Comp_time of one batch:  -0.9002556800842285\n",
            "Iteration:  26460      Current loss:  31.741302490234375      Current accuracy:  43.0      Comp_time of one batch:  -0.8783245086669922\n",
            "Iteration:  26465      Current loss:  25.191463470458984      Current accuracy:  44.0      Comp_time of one batch:  -0.8793606758117676\n",
            "Iteration:  26470      Current loss:  22.57565689086914      Current accuracy:  45.0      Comp_time of one batch:  -0.8928091526031494\n",
            "Iteration:  26475      Current loss:  21.621931076049805      Current accuracy:  46.0      Comp_time of one batch:  -0.8722944259643555\n",
            "Iteration:  26480      Current loss:  22.89299201965332      Current accuracy:  47.0      Comp_time of one batch:  -0.8748557567596436\n",
            "Iteration:  26485      Current loss:  22.95989990234375      Current accuracy:  48.0      Comp_time of one batch:  -0.874469518661499\n",
            "Iteration:  26490      Current loss:  18.8868408203125      Current accuracy:  49.0      Comp_time of one batch:  -0.8982250690460205\n",
            "Iteration:  26495      Current loss:  23.10643196105957      Current accuracy:  49.0      Comp_time of one batch:  -0.8941874504089355\n",
            "Iteration:  26500      Current loss:  22.597606658935547      Current accuracy:  0.0      Comp_time of one batch:  -0.8859152793884277\n",
            "Iteration:  26505      Current loss:  28.71009063720703      Current accuracy:  0.0      Comp_time of one batch:  -0.8937690258026123\n",
            "Iteration:  26510      Current loss:  24.68844223022461      Current accuracy:  0.0      Comp_time of one batch:  -0.8906118869781494\n",
            "Iteration:  26515      Current loss:  24.93842887878418      Current accuracy:  0.0      Comp_time of one batch:  -0.8793110847473145\n",
            "Iteration:  26520      Current loss:  23.013141632080078      Current accuracy:  1.0      Comp_time of one batch:  -0.8957898616790771\n",
            "Iteration:  26525      Current loss:  27.37627410888672      Current accuracy:  1.0      Comp_time of one batch:  -0.886256217956543\n",
            "Iteration:  26530      Current loss:  26.5103816986084      Current accuracy:  1.0      Comp_time of one batch:  -0.8985953330993652\n",
            "Iteration:  26535      Current loss:  27.519807815551758      Current accuracy:  1.0      Comp_time of one batch:  -0.8833816051483154\n",
            "Iteration:  26540      Current loss:  34.89802169799805      Current accuracy:  1.0      Comp_time of one batch:  -0.8789591789245605\n",
            "Iteration:  26545      Current loss:  25.560728073120117      Current accuracy:  1.0      Comp_time of one batch:  -0.8977212905883789\n",
            "Iteration:  26550      Current loss:  20.54309844970703      Current accuracy:  2.0      Comp_time of one batch:  -0.8817975521087646\n",
            "Iteration:  26555      Current loss:  22.108766555786133      Current accuracy:  3.0      Comp_time of one batch:  -0.8848271369934082\n",
            "Iteration:  26560      Current loss:  29.060914993286133      Current accuracy:  3.0      Comp_time of one batch:  -0.883298397064209\n",
            "Iteration:  26565      Current loss:  29.02766990661621      Current accuracy:  3.0      Comp_time of one batch:  -0.8868935108184814\n",
            "Iteration:  26570      Current loss:  17.229705810546875      Current accuracy:  4.0      Comp_time of one batch:  -0.8916747570037842\n",
            "Iteration:  26575      Current loss:  22.64529800415039      Current accuracy:  5.0      Comp_time of one batch:  -0.89703369140625\n",
            "Iteration:  26580      Current loss:  22.678102493286133      Current accuracy:  6.0      Comp_time of one batch:  -0.8798177242279053\n",
            "Iteration:  26585      Current loss:  26.858741760253906      Current accuracy:  6.0      Comp_time of one batch:  -0.8815200328826904\n",
            "Iteration:  26590      Current loss:  23.343584060668945      Current accuracy:  7.0      Comp_time of one batch:  -0.8841521739959717\n",
            "Iteration:  26595      Current loss:  19.48443031311035      Current accuracy:  8.0      Comp_time of one batch:  -0.8833231925964355\n",
            "Iteration:  26600      Current loss:  26.330026626586914      Current accuracy:  8.0      Comp_time of one batch:  -0.8922271728515625\n",
            "Iteration:  26605      Current loss:  29.323139190673828      Current accuracy:  8.0      Comp_time of one batch:  -0.8842940330505371\n",
            "Iteration:  26610      Current loss:  33.53201675415039      Current accuracy:  8.0      Comp_time of one batch:  -0.928863525390625\n",
            "Iteration:  26615      Current loss:  21.503820419311523      Current accuracy:  9.0      Comp_time of one batch:  -0.9316737651824951\n",
            "Iteration:  26620      Current loss:  29.248262405395508      Current accuracy:  9.0      Comp_time of one batch:  -0.885779857635498\n",
            "Iteration:  26625      Current loss:  24.953386306762695      Current accuracy:  9.0      Comp_time of one batch:  -0.8936316967010498\n",
            "Iteration:  26630      Current loss:  21.447917938232422      Current accuracy:  10.0      Comp_time of one batch:  -0.8946094512939453\n",
            "Iteration:  26635      Current loss:  20.878067016601562      Current accuracy:  11.0      Comp_time of one batch:  -0.8766310214996338\n",
            "Iteration:  26640      Current loss:  30.02315330505371      Current accuracy:  11.0      Comp_time of one batch:  -0.8786823749542236\n",
            "Iteration:  26645      Current loss:  21.591175079345703      Current accuracy:  12.0      Comp_time of one batch:  -0.9073081016540527\n",
            "Iteration:  26650      Current loss:  29.79905891418457      Current accuracy:  12.0      Comp_time of one batch:  -0.8851604461669922\n",
            "Iteration:  26655      Current loss:  28.14095687866211      Current accuracy:  12.0      Comp_time of one batch:  -0.8859684467315674\n",
            "Iteration:  26660      Current loss:  23.514863967895508      Current accuracy:  12.0      Comp_time of one batch:  -0.8794362545013428\n",
            "Iteration:  26665      Current loss:  25.296640396118164      Current accuracy:  12.0      Comp_time of one batch:  -0.8911552429199219\n",
            "Iteration:  26670      Current loss:  19.458650588989258      Current accuracy:  13.0      Comp_time of one batch:  -0.8879482746124268\n",
            "Iteration:  26675      Current loss:  27.461183547973633      Current accuracy:  13.0      Comp_time of one batch:  -0.878401517868042\n",
            "Iteration:  26680      Current loss:  27.134937286376953      Current accuracy:  13.0      Comp_time of one batch:  -0.8860440254211426\n",
            "Iteration:  26685      Current loss:  26.005645751953125      Current accuracy:  13.0      Comp_time of one batch:  -0.8814296722412109\n",
            "Iteration:  26690      Current loss:  18.488006591796875      Current accuracy:  14.0      Comp_time of one batch:  -0.8841381072998047\n",
            "Iteration:  26695      Current loss:  28.75846290588379      Current accuracy:  14.0      Comp_time of one batch:  -0.8842980861663818\n",
            "Iteration:  26700      Current loss:  28.517898559570312      Current accuracy:  14.0      Comp_time of one batch:  -0.8940854072570801\n",
            "Iteration:  26705      Current loss:  26.997238159179688      Current accuracy:  14.0      Comp_time of one batch:  -0.8756446838378906\n",
            "Iteration:  26710      Current loss:  23.521921157836914      Current accuracy:  15.0      Comp_time of one batch:  -0.8714394569396973\n",
            "Iteration:  26715      Current loss:  25.40790367126465      Current accuracy:  15.0      Comp_time of one batch:  -0.8717138767242432\n",
            "Iteration:  26720      Current loss:  19.762651443481445      Current accuracy:  16.0      Comp_time of one batch:  -0.8841104507446289\n",
            "Iteration:  26725      Current loss:  21.6954345703125      Current accuracy:  17.0      Comp_time of one batch:  -0.8742399215698242\n",
            "Iteration:  26730      Current loss:  24.137523651123047      Current accuracy:  18.0      Comp_time of one batch:  -0.8822407722473145\n",
            "Iteration:  26735      Current loss:  37.26952362060547      Current accuracy:  18.0      Comp_time of one batch:  -0.8870799541473389\n",
            "Iteration:  26740      Current loss:  23.208816528320312      Current accuracy:  19.0      Comp_time of one batch:  -0.8884057998657227\n",
            "Iteration:  26745      Current loss:  22.43661117553711      Current accuracy:  20.0      Comp_time of one batch:  -0.8833129405975342\n",
            "Iteration:  26750      Current loss:  29.41774559020996      Current accuracy:  20.0      Comp_time of one batch:  -0.8858604431152344\n",
            "Iteration:  26755      Current loss:  31.729690551757812      Current accuracy:  20.0      Comp_time of one batch:  -0.8887689113616943\n",
            "Iteration:  26760      Current loss:  25.345975875854492      Current accuracy:  20.0      Comp_time of one batch:  -0.8955075740814209\n",
            "Iteration:  26765      Current loss:  28.70421028137207      Current accuracy:  20.0      Comp_time of one batch:  -0.9026854038238525\n",
            "Iteration:  26770      Current loss:  24.543088912963867      Current accuracy:  20.0      Comp_time of one batch:  -0.8976242542266846\n",
            "Iteration:  26775      Current loss:  28.701452255249023      Current accuracy:  20.0      Comp_time of one batch:  -0.900399923324585\n",
            "Iteration:  26780      Current loss:  20.902433395385742      Current accuracy:  21.0      Comp_time of one batch:  -0.8882455825805664\n",
            "Iteration:  26785      Current loss:  22.139019012451172      Current accuracy:  22.0      Comp_time of one batch:  -0.9009313583374023\n",
            "Iteration:  26790      Current loss:  26.656885147094727      Current accuracy:  23.0      Comp_time of one batch:  -0.9022839069366455\n",
            "Iteration:  26795      Current loss:  21.832773208618164      Current accuracy:  24.0      Comp_time of one batch:  -0.882535457611084\n",
            "Iteration:  26800      Current loss:  27.3228759765625      Current accuracy:  25.0      Comp_time of one batch:  -0.8848705291748047\n",
            "Iteration:  26805      Current loss:  23.217987060546875      Current accuracy:  25.0      Comp_time of one batch:  -0.8905816078186035\n",
            "Iteration:  26810      Current loss:  31.722524642944336      Current accuracy:  25.0      Comp_time of one batch:  -0.909001350402832\n",
            "Iteration:  26815      Current loss:  22.840124130249023      Current accuracy:  26.0      Comp_time of one batch:  -0.9134471416473389\n",
            "Iteration:  26820      Current loss:  21.31488037109375      Current accuracy:  27.0      Comp_time of one batch:  -0.8737771511077881\n",
            "Iteration:  26825      Current loss:  25.566627502441406      Current accuracy:  27.0      Comp_time of one batch:  -0.9036111831665039\n",
            "Iteration:  26830      Current loss:  17.128835678100586      Current accuracy:  28.0      Comp_time of one batch:  -0.8754980564117432\n",
            "Iteration:  26835      Current loss:  27.34722328186035      Current accuracy:  28.0      Comp_time of one batch:  -0.8804829120635986\n",
            "Iteration:  26840      Current loss:  22.400327682495117      Current accuracy:  29.0      Comp_time of one batch:  -0.8888013362884521\n",
            "Iteration:  26845      Current loss:  22.309768676757812      Current accuracy:  30.0      Comp_time of one batch:  -0.8969278335571289\n",
            "Iteration:  26850      Current loss:  19.078664779663086      Current accuracy:  31.0      Comp_time of one batch:  -0.8968751430511475\n",
            "Iteration:  26855      Current loss:  33.70771408081055      Current accuracy:  31.0      Comp_time of one batch:  -0.8822197914123535\n",
            "Iteration:  26860      Current loss:  23.354310989379883      Current accuracy:  31.0      Comp_time of one batch:  -0.890047550201416\n",
            "Iteration:  26865      Current loss:  20.624048233032227      Current accuracy:  32.0      Comp_time of one batch:  -0.8906102180480957\n",
            "Iteration:  26870      Current loss:  24.335376739501953      Current accuracy:  32.0      Comp_time of one batch:  -0.8870053291320801\n",
            "Iteration:  26875      Current loss:  32.254051208496094      Current accuracy:  32.0      Comp_time of one batch:  -0.8749282360076904\n",
            "Iteration:  26880      Current loss:  20.351411819458008      Current accuracy:  33.0      Comp_time of one batch:  -0.8803789615631104\n",
            "Iteration:  26885      Current loss:  19.681692123413086      Current accuracy:  34.0      Comp_time of one batch:  -0.8850193023681641\n",
            "Iteration:  26890      Current loss:  26.696727752685547      Current accuracy:  34.0      Comp_time of one batch:  -0.9054811000823975\n",
            "Iteration:  26895      Current loss:  21.45683479309082      Current accuracy:  35.0      Comp_time of one batch:  -0.8981246948242188\n",
            "Iteration:  26900      Current loss:  27.042551040649414      Current accuracy:  35.0      Comp_time of one batch:  -0.8821871280670166\n",
            "Iteration:  26905      Current loss:  33.56825256347656      Current accuracy:  35.0      Comp_time of one batch:  -0.9099934101104736\n",
            "Iteration:  26910      Current loss:  19.972333908081055      Current accuracy:  36.0      Comp_time of one batch:  -0.8844728469848633\n",
            "Iteration:  26915      Current loss:  21.144865036010742      Current accuracy:  37.0      Comp_time of one batch:  -0.8755142688751221\n",
            "Iteration:  26920      Current loss:  23.407682418823242      Current accuracy:  38.0      Comp_time of one batch:  -0.900266170501709\n",
            "Iteration:  26925      Current loss:  22.054916381835938      Current accuracy:  39.0      Comp_time of one batch:  -0.9121062755584717\n",
            "Iteration:  26930      Current loss:  33.432220458984375      Current accuracy:  39.0      Comp_time of one batch:  -0.8847296237945557\n",
            "Iteration:  26935      Current loss:  21.9223575592041      Current accuracy:  40.0      Comp_time of one batch:  -0.9150817394256592\n",
            "Iteration:  26940      Current loss:  23.924776077270508      Current accuracy:  41.0      Comp_time of one batch:  -0.8975727558135986\n",
            "Iteration:  26945      Current loss:  44.7685432434082      Current accuracy:  41.0      Comp_time of one batch:  -0.8999783992767334\n",
            "Iteration:  26950      Current loss:  20.83483123779297      Current accuracy:  42.0      Comp_time of one batch:  -0.9066221714019775\n",
            "Iteration:  26955      Current loss:  21.392995834350586      Current accuracy:  43.0      Comp_time of one batch:  -0.8905878067016602\n",
            "Iteration:  26960      Current loss:  31.609235763549805      Current accuracy:  43.0      Comp_time of one batch:  -0.8824255466461182\n",
            "Iteration:  26965      Current loss:  29.68003273010254      Current accuracy:  43.0      Comp_time of one batch:  -0.9117724895477295\n",
            "Iteration:  26970      Current loss:  27.656343460083008      Current accuracy:  43.0      Comp_time of one batch:  -0.8789336681365967\n",
            "Iteration:  26975      Current loss:  30.6758975982666      Current accuracy:  43.0      Comp_time of one batch:  -0.9032273292541504\n",
            "Iteration:  26980      Current loss:  26.57537269592285      Current accuracy:  43.0      Comp_time of one batch:  -0.8850283622741699\n",
            "Iteration:  26985      Current loss:  29.927509307861328      Current accuracy:  43.0      Comp_time of one batch:  -0.8878939151763916\n",
            "Iteration:  26990      Current loss:  26.1612491607666      Current accuracy:  43.0      Comp_time of one batch:  -0.8852584362030029\n",
            "Iteration:  26995      Current loss:  24.419633865356445      Current accuracy:  43.0      Comp_time of one batch:  -0.8909480571746826\n",
            "Iteration:  27000      Current loss:  22.578248977661133      Current accuracy:  44.0      Comp_time of one batch:  -0.8915755748748779\n",
            "Iteration:  27005      Current loss:  21.78290557861328      Current accuracy:  45.0      Comp_time of one batch:  -0.8897907733917236\n",
            "Iteration:  27010      Current loss:  30.135324478149414      Current accuracy:  45.0      Comp_time of one batch:  -0.9015007019042969\n",
            "Iteration:  27015      Current loss:  26.557117462158203      Current accuracy:  45.0      Comp_time of one batch:  -0.8858308792114258\n",
            "Iteration:  27020      Current loss:  24.48072052001953      Current accuracy:  45.0      Comp_time of one batch:  -0.8940362930297852\n",
            "Iteration:  27025      Current loss:  21.66636848449707      Current accuracy:  46.0      Comp_time of one batch:  -0.8798184394836426\n",
            "Iteration:  27030      Current loss:  31.283489227294922      Current accuracy:  46.0      Comp_time of one batch:  -0.8727438449859619\n",
            "Iteration:  27035      Current loss:  19.131900787353516      Current accuracy:  47.0      Comp_time of one batch:  -0.8871667385101318\n",
            "Iteration:  27040      Current loss:  32.172176361083984      Current accuracy:  47.0      Comp_time of one batch:  -0.9193210601806641\n",
            "Iteration:  27045      Current loss:  22.887344360351562      Current accuracy:  48.0      Comp_time of one batch:  -0.9019675254821777\n",
            "Iteration:  27050      Current loss:  34.89375686645508      Current accuracy:  48.0      Comp_time of one batch:  -0.8767807483673096\n",
            "Iteration:  27055      Current loss:  23.246864318847656      Current accuracy:  48.0      Comp_time of one batch:  -0.8748128414154053\n",
            "Iteration:  27060      Current loss:  26.837013244628906      Current accuracy:  48.0      Comp_time of one batch:  -0.8903255462646484\n",
            "Iteration:  27065      Current loss:  28.630634307861328      Current accuracy:  48.0      Comp_time of one batch:  -0.8985838890075684\n",
            "Iteration:  27070      Current loss:  26.651525497436523      Current accuracy:  48.0      Comp_time of one batch:  -0.8859286308288574\n",
            "Iteration:  27075      Current loss:  19.305444717407227      Current accuracy:  49.0      Comp_time of one batch:  -0.8711521625518799\n",
            "Iteration:  27080      Current loss:  21.057178497314453      Current accuracy:  50.0      Comp_time of one batch:  -0.8761866092681885\n",
            "Iteration:  27085      Current loss:  19.78363800048828      Current accuracy:  51.0      Comp_time of one batch:  -0.8746657371520996\n",
            "Iteration:  27090      Current loss:  22.879098892211914      Current accuracy:  52.0      Comp_time of one batch:  -0.8801777362823486\n",
            "Iteration:  27095      Current loss:  25.699554443359375      Current accuracy:  52.0      Comp_time of one batch:  -0.8704550266265869\n",
            "THIS IS THE END OF THE EPOCH\n",
            "Iteration:  27097      Current loss:  28.81656265258789      Current accuracy:  52.0      Comp_time of one batch:  -0.4272284507751465\n",
            "Iteration:  5      Current loss:  61.93459701538086      Current accuracy:  0.0      Comp_time of one batch:  -0.3931138515472412\n",
            "Iteration:  10      Current loss:  109.29240417480469      Current accuracy:  0.0      Comp_time of one batch:  -0.33199381828308105\n",
            "Iteration:  15      Current loss:  85.37749481201172      Current accuracy:  0.0      Comp_time of one batch:  -0.3250148296356201\n",
            "Iteration:  20      Current loss:  104.49210357666016      Current accuracy:  0.0      Comp_time of one batch:  -0.32269811630249023\n",
            "Iteration:  25      Current loss:  136.4995880126953      Current accuracy:  0.0      Comp_time of one batch:  -0.3279740810394287\n",
            "Iteration:  30      Current loss:  94.70196533203125      Current accuracy:  0.0      Comp_time of one batch:  -0.3391692638397217\n",
            "Iteration:  35      Current loss:  131.40074157714844      Current accuracy:  0.0      Comp_time of one batch:  -0.3172304630279541\n",
            "Iteration:  40      Current loss:  122.5032730102539      Current accuracy:  0.0      Comp_time of one batch:  -0.32381391525268555\n",
            "Iteration:  45      Current loss:  157.0803985595703      Current accuracy:  0.0      Comp_time of one batch:  -0.31551146507263184\n",
            "Iteration:  50      Current loss:  126.193115234375      Current accuracy:  0.0      Comp_time of one batch:  -0.3127627372741699\n",
            "Iteration:  55      Current loss:  102.89226531982422      Current accuracy:  0.0      Comp_time of one batch:  -0.31870102882385254\n",
            "Iteration:  60      Current loss:  111.23884582519531      Current accuracy:  0.0      Comp_time of one batch:  -0.3435206413269043\n",
            "Iteration:  65      Current loss:  115.96277618408203      Current accuracy:  0.0      Comp_time of one batch:  -0.31661510467529297\n",
            "Iteration:  70      Current loss:  115.29362487792969      Current accuracy:  0.0      Comp_time of one batch:  -0.3386514186859131\n",
            "Iteration:  75      Current loss:  109.99304962158203      Current accuracy:  0.0      Comp_time of one batch:  -0.33744382858276367\n",
            "Iteration:  80      Current loss:  114.08238220214844      Current accuracy:  0.0      Comp_time of one batch:  -0.31483960151672363\n",
            "Iteration:  85      Current loss:  80.34326171875      Current accuracy:  0.0      Comp_time of one batch:  -0.31876397132873535\n",
            "Iteration:  90      Current loss:  138.46022033691406      Current accuracy:  0.0      Comp_time of one batch:  -0.32157421112060547\n",
            "Iteration:  95      Current loss:  96.31254577636719      Current accuracy:  0.0      Comp_time of one batch:  -0.3387718200683594\n",
            "Iteration:  100      Current loss:  134.97218322753906      Current accuracy:  0.0      Comp_time of one batch:  -0.3183155059814453\n",
            "Iteration:  105      Current loss:  151.6428985595703      Current accuracy:  0.0      Comp_time of one batch:  -0.31420016288757324\n",
            "Iteration:  110      Current loss:  171.17330932617188      Current accuracy:  0.0      Comp_time of one batch:  -0.3329048156738281\n",
            "Iteration:  115      Current loss:  104.83795928955078      Current accuracy:  0.0      Comp_time of one batch:  -0.32567620277404785\n",
            "Iteration:  120      Current loss:  163.4430694580078      Current accuracy:  0.0      Comp_time of one batch:  -0.3145468235015869\n",
            "Iteration:  125      Current loss:  115.22755432128906      Current accuracy:  0.0      Comp_time of one batch:  -0.336169958114624\n",
            "Iteration:  130      Current loss:  118.12670135498047      Current accuracy:  0.0      Comp_time of one batch:  -0.31728291511535645\n",
            "Iteration:  135      Current loss:  143.59446716308594      Current accuracy:  0.0      Comp_time of one batch:  -0.31583356857299805\n",
            "Iteration:  140      Current loss:  141.1310272216797      Current accuracy:  0.0      Comp_time of one batch:  -0.3440110683441162\n",
            "Iteration:  145      Current loss:  90.75849914550781      Current accuracy:  0.0      Comp_time of one batch:  -0.32543253898620605\n",
            "Iteration:  150      Current loss:  91.37495422363281      Current accuracy:  0.0      Comp_time of one batch:  -0.31965160369873047\n",
            "Iteration:  155      Current loss:  84.48108673095703      Current accuracy:  0.0      Comp_time of one batch:  -0.3189671039581299\n",
            "Iteration:  160      Current loss:  76.74620056152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3333568572998047\n",
            "Iteration:  165      Current loss:  127.35021209716797      Current accuracy:  0.0      Comp_time of one batch:  -0.3245432376861572\n",
            "Iteration:  170      Current loss:  103.21359252929688      Current accuracy:  0.0      Comp_time of one batch:  -0.3203005790710449\n",
            "Iteration:  175      Current loss:  88.99946594238281      Current accuracy:  0.0      Comp_time of one batch:  -0.32771825790405273\n",
            "Iteration:  180      Current loss:  74.87294006347656      Current accuracy:  0.0      Comp_time of one batch:  -0.3153495788574219\n",
            "Iteration:  185      Current loss:  93.20377349853516      Current accuracy:  0.0      Comp_time of one batch:  -0.33957743644714355\n",
            "Iteration:  190      Current loss:  149.7415008544922      Current accuracy:  0.0      Comp_time of one batch:  -0.322559118270874\n",
            "Iteration:  195      Current loss:  95.60799407958984      Current accuracy:  0.0      Comp_time of one batch:  -0.31910181045532227\n",
            "Iteration:  200      Current loss:  79.83828735351562      Current accuracy:  0.0      Comp_time of one batch:  -0.337749719619751\n",
            "Iteration:  205      Current loss:  114.23477935791016      Current accuracy:  0.0      Comp_time of one batch:  -0.3185889720916748\n",
            "Iteration:  210      Current loss:  98.62557983398438      Current accuracy:  0.0      Comp_time of one batch:  -0.31648898124694824\n",
            "Iteration:  215      Current loss:  109.06918334960938      Current accuracy:  0.0      Comp_time of one batch:  -0.31779909133911133\n",
            "Iteration:  220      Current loss:  122.39398193359375      Current accuracy:  0.0      Comp_time of one batch:  -0.31458187103271484\n",
            "Iteration:  225      Current loss:  96.0655288696289      Current accuracy:  0.0      Comp_time of one batch:  -0.32105493545532227\n",
            "Iteration:  230      Current loss:  108.21861267089844      Current accuracy:  0.0      Comp_time of one batch:  -0.3220689296722412\n",
            "Iteration:  235      Current loss:  72.20879364013672      Current accuracy:  0.0      Comp_time of one batch:  -0.32056736946105957\n",
            "Iteration:  240      Current loss:  108.94439697265625      Current accuracy:  0.0      Comp_time of one batch:  -0.32102513313293457\n",
            "Iteration:  245      Current loss:  80.24345397949219      Current accuracy:  0.0      Comp_time of one batch:  -0.3363230228424072\n",
            "Iteration:  250      Current loss:  115.14698028564453      Current accuracy:  0.0      Comp_time of one batch:  -0.31676411628723145\n",
            "Iteration:  255      Current loss:  77.59977722167969      Current accuracy:  0.0      Comp_time of one batch:  -0.33940839767456055\n",
            "Iteration:  260      Current loss:  91.75495147705078      Current accuracy:  0.0      Comp_time of one batch:  -0.31400489807128906\n",
            "Iteration:  265      Current loss:  125.60408782958984      Current accuracy:  0.0      Comp_time of one batch:  -0.3122851848602295\n",
            "Iteration:  270      Current loss:  88.46057891845703      Current accuracy:  0.0      Comp_time of one batch:  -0.3370940685272217\n",
            "Iteration:  275      Current loss:  96.19952392578125      Current accuracy:  0.0      Comp_time of one batch:  -0.3163795471191406\n",
            "Iteration:  280      Current loss:  92.66808319091797      Current accuracy:  0.0      Comp_time of one batch:  -0.32563042640686035\n",
            "Iteration:  285      Current loss:  98.55919647216797      Current accuracy:  0.0      Comp_time of one batch:  -0.3351438045501709\n",
            "Iteration:  290      Current loss:  80.31159210205078      Current accuracy:  0.0      Comp_time of one batch:  -0.31151819229125977\n",
            "Iteration:  295      Current loss:  158.46139526367188      Current accuracy:  0.0      Comp_time of one batch:  -0.35513830184936523\n",
            "Iteration:  300      Current loss:  136.5032958984375      Current accuracy:  0.0      Comp_time of one batch:  -0.3342127799987793\n",
            "Iteration:  305      Current loss:  96.45531463623047      Current accuracy:  0.0      Comp_time of one batch:  -0.3337833881378174\n",
            "Iteration:  310      Current loss:  115.43302154541016      Current accuracy:  0.0      Comp_time of one batch:  -0.3091897964477539\n",
            "Iteration:  315      Current loss:  168.5558624267578      Current accuracy:  0.0      Comp_time of one batch:  -0.31222033500671387\n",
            "Iteration:  320      Current loss:  146.50961303710938      Current accuracy:  0.0      Comp_time of one batch:  -0.311145544052124\n",
            "Iteration:  325      Current loss:  106.43061065673828      Current accuracy:  0.0      Comp_time of one batch:  -0.3379175662994385\n",
            "Iteration:  330      Current loss:  127.1847915649414      Current accuracy:  0.0      Comp_time of one batch:  -0.3341095447540283\n",
            "Iteration:  335      Current loss:  87.59730529785156      Current accuracy:  0.0      Comp_time of one batch:  -0.31313180923461914\n",
            "Iteration:  340      Current loss:  133.20870971679688      Current accuracy:  0.0      Comp_time of one batch:  -0.3128969669342041\n",
            "Iteration:  345      Current loss:  106.00100708007812      Current accuracy:  0.0      Comp_time of one batch:  -0.332369327545166\n",
            "Iteration:  350      Current loss:  120.93800354003906      Current accuracy:  0.0      Comp_time of one batch:  -0.3139975070953369\n",
            "Iteration:  355      Current loss:  120.65763854980469      Current accuracy:  0.0      Comp_time of one batch:  -0.3187844753265381\n",
            "Iteration:  360      Current loss:  128.39881896972656      Current accuracy:  0.0      Comp_time of one batch:  -0.3116726875305176\n",
            "Iteration:  365      Current loss:  101.87677001953125      Current accuracy:  0.0      Comp_time of one batch:  -0.31850433349609375\n",
            "Iteration:  370      Current loss:  127.90331268310547      Current accuracy:  0.0      Comp_time of one batch:  -0.3228950500488281\n",
            "Iteration:  375      Current loss:  124.83273315429688      Current accuracy:  0.0      Comp_time of one batch:  -0.31539392471313477\n",
            "Iteration:  380      Current loss:  73.69905853271484      Current accuracy:  0.0      Comp_time of one batch:  -0.3108525276184082\n",
            "Iteration:  385      Current loss:  124.65237426757812      Current accuracy:  0.0      Comp_time of one batch:  -0.3216876983642578\n",
            "Iteration:  390      Current loss:  151.4325714111328      Current accuracy:  0.0      Comp_time of one batch:  -0.31724095344543457\n",
            "Iteration:  395      Current loss:  112.40608978271484      Current accuracy:  0.0      Comp_time of one batch:  -0.3139619827270508\n",
            "Iteration:  400      Current loss:  63.31023025512695      Current accuracy:  0.0      Comp_time of one batch:  -0.3366272449493408\n",
            "Iteration:  405      Current loss:  109.21826171875      Current accuracy:  0.0      Comp_time of one batch:  -0.34305882453918457\n",
            "Iteration:  410      Current loss:  100.58860778808594      Current accuracy:  0.0      Comp_time of one batch:  -0.31077027320861816\n",
            "Iteration:  415      Current loss:  93.71409606933594      Current accuracy:  0.0      Comp_time of one batch:  -0.31748247146606445\n",
            "Iteration:  420      Current loss:  102.04747772216797      Current accuracy:  0.0      Comp_time of one batch:  -0.3412652015686035\n",
            "Iteration:  425      Current loss:  125.2825927734375      Current accuracy:  0.0      Comp_time of one batch:  -0.3121802806854248\n",
            "Iteration:  430      Current loss:  114.04219055175781      Current accuracy:  0.0      Comp_time of one batch:  -0.31201887130737305\n",
            "Iteration:  435      Current loss:  106.87630462646484      Current accuracy:  0.0      Comp_time of one batch:  -0.3113901615142822\n",
            "Iteration:  440      Current loss:  111.40486907958984      Current accuracy:  0.0      Comp_time of one batch:  -0.3345072269439697\n",
            "Iteration:  445      Current loss:  122.9345932006836      Current accuracy:  0.0      Comp_time of one batch:  -0.32486581802368164\n",
            "Iteration:  450      Current loss:  134.11167907714844      Current accuracy:  0.0      Comp_time of one batch:  -0.3506464958190918\n",
            "Iteration:  455      Current loss:  104.55883026123047      Current accuracy:  0.0      Comp_time of one batch:  -0.33841776847839355\n",
            "Iteration:  460      Current loss:  165.7380828857422      Current accuracy:  0.0      Comp_time of one batch:  -0.3215608596801758\n",
            "Iteration:  465      Current loss:  107.50444030761719      Current accuracy:  0.0      Comp_time of one batch:  -0.33697056770324707\n",
            "Iteration:  470      Current loss:  107.62801361083984      Current accuracy:  0.0      Comp_time of one batch:  -0.3607470989227295\n",
            "Iteration:  475      Current loss:  118.56214904785156      Current accuracy:  0.0      Comp_time of one batch:  -0.3336799144744873\n",
            "Iteration:  480      Current loss:  121.76737976074219      Current accuracy:  0.0      Comp_time of one batch:  -0.3171803951263428\n",
            "Iteration:  485      Current loss:  175.05812072753906      Current accuracy:  0.0      Comp_time of one batch:  -0.3393261432647705\n",
            "Iteration:  490      Current loss:  116.61685180664062      Current accuracy:  0.0      Comp_time of one batch:  -0.319690465927124\n",
            "Iteration:  495      Current loss:  102.64344787597656      Current accuracy:  0.0      Comp_time of one batch:  -0.31792306900024414\n",
            "Iteration:  500      Current loss:  142.6312255859375      Current accuracy:  0.0      Comp_time of one batch:  -0.3550732135772705\n",
            "Iteration:  505      Current loss:  92.75363159179688      Current accuracy:  0.0      Comp_time of one batch:  -0.33116960525512695\n",
            "Iteration:  510      Current loss:  93.16033172607422      Current accuracy:  0.0      Comp_time of one batch:  -0.34004878997802734\n",
            "Iteration:  515      Current loss:  110.2025146484375      Current accuracy:  0.0      Comp_time of one batch:  -0.3401021957397461\n",
            "Iteration:  520      Current loss:  126.17323303222656      Current accuracy:  0.0      Comp_time of one batch:  -0.32638001441955566\n",
            "Iteration:  525      Current loss:  116.28482055664062      Current accuracy:  0.0      Comp_time of one batch:  -0.3399639129638672\n",
            "Iteration:  530      Current loss:  108.2446517944336      Current accuracy:  0.0      Comp_time of one batch:  -0.34157776832580566\n",
            "Iteration:  535      Current loss:  147.82041931152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3242921829223633\n",
            "Iteration:  540      Current loss:  128.6020965576172      Current accuracy:  0.0      Comp_time of one batch:  -0.3429429531097412\n",
            "Iteration:  545      Current loss:  103.72697448730469      Current accuracy:  0.0      Comp_time of one batch:  -0.31717848777770996\n",
            "Iteration:  550      Current loss:  125.88825225830078      Current accuracy:  0.0      Comp_time of one batch:  -0.3389396667480469\n",
            "Iteration:  555      Current loss:  166.7002410888672      Current accuracy:  0.0      Comp_time of one batch:  -0.33544039726257324\n",
            "Iteration:  560      Current loss:  104.638916015625      Current accuracy:  0.0      Comp_time of one batch:  -0.33340954780578613\n",
            "Iteration:  565      Current loss:  122.218017578125      Current accuracy:  0.0      Comp_time of one batch:  -0.32805800437927246\n",
            "Iteration:  570      Current loss:  153.9444122314453      Current accuracy:  0.0      Comp_time of one batch:  -0.3149073123931885\n",
            "Iteration:  575      Current loss:  75.99860382080078      Current accuracy:  0.0      Comp_time of one batch:  -0.3367772102355957\n",
            "Iteration:  580      Current loss:  120.83634948730469      Current accuracy:  0.0      Comp_time of one batch:  -0.3264784812927246\n",
            "Iteration:  585      Current loss:  129.24403381347656      Current accuracy:  0.0      Comp_time of one batch:  -0.3194761276245117\n",
            "Iteration:  590      Current loss:  126.3459243774414      Current accuracy:  0.0      Comp_time of one batch:  -0.33997058868408203\n",
            "Iteration:  595      Current loss:  139.4552001953125      Current accuracy:  0.0      Comp_time of one batch:  -0.3655383586883545\n",
            "Iteration:  600      Current loss:  105.8449935913086      Current accuracy:  0.0      Comp_time of one batch:  -0.33161067962646484\n",
            "Iteration:  605      Current loss:  124.12418365478516      Current accuracy:  0.0      Comp_time of one batch:  -0.3152885437011719\n",
            "Iteration:  610      Current loss:  135.05780029296875      Current accuracy:  0.0      Comp_time of one batch:  -0.3181138038635254\n",
            "Iteration:  615      Current loss:  105.20391845703125      Current accuracy:  0.0      Comp_time of one batch:  -0.3178229331970215\n",
            "Iteration:  620      Current loss:  118.74231719970703      Current accuracy:  0.0      Comp_time of one batch:  -0.3148488998413086\n",
            "Iteration:  625      Current loss:  83.8884506225586      Current accuracy:  0.0      Comp_time of one batch:  -0.3112297058105469\n",
            "Iteration:  630      Current loss:  110.72045135498047      Current accuracy:  0.0      Comp_time of one batch:  -0.3125452995300293\n",
            "Iteration:  635      Current loss:  93.92305755615234      Current accuracy:  0.0      Comp_time of one batch:  -0.3143758773803711\n",
            "Iteration:  640      Current loss:  104.69871520996094      Current accuracy:  0.0      Comp_time of one batch:  -0.3119065761566162\n",
            "Iteration:  645      Current loss:  88.34504699707031      Current accuracy:  0.0      Comp_time of one batch:  -0.34595274925231934\n",
            "Iteration:  650      Current loss:  158.5618896484375      Current accuracy:  0.0      Comp_time of one batch:  -0.31809425354003906\n",
            "Iteration:  655      Current loss:  93.87904357910156      Current accuracy:  0.0      Comp_time of one batch:  -0.3182663917541504\n",
            "Iteration:  660      Current loss:  103.78707122802734      Current accuracy:  0.0      Comp_time of one batch:  -0.31606554985046387\n",
            "Iteration:  665      Current loss:  127.86481475830078      Current accuracy:  0.0      Comp_time of one batch:  -0.3438897132873535\n",
            "Iteration:  670      Current loss:  114.85054016113281      Current accuracy:  0.0      Comp_time of one batch:  -0.343616247177124\n",
            "Iteration:  675      Current loss:  113.2376480102539      Current accuracy:  0.0      Comp_time of one batch:  -0.32306981086730957\n",
            "Iteration:  680      Current loss:  81.40941619873047      Current accuracy:  0.0      Comp_time of one batch:  -0.33661723136901855\n",
            "Iteration:  685      Current loss:  99.97779083251953      Current accuracy:  0.0      Comp_time of one batch:  -0.33607029914855957\n",
            "Iteration:  690      Current loss:  114.00494384765625      Current accuracy:  0.0      Comp_time of one batch:  -0.3575477600097656\n",
            "Iteration:  695      Current loss:  136.17294311523438      Current accuracy:  0.0      Comp_time of one batch:  -0.32344841957092285\n",
            "Iteration:  700      Current loss:  82.85765838623047      Current accuracy:  0.0      Comp_time of one batch:  -0.3385128974914551\n",
            "Iteration:  705      Current loss:  115.64158630371094      Current accuracy:  0.0      Comp_time of one batch:  -0.3222005367279053\n",
            "Iteration:  710      Current loss:  108.22178649902344      Current accuracy:  0.0      Comp_time of one batch:  -0.33185887336730957\n",
            "Iteration:  715      Current loss:  128.56417846679688      Current accuracy:  0.0      Comp_time of one batch:  -0.3367283344268799\n",
            "Iteration:  720      Current loss:  119.81883239746094      Current accuracy:  0.0      Comp_time of one batch:  -0.35102391242980957\n",
            "Iteration:  725      Current loss:  157.32530212402344      Current accuracy:  0.0      Comp_time of one batch:  -0.3149847984313965\n",
            "Iteration:  730      Current loss:  115.55137634277344      Current accuracy:  0.0      Comp_time of one batch:  -0.31395435333251953\n",
            "Iteration:  735      Current loss:  147.0736083984375      Current accuracy:  0.0      Comp_time of one batch:  -0.3374786376953125\n",
            "Iteration:  740      Current loss:  123.24871826171875      Current accuracy:  0.0      Comp_time of one batch:  -0.354081392288208\n",
            "Iteration:  745      Current loss:  130.31741333007812      Current accuracy:  0.0      Comp_time of one batch:  -0.3361384868621826\n",
            "Iteration:  750      Current loss:  120.2389907836914      Current accuracy:  0.0      Comp_time of one batch:  -0.31894445419311523\n",
            "Iteration:  755      Current loss:  96.02205657958984      Current accuracy:  0.0      Comp_time of one batch:  -0.3324246406555176\n",
            "Iteration:  760      Current loss:  111.35192108154297      Current accuracy:  0.0      Comp_time of one batch:  -0.3082084655761719\n",
            "Iteration:  765      Current loss:  106.27229309082031      Current accuracy:  0.0      Comp_time of one batch:  -0.32265758514404297\n",
            "Iteration:  770      Current loss:  124.68880462646484      Current accuracy:  0.0      Comp_time of one batch:  -0.3577280044555664\n",
            "Iteration:  775      Current loss:  116.65455627441406      Current accuracy:  0.0      Comp_time of one batch:  -0.33525681495666504\n",
            "Iteration:  780      Current loss:  78.7390365600586      Current accuracy:  0.0      Comp_time of one batch:  -0.33638834953308105\n",
            "Iteration:  785      Current loss:  114.04178619384766      Current accuracy:  0.0      Comp_time of one batch:  -0.3298912048339844\n",
            "Iteration:  790      Current loss:  126.44119262695312      Current accuracy:  0.0      Comp_time of one batch:  -0.31426382064819336\n",
            "Iteration:  795      Current loss:  111.708984375      Current accuracy:  0.0      Comp_time of one batch:  -0.34291577339172363\n",
            "Iteration:  800      Current loss:  77.00846862792969      Current accuracy:  0.0      Comp_time of one batch:  -0.3165552616119385\n",
            "Iteration:  805      Current loss:  145.64100646972656      Current accuracy:  0.0      Comp_time of one batch:  -0.3173692226409912\n",
            "Iteration:  810      Current loss:  97.37601470947266      Current accuracy:  0.0      Comp_time of one batch:  -0.3705282211303711\n",
            "Iteration:  815      Current loss:  128.47276306152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3236393928527832\n",
            "Iteration:  820      Current loss:  89.4815902709961      Current accuracy:  0.0      Comp_time of one batch:  -0.3256993293762207\n",
            "Iteration:  825      Current loss:  114.82206726074219      Current accuracy:  0.0      Comp_time of one batch:  -0.32042932510375977\n",
            "Iteration:  830      Current loss:  115.16059875488281      Current accuracy:  0.0      Comp_time of one batch:  -0.3147857189178467\n",
            "Iteration:  835      Current loss:  117.99166870117188      Current accuracy:  0.0      Comp_time of one batch:  -0.31699061393737793\n",
            "Iteration:  840      Current loss:  141.2327423095703      Current accuracy:  0.0      Comp_time of one batch:  -0.32330942153930664\n",
            "Iteration:  845      Current loss:  111.66586303710938      Current accuracy:  0.0      Comp_time of one batch:  -0.31911158561706543\n",
            "Iteration:  850      Current loss:  144.04425048828125      Current accuracy:  0.0      Comp_time of one batch:  -0.3113274574279785\n",
            "Iteration:  855      Current loss:  90.11426544189453      Current accuracy:  0.0      Comp_time of one batch:  -0.3363921642303467\n",
            "Iteration:  860      Current loss:  130.45379638671875      Current accuracy:  0.0      Comp_time of one batch:  -0.3388838768005371\n",
            "Iteration:  865      Current loss:  100.93597412109375      Current accuracy:  0.0      Comp_time of one batch:  -0.31943321228027344\n",
            "Iteration:  870      Current loss:  78.5218276977539      Current accuracy:  0.0      Comp_time of one batch:  -0.3417177200317383\n",
            "Iteration:  875      Current loss:  126.1968002319336      Current accuracy:  0.0      Comp_time of one batch:  -0.33295297622680664\n",
            "Iteration:  880      Current loss:  100.42049407958984      Current accuracy:  0.0      Comp_time of one batch:  -0.3179042339324951\n",
            "Iteration:  885      Current loss:  72.65249633789062      Current accuracy:  0.0      Comp_time of one batch:  -0.3215062618255615\n",
            "Iteration:  890      Current loss:  133.00965881347656      Current accuracy:  0.0      Comp_time of one batch:  -0.3222956657409668\n",
            "Iteration:  895      Current loss:  125.02144622802734      Current accuracy:  0.0      Comp_time of one batch:  -0.33223652839660645\n",
            "Iteration:  900      Current loss:  71.93390655517578      Current accuracy:  0.0      Comp_time of one batch:  -0.3393697738647461\n",
            "Iteration:  905      Current loss:  88.05467987060547      Current accuracy:  0.0      Comp_time of one batch:  -0.3412308692932129\n",
            "Iteration:  910      Current loss:  131.21670532226562      Current accuracy:  0.0      Comp_time of one batch:  -0.3319988250732422\n",
            "Iteration:  915      Current loss:  87.52519226074219      Current accuracy:  0.0      Comp_time of one batch:  -0.34739208221435547\n",
            "Iteration:  920      Current loss:  112.39447784423828      Current accuracy:  0.0      Comp_time of one batch:  -0.3379378318786621\n",
            "Iteration:  925      Current loss:  108.52630615234375      Current accuracy:  0.0      Comp_time of one batch:  -0.34001636505126953\n",
            "Iteration:  930      Current loss:  137.29457092285156      Current accuracy:  0.0      Comp_time of one batch:  -0.32318711280822754\n",
            "Iteration:  935      Current loss:  116.4492416381836      Current accuracy:  0.0      Comp_time of one batch:  -0.31794309616088867\n",
            "Iteration:  940      Current loss:  105.96092224121094      Current accuracy:  0.0      Comp_time of one batch:  -0.3143041133880615\n",
            "Iteration:  945      Current loss:  119.20681762695312      Current accuracy:  0.0      Comp_time of one batch:  -0.31658267974853516\n",
            "Iteration:  950      Current loss:  91.87726593017578      Current accuracy:  0.0      Comp_time of one batch:  -0.31107664108276367\n",
            "Iteration:  955      Current loss:  120.65489196777344      Current accuracy:  0.0      Comp_time of one batch:  -0.3137223720550537\n",
            "Iteration:  960      Current loss:  111.62519073486328      Current accuracy:  0.0      Comp_time of one batch:  -0.3221292495727539\n",
            "Iteration:  965      Current loss:  135.28355407714844      Current accuracy:  0.0      Comp_time of one batch:  -0.3506007194519043\n",
            "Iteration:  970      Current loss:  101.40237426757812      Current accuracy:  0.0      Comp_time of one batch:  -0.3374648094177246\n",
            "Iteration:  975      Current loss:  105.50653076171875      Current accuracy:  0.0      Comp_time of one batch:  -0.3236403465270996\n",
            "Iteration:  980      Current loss:  104.6037826538086      Current accuracy:  0.0      Comp_time of one batch:  -0.3169243335723877\n",
            "Iteration:  985      Current loss:  137.35501098632812      Current accuracy:  0.0      Comp_time of one batch:  -0.3178749084472656\n",
            "Iteration:  990      Current loss:  93.33271026611328      Current accuracy:  0.0      Comp_time of one batch:  -0.3193655014038086\n",
            "Iteration:  995      Current loss:  119.06299591064453      Current accuracy:  0.0      Comp_time of one batch:  -0.3379056453704834\n",
            "Iteration:  1000      Current loss:  132.31260681152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3130195140838623\n",
            "Iteration:  1005      Current loss:  131.62619018554688      Current accuracy:  0.0      Comp_time of one batch:  -0.3208649158477783\n",
            "Iteration:  1010      Current loss:  98.31105041503906      Current accuracy:  0.0      Comp_time of one batch:  -0.3180968761444092\n",
            "Iteration:  1015      Current loss:  155.77626037597656      Current accuracy:  0.0      Comp_time of one batch:  -0.31940627098083496\n",
            "Iteration:  1020      Current loss:  106.5941162109375      Current accuracy:  0.0      Comp_time of one batch:  -0.33247947692871094\n",
            "Iteration:  1025      Current loss:  113.2099380493164      Current accuracy:  0.0      Comp_time of one batch:  -0.32288479804992676\n",
            "Iteration:  1030      Current loss:  137.87026977539062      Current accuracy:  0.0      Comp_time of one batch:  -0.31928229331970215\n",
            "Iteration:  1035      Current loss:  124.43997955322266      Current accuracy:  0.0      Comp_time of one batch:  -0.31378889083862305\n",
            "Iteration:  1040      Current loss:  89.9410400390625      Current accuracy:  0.0      Comp_time of one batch:  -0.32375502586364746\n",
            "Iteration:  1045      Current loss:  113.1824722290039      Current accuracy:  0.0      Comp_time of one batch:  -0.31600356101989746\n",
            "Iteration:  1050      Current loss:  43.08329391479492      Current accuracy:  0.0      Comp_time of one batch:  -0.358593225479126\n",
            "Iteration:  1055      Current loss:  87.54933166503906      Current accuracy:  0.0      Comp_time of one batch:  -0.36235928535461426\n",
            "Iteration:  1060      Current loss:  129.3443145751953      Current accuracy:  0.0      Comp_time of one batch:  -0.3105912208557129\n",
            "Iteration:  1065      Current loss:  90.13628387451172      Current accuracy:  0.0      Comp_time of one batch:  -0.3209950923919678\n",
            "Iteration:  1070      Current loss:  109.47802734375      Current accuracy:  0.0      Comp_time of one batch:  -0.3181917667388916\n",
            "Iteration:  1075      Current loss:  128.25608825683594      Current accuracy:  0.0      Comp_time of one batch:  -0.34734034538269043\n",
            "Iteration:  1080      Current loss:  129.8869171142578      Current accuracy:  0.0      Comp_time of one batch:  -0.3212246894836426\n",
            "Iteration:  1085      Current loss:  127.7926025390625      Current accuracy:  0.0      Comp_time of one batch:  -0.3164365291595459\n",
            "Iteration:  1090      Current loss:  118.28650665283203      Current accuracy:  0.0      Comp_time of one batch:  -0.31595778465270996\n",
            "Iteration:  1095      Current loss:  109.86253356933594      Current accuracy:  0.0      Comp_time of one batch:  -0.32349610328674316\n",
            "Iteration:  1100      Current loss:  125.44602966308594      Current accuracy:  0.0      Comp_time of one batch:  -0.32598423957824707\n",
            "Iteration:  1105      Current loss:  149.9215850830078      Current accuracy:  0.0      Comp_time of one batch:  -0.3515651226043701\n",
            "Iteration:  1110      Current loss:  145.46653747558594      Current accuracy:  0.0      Comp_time of one batch:  -0.3333890438079834\n",
            "Iteration:  1115      Current loss:  126.144287109375      Current accuracy:  0.0      Comp_time of one batch:  -0.3195974826812744\n",
            "Iteration:  1120      Current loss:  99.20680236816406      Current accuracy:  0.0      Comp_time of one batch:  -0.31826090812683105\n",
            "Iteration:  1125      Current loss:  128.07627868652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3179171085357666\n",
            "Iteration:  1130      Current loss:  126.50350189208984      Current accuracy:  0.0      Comp_time of one batch:  -0.317185640335083\n",
            "Iteration:  1135      Current loss:  72.15652465820312      Current accuracy:  0.0      Comp_time of one batch:  -0.3106544017791748\n",
            "Iteration:  1140      Current loss:  223.6953125      Current accuracy:  0.0      Comp_time of one batch:  -0.31308531761169434\n",
            "Iteration:  1145      Current loss:  152.4101104736328      Current accuracy:  0.0      Comp_time of one batch:  -0.3166487216949463\n",
            "Iteration:  1150      Current loss:  129.0631866455078      Current accuracy:  0.0      Comp_time of one batch:  -0.3171195983886719\n",
            "Iteration:  1155      Current loss:  119.05918884277344      Current accuracy:  0.0      Comp_time of one batch:  -0.3183624744415283\n",
            "Iteration:  1160      Current loss:  132.1405792236328      Current accuracy:  0.0      Comp_time of one batch:  -0.3109886646270752\n",
            "Iteration:  1165      Current loss:  103.06097412109375      Current accuracy:  0.0      Comp_time of one batch:  -0.31319618225097656\n",
            "Iteration:  1170      Current loss:  128.66978454589844      Current accuracy:  0.0      Comp_time of one batch:  -0.31282854080200195\n",
            "Iteration:  1175      Current loss:  111.65648651123047      Current accuracy:  0.0      Comp_time of one batch:  -0.3639044761657715\n",
            "Iteration:  1180      Current loss:  109.32901763916016      Current accuracy:  0.0      Comp_time of one batch:  -0.3224067687988281\n",
            "Iteration:  1185      Current loss:  108.1240005493164      Current accuracy:  0.0      Comp_time of one batch:  -0.33584022521972656\n",
            "Iteration:  1190      Current loss:  74.76032257080078      Current accuracy:  0.0      Comp_time of one batch:  -0.31467485427856445\n",
            "Iteration:  1195      Current loss:  75.38325500488281      Current accuracy:  0.0      Comp_time of one batch:  -0.3392825126647949\n",
            "Iteration:  1200      Current loss:  147.75811767578125      Current accuracy:  0.0      Comp_time of one batch:  -0.34915733337402344\n",
            "Iteration:  1205      Current loss:  111.42842102050781      Current accuracy:  0.0      Comp_time of one batch:  -0.328754186630249\n",
            "Iteration:  1210      Current loss:  101.52378845214844      Current accuracy:  0.0      Comp_time of one batch:  -0.3356192111968994\n",
            "Iteration:  1215      Current loss:  118.43331909179688      Current accuracy:  0.0      Comp_time of one batch:  -0.31554150581359863\n",
            "Iteration:  1220      Current loss:  115.66132354736328      Current accuracy:  0.0      Comp_time of one batch:  -0.31291723251342773\n",
            "Iteration:  1225      Current loss:  149.58987426757812      Current accuracy:  0.0      Comp_time of one batch:  -0.33553242683410645\n",
            "Iteration:  1230      Current loss:  107.2421646118164      Current accuracy:  0.0      Comp_time of one batch:  -0.34160661697387695\n",
            "Iteration:  1235      Current loss:  141.72007751464844      Current accuracy:  0.0      Comp_time of one batch:  -0.3146250247955322\n",
            "Iteration:  1240      Current loss:  101.89246368408203      Current accuracy:  0.0      Comp_time of one batch:  -0.33615541458129883\n",
            "Iteration:  1245      Current loss:  105.94256591796875      Current accuracy:  0.0      Comp_time of one batch:  -0.33739399909973145\n",
            "Iteration:  1250      Current loss:  110.32723236083984      Current accuracy:  0.0      Comp_time of one batch:  -0.3369598388671875\n",
            "Iteration:  1255      Current loss:  174.18051147460938      Current accuracy:  0.0      Comp_time of one batch:  -0.3161735534667969\n",
            "Iteration:  1260      Current loss:  97.5595932006836      Current accuracy:  0.0      Comp_time of one batch:  -0.3453371524810791\n",
            "Iteration:  1265      Current loss:  136.17005920410156      Current accuracy:  0.0      Comp_time of one batch:  -0.33559703826904297\n",
            "Iteration:  1270      Current loss:  120.7680435180664      Current accuracy:  0.0      Comp_time of one batch:  -0.3308980464935303\n",
            "Iteration:  1275      Current loss:  76.60238647460938      Current accuracy:  0.0      Comp_time of one batch:  -0.3182680606842041\n",
            "Iteration:  1280      Current loss:  99.91179656982422      Current accuracy:  0.0      Comp_time of one batch:  -0.33747172355651855\n",
            "Iteration:  1285      Current loss:  95.43112182617188      Current accuracy:  0.0      Comp_time of one batch:  -0.32198643684387207\n",
            "Iteration:  1290      Current loss:  128.99778747558594      Current accuracy:  0.0      Comp_time of one batch:  -0.31569862365722656\n",
            "Iteration:  1295      Current loss:  166.9824981689453      Current accuracy:  0.0      Comp_time of one batch:  -0.32048606872558594\n",
            "Iteration:  1300      Current loss:  100.44270324707031      Current accuracy:  0.0      Comp_time of one batch:  -0.3254587650299072\n",
            "Iteration:  1305      Current loss:  116.75227355957031      Current accuracy:  0.0      Comp_time of one batch:  -0.3219931125640869\n",
            "Iteration:  1310      Current loss:  125.1145248413086      Current accuracy:  0.0      Comp_time of one batch:  -0.3375072479248047\n",
            "Iteration:  1315      Current loss:  102.8974838256836      Current accuracy:  0.0      Comp_time of one batch:  -0.33492517471313477\n",
            "Iteration:  1320      Current loss:  130.21759033203125      Current accuracy:  0.0      Comp_time of one batch:  -0.3166627883911133\n",
            "Iteration:  1325      Current loss:  129.22369384765625      Current accuracy:  0.0      Comp_time of one batch:  -0.3368666172027588\n",
            "Iteration:  1330      Current loss:  110.60740661621094      Current accuracy:  0.0      Comp_time of one batch:  -0.31540918350219727\n",
            "Iteration:  1335      Current loss:  94.74961853027344      Current accuracy:  0.0      Comp_time of one batch:  -0.31667137145996094\n",
            "Iteration:  1340      Current loss:  106.76982116699219      Current accuracy:  0.0      Comp_time of one batch:  -0.3413674831390381\n",
            "Iteration:  1345      Current loss:  174.35934448242188      Current accuracy:  0.0      Comp_time of one batch:  -0.32072877883911133\n",
            "Iteration:  1350      Current loss:  108.69873809814453      Current accuracy:  0.0      Comp_time of one batch:  -0.31758689880371094\n",
            "Iteration:  1355      Current loss:  107.48094940185547      Current accuracy:  0.0      Comp_time of one batch:  -0.315082311630249\n",
            "Iteration:  1360      Current loss:  125.07707214355469      Current accuracy:  0.0      Comp_time of one batch:  -0.32111692428588867\n",
            "Iteration:  1365      Current loss:  123.97449493408203      Current accuracy:  0.0      Comp_time of one batch:  -0.31522679328918457\n",
            "Iteration:  1370      Current loss:  104.56066131591797      Current accuracy:  0.0      Comp_time of one batch:  -0.3140077590942383\n",
            "Iteration:  1375      Current loss:  145.91189575195312      Current accuracy:  0.0      Comp_time of one batch:  -0.3123900890350342\n",
            "Iteration:  1380      Current loss:  97.14431762695312      Current accuracy:  0.0      Comp_time of one batch:  -0.3158707618713379\n",
            "Iteration:  1385      Current loss:  120.40314483642578      Current accuracy:  0.0      Comp_time of one batch:  -0.3146960735321045\n",
            "Iteration:  1390      Current loss:  138.00860595703125      Current accuracy:  0.0      Comp_time of one batch:  -0.31105732917785645\n",
            "Iteration:  1395      Current loss:  95.50009155273438      Current accuracy:  0.0      Comp_time of one batch:  -0.33824753761291504\n",
            "Iteration:  1400      Current loss:  104.1304931640625      Current accuracy:  0.0      Comp_time of one batch:  -0.3273963928222656\n",
            "Iteration:  1405      Current loss:  112.35044860839844      Current accuracy:  0.0      Comp_time of one batch:  -0.3384888172149658\n",
            "Iteration:  1410      Current loss:  156.53623962402344      Current accuracy:  0.0      Comp_time of one batch:  -0.34343743324279785\n",
            "Iteration:  1415      Current loss:  130.09783935546875      Current accuracy:  0.0      Comp_time of one batch:  -0.3454451560974121\n",
            "Iteration:  1420      Current loss:  137.09515380859375      Current accuracy:  0.0      Comp_time of one batch:  -0.3292059898376465\n",
            "Iteration:  1425      Current loss:  147.0703125      Current accuracy:  0.0      Comp_time of one batch:  -0.326141357421875\n",
            "Iteration:  1430      Current loss:  84.24120330810547      Current accuracy:  0.0      Comp_time of one batch:  -0.3134725093841553\n",
            "Iteration:  1435      Current loss:  118.74301147460938      Current accuracy:  0.0      Comp_time of one batch:  -0.3121809959411621\n",
            "Iteration:  1440      Current loss:  104.38512420654297      Current accuracy:  0.0      Comp_time of one batch:  -0.3208346366882324\n",
            "Iteration:  1445      Current loss:  61.54096984863281      Current accuracy:  0.0      Comp_time of one batch:  -0.3164045810699463\n",
            "Iteration:  1450      Current loss:  93.05885314941406      Current accuracy:  0.0      Comp_time of one batch:  -0.31018829345703125\n",
            "Iteration:  1455      Current loss:  138.6707000732422      Current accuracy:  0.0      Comp_time of one batch:  -0.3336372375488281\n",
            "Iteration:  1460      Current loss:  131.30966186523438      Current accuracy:  0.0      Comp_time of one batch:  -0.32218432426452637\n",
            "Iteration:  1465      Current loss:  79.09172821044922      Current accuracy:  0.0      Comp_time of one batch:  -0.3204975128173828\n",
            "Iteration:  1470      Current loss:  104.5112533569336      Current accuracy:  0.0      Comp_time of one batch:  -0.3297436237335205\n",
            "Iteration:  1475      Current loss:  148.75018310546875      Current accuracy:  0.0      Comp_time of one batch:  -0.3420069217681885\n",
            "Iteration:  1480      Current loss:  95.27156066894531      Current accuracy:  0.0      Comp_time of one batch:  -0.33101463317871094\n",
            "Iteration:  1485      Current loss:  133.71426391601562      Current accuracy:  0.0      Comp_time of one batch:  -0.33054590225219727\n",
            "Iteration:  1490      Current loss:  84.03662109375      Current accuracy:  0.0      Comp_time of one batch:  -0.3211064338684082\n",
            "Iteration:  1495      Current loss:  129.412109375      Current accuracy:  0.0      Comp_time of one batch:  -0.33341097831726074\n",
            "Iteration:  1500      Current loss:  147.1373748779297      Current accuracy:  0.0      Comp_time of one batch:  -0.33055663108825684\n",
            "Iteration:  1505      Current loss:  87.9009017944336      Current accuracy:  0.0      Comp_time of one batch:  -0.3095815181732178\n",
            "Iteration:  1510      Current loss:  91.72362518310547      Current accuracy:  0.0      Comp_time of one batch:  -0.33142542839050293\n",
            "Iteration:  1515      Current loss:  111.73014831542969      Current accuracy:  0.0      Comp_time of one batch:  -0.3269660472869873\n",
            "Iteration:  1520      Current loss:  112.58583068847656      Current accuracy:  0.0      Comp_time of one batch:  -0.3266270160675049\n",
            "Iteration:  1525      Current loss:  128.6484375      Current accuracy:  0.0      Comp_time of one batch:  -0.3440127372741699\n",
            "Iteration:  1530      Current loss:  137.72573852539062      Current accuracy:  0.0      Comp_time of one batch:  -0.33957958221435547\n",
            "Iteration:  1535      Current loss:  86.32745361328125      Current accuracy:  0.0      Comp_time of one batch:  -0.32163548469543457\n",
            "Iteration:  1540      Current loss:  125.75119018554688      Current accuracy:  0.0      Comp_time of one batch:  -0.3309483528137207\n",
            "Iteration:  1545      Current loss:  111.1412353515625      Current accuracy:  0.0      Comp_time of one batch:  -0.3409554958343506\n",
            "Iteration:  1550      Current loss:  107.28900909423828      Current accuracy:  0.0      Comp_time of one batch:  -0.3281679153442383\n",
            "Iteration:  1555      Current loss:  82.4070053100586      Current accuracy:  0.0      Comp_time of one batch:  -0.34450459480285645\n",
            "Iteration:  1560      Current loss:  165.80018615722656      Current accuracy:  0.0      Comp_time of one batch:  -0.35353541374206543\n",
            "Iteration:  1565      Current loss:  100.62934112548828      Current accuracy:  0.0      Comp_time of one batch:  -0.3307220935821533\n",
            "Iteration:  1570      Current loss:  91.89775848388672      Current accuracy:  0.0      Comp_time of one batch:  -0.327054500579834\n",
            "Iteration:  1575      Current loss:  120.23970031738281      Current accuracy:  0.0      Comp_time of one batch:  -0.3206768035888672\n",
            "Iteration:  1580      Current loss:  105.26143646240234      Current accuracy:  0.0      Comp_time of one batch:  -0.3162376880645752\n",
            "Iteration:  1585      Current loss:  79.78782653808594      Current accuracy:  0.0      Comp_time of one batch:  -0.312030553817749\n",
            "Iteration:  1590      Current loss:  169.97027587890625      Current accuracy:  0.0      Comp_time of one batch:  -0.3403942584991455\n",
            "Iteration:  1595      Current loss:  132.00723266601562      Current accuracy:  0.0      Comp_time of one batch:  -0.3228309154510498\n",
            "Iteration:  1600      Current loss:  88.38323211669922      Current accuracy:  0.0      Comp_time of one batch:  -0.3170473575592041\n",
            "Iteration:  1605      Current loss:  143.45501708984375      Current accuracy:  0.0      Comp_time of one batch:  -0.32150745391845703\n",
            "Iteration:  1610      Current loss:  110.80171966552734      Current accuracy:  0.0      Comp_time of one batch:  -0.32056093215942383\n",
            "Iteration:  1615      Current loss:  138.58949279785156      Current accuracy:  0.0      Comp_time of one batch:  -0.3143761157989502\n",
            "Iteration:  1620      Current loss:  129.48390197753906      Current accuracy:  0.0      Comp_time of one batch:  -0.346677303314209\n",
            "Iteration:  1625      Current loss:  89.6220932006836      Current accuracy:  0.0      Comp_time of one batch:  -0.3356618881225586\n",
            "Iteration:  1630      Current loss:  110.23552703857422      Current accuracy:  0.0      Comp_time of one batch:  -0.3159358501434326\n",
            "Iteration:  1635      Current loss:  112.16722869873047      Current accuracy:  0.0      Comp_time of one batch:  -0.33493685722351074\n",
            "Iteration:  1640      Current loss:  103.13532257080078      Current accuracy:  0.0      Comp_time of one batch:  -0.31435155868530273\n",
            "Iteration:  1645      Current loss:  147.7608184814453      Current accuracy:  0.0      Comp_time of one batch:  -0.31444764137268066\n",
            "Iteration:  1650      Current loss:  75.58409118652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3515634536743164\n",
            "Iteration:  1655      Current loss:  106.44076538085938      Current accuracy:  0.0      Comp_time of one batch:  -0.31817078590393066\n",
            "Iteration:  1660      Current loss:  114.73026275634766      Current accuracy:  0.0      Comp_time of one batch:  -0.3181746006011963\n",
            "Iteration:  1665      Current loss:  131.4039306640625      Current accuracy:  0.0      Comp_time of one batch:  -0.3145484924316406\n",
            "Iteration:  1670      Current loss:  72.29645538330078      Current accuracy:  0.0      Comp_time of one batch:  -0.32152819633483887\n",
            "Iteration:  1675      Current loss:  161.38189697265625      Current accuracy:  0.0      Comp_time of one batch:  -0.33109045028686523\n",
            "Iteration:  1680      Current loss:  107.20458984375      Current accuracy:  0.0      Comp_time of one batch:  -0.3488955497741699\n",
            "Iteration:  1685      Current loss:  92.91958618164062      Current accuracy:  0.0      Comp_time of one batch:  -0.32988619804382324\n",
            "Iteration:  1690      Current loss:  115.29826354980469      Current accuracy:  0.0      Comp_time of one batch:  -0.3223304748535156\n",
            "Iteration:  1695      Current loss:  127.86664581298828      Current accuracy:  0.0      Comp_time of one batch:  -0.31865906715393066\n",
            "Iteration:  1700      Current loss:  147.75523376464844      Current accuracy:  0.0      Comp_time of one batch:  -0.34018707275390625\n",
            "Iteration:  1705      Current loss:  145.99081420898438      Current accuracy:  0.0      Comp_time of one batch:  -0.34218716621398926\n",
            "Iteration:  1710      Current loss:  156.3601531982422      Current accuracy:  0.0      Comp_time of one batch:  -0.3283054828643799\n",
            "Iteration:  1715      Current loss:  103.65132904052734      Current accuracy:  0.0      Comp_time of one batch:  -0.3384678363800049\n",
            "Iteration:  1720      Current loss:  114.66047668457031      Current accuracy:  0.0      Comp_time of one batch:  -0.3177490234375\n",
            "Iteration:  1725      Current loss:  121.60294342041016      Current accuracy:  0.0      Comp_time of one batch:  -0.3159356117248535\n",
            "Iteration:  1730      Current loss:  84.77214813232422      Current accuracy:  0.0      Comp_time of one batch:  -0.3365194797515869\n",
            "Iteration:  1735      Current loss:  143.73292541503906      Current accuracy:  0.0      Comp_time of one batch:  -0.3456146717071533\n",
            "Iteration:  1740      Current loss:  174.26174926757812      Current accuracy:  0.0      Comp_time of one batch:  -0.34578800201416016\n",
            "Iteration:  1745      Current loss:  105.73856353759766      Current accuracy:  0.0      Comp_time of one batch:  -0.3423478603363037\n",
            "Iteration:  1750      Current loss:  90.74855041503906      Current accuracy:  0.0      Comp_time of one batch:  -0.3283042907714844\n",
            "Iteration:  1755      Current loss:  146.4866180419922      Current accuracy:  0.0      Comp_time of one batch:  -0.31565046310424805\n",
            "Iteration:  1760      Current loss:  107.6079330444336      Current accuracy:  0.0      Comp_time of one batch:  -0.3390693664550781\n",
            "Iteration:  1765      Current loss:  127.40914154052734      Current accuracy:  0.0      Comp_time of one batch:  -0.3227086067199707\n",
            "Iteration:  1770      Current loss:  144.1103515625      Current accuracy:  0.0      Comp_time of one batch:  -0.34543299674987793\n",
            "Iteration:  1775      Current loss:  134.2020263671875      Current accuracy:  0.0      Comp_time of one batch:  -0.3374793529510498\n",
            "Iteration:  1780      Current loss:  108.56771850585938      Current accuracy:  0.0      Comp_time of one batch:  -0.3295562267303467\n",
            "Iteration:  1785      Current loss:  130.93997192382812      Current accuracy:  0.0      Comp_time of one batch:  -0.31886768341064453\n",
            "Iteration:  1790      Current loss:  70.45707702636719      Current accuracy:  0.0      Comp_time of one batch:  -0.3320128917694092\n",
            "Iteration:  1795      Current loss:  88.04920196533203      Current accuracy:  0.0      Comp_time of one batch:  -0.3408968448638916\n",
            "Iteration:  1800      Current loss:  97.31853485107422      Current accuracy:  0.0      Comp_time of one batch:  -0.32129931449890137\n",
            "Iteration:  1805      Current loss:  112.08858489990234      Current accuracy:  0.0      Comp_time of one batch:  -0.32007288932800293\n",
            "Iteration:  1810      Current loss:  117.98967742919922      Current accuracy:  0.0      Comp_time of one batch:  -0.32285284996032715\n",
            "Iteration:  1815      Current loss:  156.3740692138672      Current accuracy:  0.0      Comp_time of one batch:  -0.3168601989746094\n",
            "Iteration:  1820      Current loss:  99.78961181640625      Current accuracy:  0.0      Comp_time of one batch:  -0.32289743423461914\n",
            "Iteration:  1825      Current loss:  89.44385528564453      Current accuracy:  0.0      Comp_time of one batch:  -0.36650681495666504\n",
            "Iteration:  1830      Current loss:  112.54960632324219      Current accuracy:  0.0      Comp_time of one batch:  -0.34772801399230957\n",
            "Iteration:  1835      Current loss:  130.04100036621094      Current accuracy:  0.0      Comp_time of one batch:  -0.31937122344970703\n",
            "Iteration:  1840      Current loss:  136.2173309326172      Current accuracy:  0.0      Comp_time of one batch:  -0.3177506923675537\n",
            "Iteration:  1845      Current loss:  107.16158294677734      Current accuracy:  0.0      Comp_time of one batch:  -0.3347358703613281\n",
            "Iteration:  1850      Current loss:  140.0113983154297      Current accuracy:  0.0      Comp_time of one batch:  -0.3196549415588379\n",
            "Iteration:  1855      Current loss:  104.98841094970703      Current accuracy:  0.0      Comp_time of one batch:  -0.3138163089752197\n",
            "Iteration:  1860      Current loss:  139.44644165039062      Current accuracy:  0.0      Comp_time of one batch:  -0.34479641914367676\n",
            "Iteration:  1865      Current loss:  87.26367950439453      Current accuracy:  0.0      Comp_time of one batch:  -0.3223276138305664\n",
            "Iteration:  1870      Current loss:  118.58399963378906      Current accuracy:  0.0      Comp_time of one batch:  -0.3195970058441162\n",
            "Iteration:  1875      Current loss:  144.9170684814453      Current accuracy:  0.0      Comp_time of one batch:  -0.3220822811126709\n",
            "Iteration:  1880      Current loss:  86.93001556396484      Current accuracy:  0.0      Comp_time of one batch:  -0.317929744720459\n",
            "Iteration:  1885      Current loss:  95.20260620117188      Current accuracy:  0.0      Comp_time of one batch:  -0.33989763259887695\n",
            "Iteration:  1890      Current loss:  113.32276916503906      Current accuracy:  0.0      Comp_time of one batch:  -0.3135986328125\n",
            "Iteration:  1895      Current loss:  106.5351791381836      Current accuracy:  0.0      Comp_time of one batch:  -0.3191263675689697\n",
            "Iteration:  1900      Current loss:  81.8132095336914      Current accuracy:  0.0      Comp_time of one batch:  -0.3154881000518799\n",
            "Iteration:  1905      Current loss:  158.37266540527344      Current accuracy:  0.0      Comp_time of one batch:  -0.3130166530609131\n",
            "Iteration:  1910      Current loss:  115.15802764892578      Current accuracy:  0.0      Comp_time of one batch:  -0.32215070724487305\n",
            "Iteration:  1915      Current loss:  120.8968505859375      Current accuracy:  0.0      Comp_time of one batch:  -0.3165445327758789\n",
            "Iteration:  1920      Current loss:  142.11473083496094      Current accuracy:  0.0      Comp_time of one batch:  -0.319136381149292\n",
            "Iteration:  1925      Current loss:  90.3554916381836      Current accuracy:  0.0      Comp_time of one batch:  -0.3280789852142334\n",
            "Iteration:  1930      Current loss:  135.3055877685547      Current accuracy:  0.0      Comp_time of one batch:  -0.3288278579711914\n",
            "Iteration:  1935      Current loss:  124.71688079833984      Current accuracy:  0.0      Comp_time of one batch:  -0.34955692291259766\n",
            "Iteration:  1940      Current loss:  128.68992614746094      Current accuracy:  0.0      Comp_time of one batch:  -0.32199621200561523\n",
            "Iteration:  1945      Current loss:  69.37996673583984      Current accuracy:  0.0      Comp_time of one batch:  -0.32271528244018555\n",
            "Iteration:  1950      Current loss:  148.96551513671875      Current accuracy:  0.0      Comp_time of one batch:  -0.32935070991516113\n",
            "Iteration:  1955      Current loss:  92.32211303710938      Current accuracy:  0.0      Comp_time of one batch:  -0.3379507064819336\n",
            "Iteration:  1960      Current loss:  139.01177978515625      Current accuracy:  0.0      Comp_time of one batch:  -0.3153064250946045\n",
            "Iteration:  1965      Current loss:  143.26222229003906      Current accuracy:  0.0      Comp_time of one batch:  -0.3162651062011719\n",
            "Iteration:  1970      Current loss:  101.45040893554688      Current accuracy:  0.0      Comp_time of one batch:  -0.3220686912536621\n",
            "Iteration:  1975      Current loss:  108.16621398925781      Current accuracy:  0.0      Comp_time of one batch:  -0.3177664279937744\n",
            "Iteration:  1980      Current loss:  109.1457748413086      Current accuracy:  0.0      Comp_time of one batch:  -0.3165404796600342\n",
            "Iteration:  1985      Current loss:  146.47129821777344      Current accuracy:  0.0      Comp_time of one batch:  -0.3209657669067383\n",
            "Iteration:  1990      Current loss:  86.25334930419922      Current accuracy:  0.0      Comp_time of one batch:  -0.3238050937652588\n",
            "Iteration:  1995      Current loss:  161.20645141601562      Current accuracy:  0.0      Comp_time of one batch:  -0.3158695697784424\n",
            "Iteration:  2000      Current loss:  108.72049713134766      Current accuracy:  0.0      Comp_time of one batch:  -0.3378164768218994\n",
            "Iteration:  2005      Current loss:  115.69633483886719      Current accuracy:  0.0      Comp_time of one batch:  -0.31539011001586914\n",
            "Iteration:  2010      Current loss:  82.82616424560547      Current accuracy:  0.0      Comp_time of one batch:  -0.3223145008087158\n",
            "Iteration:  2015      Current loss:  85.9906234741211      Current accuracy:  0.0      Comp_time of one batch:  -0.3325378894805908\n",
            "Iteration:  2020      Current loss:  100.55656433105469      Current accuracy:  0.0      Comp_time of one batch:  -0.3215816020965576\n",
            "Iteration:  2025      Current loss:  145.83038330078125      Current accuracy:  0.0      Comp_time of one batch:  -0.3123183250427246\n",
            "Iteration:  2030      Current loss:  67.87759399414062      Current accuracy:  0.0      Comp_time of one batch:  -0.33350491523742676\n",
            "Iteration:  2035      Current loss:  122.59724426269531      Current accuracy:  0.0      Comp_time of one batch:  -0.33724212646484375\n",
            "Iteration:  2040      Current loss:  118.02530670166016      Current accuracy:  0.0      Comp_time of one batch:  -0.34660935401916504\n",
            "Iteration:  2045      Current loss:  72.2117691040039      Current accuracy:  0.0      Comp_time of one batch:  -0.30974435806274414\n",
            "Iteration:  2050      Current loss:  104.80870056152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3349447250366211\n",
            "Iteration:  2055      Current loss:  107.60108184814453      Current accuracy:  0.0      Comp_time of one batch:  -0.3213036060333252\n",
            "Iteration:  2060      Current loss:  129.99514770507812      Current accuracy:  0.0      Comp_time of one batch:  -0.32094502449035645\n",
            "Iteration:  2065      Current loss:  147.95323181152344      Current accuracy:  0.0      Comp_time of one batch:  -0.3210408687591553\n",
            "Iteration:  2070      Current loss:  152.8112335205078      Current accuracy:  0.0      Comp_time of one batch:  -0.344696044921875\n",
            "Iteration:  2075      Current loss:  134.60260009765625      Current accuracy:  0.0      Comp_time of one batch:  -0.35155248641967773\n",
            "Iteration:  2080      Current loss:  114.31391143798828      Current accuracy:  0.0      Comp_time of one batch:  -0.32993030548095703\n",
            "Iteration:  2085      Current loss:  124.47695922851562      Current accuracy:  0.0      Comp_time of one batch:  -0.31081676483154297\n",
            "Iteration:  2090      Current loss:  90.21827697753906      Current accuracy:  0.0      Comp_time of one batch:  -0.3432140350341797\n",
            "Iteration:  2095      Current loss:  136.2825469970703      Current accuracy:  0.0      Comp_time of one batch:  -0.3198511600494385\n",
            "Iteration:  2100      Current loss:  87.03242492675781      Current accuracy:  0.0      Comp_time of one batch:  -0.314227819442749\n",
            "Iteration:  2105      Current loss:  89.01038360595703      Current accuracy:  0.0      Comp_time of one batch:  -0.3326880931854248\n",
            "Iteration:  2110      Current loss:  84.5261459350586      Current accuracy:  0.0      Comp_time of one batch:  -0.3497955799102783\n",
            "Iteration:  2115      Current loss:  88.00160217285156      Current accuracy:  0.0      Comp_time of one batch:  -0.31125330924987793\n",
            "Iteration:  2120      Current loss:  78.08960723876953      Current accuracy:  0.0      Comp_time of one batch:  -0.31165051460266113\n",
            "Iteration:  2125      Current loss:  120.28348541259766      Current accuracy:  0.0      Comp_time of one batch:  -0.33201169967651367\n",
            "Iteration:  2130      Current loss:  101.8487319946289      Current accuracy:  0.0      Comp_time of one batch:  -0.36076927185058594\n",
            "Iteration:  2135      Current loss:  108.83963012695312      Current accuracy:  0.0      Comp_time of one batch:  -0.31842494010925293\n",
            "Iteration:  2140      Current loss:  80.58546447753906      Current accuracy:  0.0      Comp_time of one batch:  -0.33636021614074707\n",
            "Iteration:  2145      Current loss:  103.0649642944336      Current accuracy:  0.0      Comp_time of one batch:  -0.33521437644958496\n",
            "Iteration:  2150      Current loss:  117.82392120361328      Current accuracy:  0.0      Comp_time of one batch:  -0.332672119140625\n",
            "Iteration:  2155      Current loss:  156.94577026367188      Current accuracy:  0.0      Comp_time of one batch:  -0.32319211959838867\n",
            "Iteration:  2160      Current loss:  130.04400634765625      Current accuracy:  0.0      Comp_time of one batch:  -0.312497615814209\n",
            "Iteration:  2165      Current loss:  102.27172088623047      Current accuracy:  0.0      Comp_time of one batch:  -0.3226349353790283\n",
            "Iteration:  2170      Current loss:  71.39048767089844      Current accuracy:  0.0      Comp_time of one batch:  -0.31729936599731445\n",
            "Iteration:  2175      Current loss:  143.25148010253906      Current accuracy:  0.0      Comp_time of one batch:  -0.3179178237915039\n",
            "Iteration:  2180      Current loss:  90.37767791748047      Current accuracy:  0.0      Comp_time of one batch:  -0.3204038143157959\n",
            "Iteration:  2185      Current loss:  134.09519958496094      Current accuracy:  0.0      Comp_time of one batch:  -0.3170151710510254\n",
            "Iteration:  2190      Current loss:  118.63028717041016      Current accuracy:  0.0      Comp_time of one batch:  -0.3629941940307617\n",
            "Iteration:  2195      Current loss:  143.02597045898438      Current accuracy:  0.0      Comp_time of one batch:  -0.31732726097106934\n",
            "Iteration:  2200      Current loss:  129.0348663330078      Current accuracy:  0.0      Comp_time of one batch:  -0.3175694942474365\n",
            "Iteration:  2205      Current loss:  95.78331756591797      Current accuracy:  0.0      Comp_time of one batch:  -0.33312439918518066\n",
            "Iteration:  2210      Current loss:  130.7071533203125      Current accuracy:  0.0      Comp_time of one batch:  -0.3314943313598633\n",
            "Iteration:  2215      Current loss:  117.71559143066406      Current accuracy:  0.0      Comp_time of one batch:  -0.33081984519958496\n",
            "Iteration:  2220      Current loss:  138.44842529296875      Current accuracy:  0.0      Comp_time of one batch:  -0.3200509548187256\n",
            "Iteration:  2225      Current loss:  150.73574829101562      Current accuracy:  0.0      Comp_time of one batch:  -0.33043527603149414\n",
            "Iteration:  2230      Current loss:  117.27134704589844      Current accuracy:  0.0      Comp_time of one batch:  -0.3285849094390869\n",
            "Iteration:  2235      Current loss:  171.538330078125      Current accuracy:  0.0      Comp_time of one batch:  -0.3409843444824219\n",
            "Iteration:  2240      Current loss:  105.98405456542969      Current accuracy:  0.0      Comp_time of one batch:  -0.32216811180114746\n",
            "Iteration:  2245      Current loss:  143.29820251464844      Current accuracy:  0.0      Comp_time of one batch:  -0.31156373023986816\n",
            "Iteration:  2250      Current loss:  129.90399169921875      Current accuracy:  0.0      Comp_time of one batch:  -0.34324049949645996\n",
            "Iteration:  2255      Current loss:  112.92371368408203      Current accuracy:  0.0      Comp_time of one batch:  -0.32436299324035645\n",
            "Iteration:  2260      Current loss:  136.57650756835938      Current accuracy:  0.0      Comp_time of one batch:  -0.313976526260376\n",
            "Iteration:  2265      Current loss:  89.70812225341797      Current accuracy:  0.0      Comp_time of one batch:  -0.3200511932373047\n",
            "Iteration:  2270      Current loss:  105.65243530273438      Current accuracy:  0.0      Comp_time of one batch:  -0.3202676773071289\n",
            "Iteration:  2275      Current loss:  168.3440399169922      Current accuracy:  0.0      Comp_time of one batch:  -0.31283020973205566\n",
            "Iteration:  2280      Current loss:  100.04422760009766      Current accuracy:  0.0      Comp_time of one batch:  -0.34746527671813965\n",
            "Iteration:  2285      Current loss:  126.50223541259766      Current accuracy:  0.0      Comp_time of one batch:  -0.3346545696258545\n",
            "Iteration:  2290      Current loss:  113.1241226196289      Current accuracy:  0.0      Comp_time of one batch:  -0.3111536502838135\n",
            "Iteration:  2295      Current loss:  138.85003662109375      Current accuracy:  0.0      Comp_time of one batch:  -0.35166096687316895\n",
            "Iteration:  2300      Current loss:  86.88228607177734      Current accuracy:  0.0      Comp_time of one batch:  -0.33147192001342773\n",
            "Iteration:  2305      Current loss:  105.97613525390625      Current accuracy:  0.0      Comp_time of one batch:  -0.3337125778198242\n",
            "Iteration:  2310      Current loss:  106.73625946044922      Current accuracy:  0.0      Comp_time of one batch:  -0.3521158695220947\n",
            "Iteration:  2315      Current loss:  126.18440246582031      Current accuracy:  0.0      Comp_time of one batch:  -0.3370020389556885\n",
            "Iteration:  2320      Current loss:  146.85035705566406      Current accuracy:  0.0      Comp_time of one batch:  -0.3268451690673828\n",
            "Iteration:  2325      Current loss:  86.52383422851562      Current accuracy:  0.0      Comp_time of one batch:  -0.3205912113189697\n",
            "Iteration:  2330      Current loss:  79.2081298828125      Current accuracy:  0.0      Comp_time of one batch:  -0.3180558681488037\n",
            "Iteration:  2335      Current loss:  70.90607452392578      Current accuracy:  0.0      Comp_time of one batch:  -0.31093454360961914\n",
            "Iteration:  2340      Current loss:  67.93228149414062      Current accuracy:  0.0      Comp_time of one batch:  -0.3233063220977783\n",
            "Iteration:  2345      Current loss:  66.84171295166016      Current accuracy:  0.0      Comp_time of one batch:  -0.3120455741882324\n",
            "Iteration:  2350      Current loss:  157.5572509765625      Current accuracy:  0.0      Comp_time of one batch:  -0.325314998626709\n",
            "Iteration:  2355      Current loss:  96.6531982421875      Current accuracy:  0.0      Comp_time of one batch:  -0.3267486095428467\n",
            "Iteration:  2360      Current loss:  85.45490264892578      Current accuracy:  0.0      Comp_time of one batch:  -0.3120901584625244\n",
            "Iteration:  2365      Current loss:  159.28407287597656      Current accuracy:  0.0      Comp_time of one batch:  -0.318347692489624\n",
            "Iteration:  2370      Current loss:  119.66317749023438      Current accuracy:  0.0      Comp_time of one batch:  -0.3373425006866455\n",
            "Iteration:  2375      Current loss:  88.19409942626953      Current accuracy:  0.0      Comp_time of one batch:  -0.31615328788757324\n",
            "Iteration:  2380      Current loss:  111.48585510253906      Current accuracy:  0.0      Comp_time of one batch:  -0.3401777744293213\n",
            "Iteration:  2385      Current loss:  151.3331756591797      Current accuracy:  0.0      Comp_time of one batch:  -0.3143641948699951\n",
            "Iteration:  2390      Current loss:  72.6410140991211      Current accuracy:  0.0      Comp_time of one batch:  -0.31711530685424805\n",
            "Iteration:  2395      Current loss:  83.58723449707031      Current accuracy:  0.0      Comp_time of one batch:  -0.3107125759124756\n",
            "Iteration:  2400      Current loss:  138.9062957763672      Current accuracy:  0.0      Comp_time of one batch:  -0.3311896324157715\n",
            "Iteration:  2405      Current loss:  111.3659439086914      Current accuracy:  0.0      Comp_time of one batch:  -0.35124826431274414\n",
            "Iteration:  2410      Current loss:  114.74658966064453      Current accuracy:  0.0      Comp_time of one batch:  -0.3077964782714844\n",
            "Iteration:  2415      Current loss:  75.87588500976562      Current accuracy:  0.0      Comp_time of one batch:  -0.34979987144470215\n",
            "Iteration:  2420      Current loss:  122.60447692871094      Current accuracy:  0.0      Comp_time of one batch:  -0.3127269744873047\n",
            "Iteration:  2425      Current loss:  190.36534118652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3099253177642822\n",
            "Iteration:  2430      Current loss:  180.3358612060547      Current accuracy:  0.0      Comp_time of one batch:  -0.3512229919433594\n",
            "Iteration:  2435      Current loss:  131.60565185546875      Current accuracy:  0.0      Comp_time of one batch:  -0.3356485366821289\n",
            "Iteration:  2440      Current loss:  110.03067016601562      Current accuracy:  0.0      Comp_time of one batch:  -0.31569385528564453\n",
            "Iteration:  2445      Current loss:  164.02488708496094      Current accuracy:  0.0      Comp_time of one batch:  -0.3125443458557129\n",
            "Iteration:  2450      Current loss:  132.95602416992188      Current accuracy:  0.0      Comp_time of one batch:  -0.3242642879486084\n",
            "Iteration:  2455      Current loss:  105.64286804199219      Current accuracy:  0.0      Comp_time of one batch:  -0.3376171588897705\n",
            "Iteration:  2460      Current loss:  119.8219223022461      Current accuracy:  0.0      Comp_time of one batch:  -0.337618350982666\n",
            "Iteration:  2465      Current loss:  75.26251220703125      Current accuracy:  0.0      Comp_time of one batch:  -0.33725905418395996\n",
            "Iteration:  2470      Current loss:  119.89362335205078      Current accuracy:  0.0      Comp_time of one batch:  -0.34250831604003906\n",
            "Iteration:  2475      Current loss:  116.33622741699219      Current accuracy:  0.0      Comp_time of one batch:  -0.31947803497314453\n",
            "Iteration:  2480      Current loss:  82.49315643310547      Current accuracy:  0.0      Comp_time of one batch:  -0.31593871116638184\n",
            "Iteration:  2485      Current loss:  104.57659149169922      Current accuracy:  0.0      Comp_time of one batch:  -0.31537866592407227\n",
            "Iteration:  2490      Current loss:  119.76911163330078      Current accuracy:  0.0      Comp_time of one batch:  -0.3123173713684082\n",
            "Iteration:  2495      Current loss:  97.6076431274414      Current accuracy:  0.0      Comp_time of one batch:  -0.31248021125793457\n",
            "Iteration:  2500      Current loss:  93.11579132080078      Current accuracy:  0.0      Comp_time of one batch:  -0.32263994216918945\n",
            "Iteration:  2505      Current loss:  104.78203582763672      Current accuracy:  0.0      Comp_time of one batch:  -0.3135828971862793\n",
            "Iteration:  2510      Current loss:  114.75521087646484      Current accuracy:  0.0      Comp_time of one batch:  -0.32511210441589355\n",
            "Iteration:  2515      Current loss:  125.76998138427734      Current accuracy:  0.0      Comp_time of one batch:  -0.3159463405609131\n",
            "Iteration:  2520      Current loss:  116.7924575805664      Current accuracy:  0.0      Comp_time of one batch:  -0.31484150886535645\n",
            "Iteration:  2525      Current loss:  92.72412109375      Current accuracy:  0.0      Comp_time of one batch:  -0.325488805770874\n",
            "Iteration:  2530      Current loss:  123.7852783203125      Current accuracy:  0.0      Comp_time of one batch:  -0.317305326461792\n",
            "Iteration:  2535      Current loss:  118.7767333984375      Current accuracy:  0.0      Comp_time of one batch:  -0.3160586357116699\n",
            "Iteration:  2540      Current loss:  102.25025939941406      Current accuracy:  0.0      Comp_time of one batch:  -0.33007264137268066\n",
            "Iteration:  2545      Current loss:  118.07196044921875      Current accuracy:  0.0      Comp_time of one batch:  -0.31896328926086426\n",
            "Iteration:  2550      Current loss:  113.15889739990234      Current accuracy:  0.0      Comp_time of one batch:  -0.3177917003631592\n",
            "Iteration:  2555      Current loss:  69.9626235961914      Current accuracy:  0.0      Comp_time of one batch:  -0.32082605361938477\n",
            "Iteration:  2560      Current loss:  85.77531433105469      Current accuracy:  0.0      Comp_time of one batch:  -0.33478498458862305\n",
            "Iteration:  2565      Current loss:  84.85657501220703      Current accuracy:  0.0      Comp_time of one batch:  -0.33562707901000977\n",
            "Iteration:  2570      Current loss:  137.80404663085938      Current accuracy:  0.0      Comp_time of one batch:  -0.32348179817199707\n",
            "Iteration:  2575      Current loss:  137.76034545898438      Current accuracy:  0.0      Comp_time of one batch:  -0.3210935592651367\n",
            "Iteration:  2580      Current loss:  109.26641845703125      Current accuracy:  0.0      Comp_time of one batch:  -0.33391666412353516\n",
            "Iteration:  2585      Current loss:  119.08173370361328      Current accuracy:  0.0      Comp_time of one batch:  -0.3169889450073242\n",
            "Iteration:  2590      Current loss:  120.90730285644531      Current accuracy:  0.0      Comp_time of one batch:  -0.32573461532592773\n",
            "Iteration:  2595      Current loss:  137.30152893066406      Current accuracy:  0.0      Comp_time of one batch:  -0.3226349353790283\n",
            "Iteration:  2600      Current loss:  96.23578643798828      Current accuracy:  0.0      Comp_time of one batch:  -0.3455333709716797\n",
            "Iteration:  2605      Current loss:  110.6579360961914      Current accuracy:  0.0      Comp_time of one batch:  -0.31612396240234375\n",
            "Iteration:  2610      Current loss:  116.92619323730469      Current accuracy:  0.0      Comp_time of one batch:  -0.3168525695800781\n",
            "Iteration:  2615      Current loss:  105.31158447265625      Current accuracy:  0.0      Comp_time of one batch:  -0.3271293640136719\n",
            "Iteration:  2620      Current loss:  129.3558807373047      Current accuracy:  0.0      Comp_time of one batch:  -0.31315159797668457\n",
            "Iteration:  2625      Current loss:  122.29076385498047      Current accuracy:  0.0      Comp_time of one batch:  -0.3101930618286133\n",
            "Iteration:  2630      Current loss:  91.12238311767578      Current accuracy:  0.0      Comp_time of one batch:  -0.3349308967590332\n",
            "Iteration:  2635      Current loss:  87.45311737060547      Current accuracy:  0.0      Comp_time of one batch:  -0.33790111541748047\n",
            "Iteration:  2640      Current loss:  121.25343322753906      Current accuracy:  0.0      Comp_time of one batch:  -0.34107494354248047\n",
            "Iteration:  2645      Current loss:  121.00001525878906      Current accuracy:  0.0      Comp_time of one batch:  -0.35658717155456543\n",
            "Iteration:  2650      Current loss:  113.21704864501953      Current accuracy:  0.0      Comp_time of one batch:  -0.3465240001678467\n",
            "Iteration:  2655      Current loss:  78.84487915039062      Current accuracy:  0.0      Comp_time of one batch:  -0.3264737129211426\n",
            "Iteration:  2660      Current loss:  105.0980224609375      Current accuracy:  0.0      Comp_time of one batch:  -0.33507394790649414\n",
            "Iteration:  2665      Current loss:  115.76744842529297      Current accuracy:  0.0      Comp_time of one batch:  -0.3121466636657715\n",
            "Iteration:  2670      Current loss:  100.76387023925781      Current accuracy:  0.0      Comp_time of one batch:  -0.32057642936706543\n",
            "Iteration:  2675      Current loss:  127.8346176147461      Current accuracy:  0.0      Comp_time of one batch:  -0.36307239532470703\n",
            "Iteration:  2680      Current loss:  97.8010482788086      Current accuracy:  0.0      Comp_time of one batch:  -0.3188347816467285\n",
            "Iteration:  2685      Current loss:  122.00424194335938      Current accuracy:  0.0      Comp_time of one batch:  -0.3264775276184082\n",
            "Iteration:  2690      Current loss:  73.1475830078125      Current accuracy:  0.0      Comp_time of one batch:  -0.34647369384765625\n",
            "Iteration:  2695      Current loss:  68.998291015625      Current accuracy:  0.0      Comp_time of one batch:  -0.31658363342285156\n",
            "Iteration:  2700      Current loss:  92.18327331542969      Current accuracy:  0.0      Comp_time of one batch:  -0.3298676013946533\n",
            "Iteration:  2705      Current loss:  105.73320770263672      Current accuracy:  0.0      Comp_time of one batch:  -0.3422884941101074\n",
            "Iteration:  2710      Current loss:  118.41734313964844      Current accuracy:  0.0      Comp_time of one batch:  -0.3180229663848877\n",
            "Iteration:  2715      Current loss:  140.00486755371094      Current accuracy:  0.0      Comp_time of one batch:  -0.32955336570739746\n",
            "Iteration:  2720      Current loss:  103.71566772460938      Current accuracy:  0.0      Comp_time of one batch:  -0.34014105796813965\n",
            "Iteration:  2725      Current loss:  125.97655487060547      Current accuracy:  0.0      Comp_time of one batch:  -0.31693577766418457\n",
            "Iteration:  2730      Current loss:  70.66598510742188      Current accuracy:  0.0      Comp_time of one batch:  -0.3385157585144043\n",
            "Iteration:  2735      Current loss:  111.10061645507812      Current accuracy:  0.0      Comp_time of one batch:  -0.3587534427642822\n",
            "Iteration:  2740      Current loss:  105.96951293945312      Current accuracy:  0.0      Comp_time of one batch:  -0.3229091167449951\n",
            "Iteration:  2745      Current loss:  136.2903594970703      Current accuracy:  0.0      Comp_time of one batch:  -0.33190226554870605\n",
            "Iteration:  2750      Current loss:  134.48081970214844      Current accuracy:  0.0      Comp_time of one batch:  -0.34349560737609863\n",
            "Iteration:  2755      Current loss:  143.41090393066406      Current accuracy:  0.0      Comp_time of one batch:  -0.316464900970459\n",
            "Iteration:  2760      Current loss:  127.71768188476562      Current accuracy:  0.0      Comp_time of one batch:  -0.32566356658935547\n",
            "Iteration:  2765      Current loss:  107.76203918457031      Current accuracy:  0.0      Comp_time of one batch:  -0.340130090713501\n",
            "Iteration:  2770      Current loss:  123.8251724243164      Current accuracy:  0.0      Comp_time of one batch:  -0.325960636138916\n",
            "Iteration:  2775      Current loss:  103.81964111328125      Current accuracy:  0.0      Comp_time of one batch:  -0.3158848285675049\n",
            "Iteration:  2780      Current loss:  70.11587524414062      Current accuracy:  0.0      Comp_time of one batch:  -0.33380842208862305\n",
            "Iteration:  2785      Current loss:  166.3084259033203      Current accuracy:  0.0      Comp_time of one batch:  -0.3315112590789795\n",
            "Iteration:  2790      Current loss:  93.27571105957031      Current accuracy:  0.0      Comp_time of one batch:  -0.33175015449523926\n",
            "Iteration:  2795      Current loss:  63.15839767456055      Current accuracy:  0.0      Comp_time of one batch:  -0.3205909729003906\n",
            "Iteration:  2800      Current loss:  117.06503295898438      Current accuracy:  0.0      Comp_time of one batch:  -0.33301424980163574\n",
            "Iteration:  2805      Current loss:  143.2671356201172      Current accuracy:  0.0      Comp_time of one batch:  -0.33037495613098145\n",
            "Iteration:  2810      Current loss:  76.19845581054688      Current accuracy:  0.0      Comp_time of one batch:  -0.34419870376586914\n",
            "Iteration:  2815      Current loss:  156.4276123046875      Current accuracy:  0.0      Comp_time of one batch:  -0.31494784355163574\n",
            "Iteration:  2820      Current loss:  86.03922271728516      Current accuracy:  0.0      Comp_time of one batch:  -0.3202178478240967\n",
            "Iteration:  2825      Current loss:  115.6236343383789      Current accuracy:  0.0      Comp_time of one batch:  -0.33623838424682617\n",
            "Iteration:  2830      Current loss:  87.92010498046875      Current accuracy:  0.0      Comp_time of one batch:  -0.31020498275756836\n",
            "Iteration:  2835      Current loss:  103.84796905517578      Current accuracy:  0.0      Comp_time of one batch:  -0.31272006034851074\n",
            "Iteration:  2840      Current loss:  106.17105102539062      Current accuracy:  0.0      Comp_time of one batch:  -0.3368494510650635\n",
            "Iteration:  2845      Current loss:  114.70372772216797      Current accuracy:  0.0      Comp_time of one batch:  -0.32157230377197266\n",
            "Iteration:  2850      Current loss:  95.85812377929688      Current accuracy:  0.0      Comp_time of one batch:  -0.3202071189880371\n",
            "Iteration:  2855      Current loss:  141.73951721191406      Current accuracy:  0.0      Comp_time of one batch:  -0.3213307857513428\n",
            "Iteration:  2860      Current loss:  153.5077667236328      Current accuracy:  0.0      Comp_time of one batch:  -0.3242824077606201\n",
            "Iteration:  2865      Current loss:  99.72212219238281      Current accuracy:  0.0      Comp_time of one batch:  -0.3190786838531494\n",
            "Iteration:  2870      Current loss:  104.07305908203125      Current accuracy:  0.0      Comp_time of one batch:  -0.3431985378265381\n",
            "Iteration:  2875      Current loss:  162.5695037841797      Current accuracy:  0.0      Comp_time of one batch:  -0.3414132595062256\n",
            "Iteration:  2880      Current loss:  118.02075958251953      Current accuracy:  0.0      Comp_time of one batch:  -0.3111116886138916\n",
            "Iteration:  2885      Current loss:  89.40201568603516      Current accuracy:  0.0      Comp_time of one batch:  -0.3337218761444092\n",
            "Iteration:  2890      Current loss:  98.56946563720703      Current accuracy:  0.0      Comp_time of one batch:  -0.3413097858428955\n",
            "Iteration:  2895      Current loss:  126.66873931884766      Current accuracy:  0.0      Comp_time of one batch:  -0.31647157669067383\n",
            "Iteration:  2900      Current loss:  84.19071960449219      Current accuracy:  0.0      Comp_time of one batch:  -0.3150351047515869\n",
            "Iteration:  2905      Current loss:  111.20657348632812      Current accuracy:  0.0      Comp_time of one batch:  -0.3216111660003662\n",
            "Iteration:  2910      Current loss:  107.19900512695312      Current accuracy:  0.0      Comp_time of one batch:  -0.371290922164917\n",
            "Iteration:  2915      Current loss:  136.17031860351562      Current accuracy:  0.0      Comp_time of one batch:  -0.3526153564453125\n",
            "Iteration:  2920      Current loss:  121.88848114013672      Current accuracy:  0.0      Comp_time of one batch:  -0.32671308517456055\n",
            "Iteration:  2925      Current loss:  74.09747314453125      Current accuracy:  0.0      Comp_time of one batch:  -0.33185601234436035\n",
            "Iteration:  2930      Current loss:  107.56512451171875      Current accuracy:  0.0      Comp_time of one batch:  -0.33620476722717285\n",
            "Iteration:  2935      Current loss:  87.52617645263672      Current accuracy:  0.0      Comp_time of one batch:  -0.319934606552124\n",
            "Iteration:  2940      Current loss:  120.96674346923828      Current accuracy:  0.0      Comp_time of one batch:  -0.3305234909057617\n",
            "Iteration:  2945      Current loss:  143.47891235351562      Current accuracy:  0.0      Comp_time of one batch:  -0.334139347076416\n",
            "Iteration:  2950      Current loss:  99.72876739501953      Current accuracy:  0.0      Comp_time of one batch:  -0.316072940826416\n",
            "Iteration:  2955      Current loss:  91.24813079833984      Current accuracy:  0.0      Comp_time of one batch:  -0.33905911445617676\n",
            "Iteration:  2960      Current loss:  112.55517578125      Current accuracy:  0.0      Comp_time of one batch:  -0.3116486072540283\n",
            "Iteration:  2965      Current loss:  115.00891876220703      Current accuracy:  0.0      Comp_time of one batch:  -0.31245946884155273\n",
            "Iteration:  2970      Current loss:  98.48104858398438      Current accuracy:  0.0      Comp_time of one batch:  -0.3497314453125\n",
            "Iteration:  2975      Current loss:  152.6497039794922      Current accuracy:  0.0      Comp_time of one batch:  -0.32285475730895996\n",
            "Iteration:  2980      Current loss:  120.33985900878906      Current accuracy:  0.0      Comp_time of one batch:  -0.3237419128417969\n",
            "Iteration:  2985      Current loss:  61.714908599853516      Current accuracy:  0.0      Comp_time of one batch:  -0.3252527713775635\n",
            "Iteration:  2990      Current loss:  115.20228576660156      Current accuracy:  0.0      Comp_time of one batch:  -0.3294968605041504\n",
            "Iteration:  2995      Current loss:  97.88349151611328      Current accuracy:  0.0      Comp_time of one batch:  -0.32657480239868164\n",
            "Iteration:  3000      Current loss:  140.508544921875      Current accuracy:  0.0      Comp_time of one batch:  -0.36047935485839844\n",
            "Iteration:  3005      Current loss:  132.05528259277344      Current accuracy:  0.0      Comp_time of one batch:  -0.3296530246734619\n",
            "Iteration:  3010      Current loss:  112.20817565917969      Current accuracy:  0.0      Comp_time of one batch:  -0.3341946601867676\n",
            "Iteration:  3015      Current loss:  55.0705680847168      Current accuracy:  0.0      Comp_time of one batch:  -0.33345770835876465\n",
            "Iteration:  3020      Current loss:  64.33074951171875      Current accuracy:  0.0      Comp_time of one batch:  -0.31954216957092285\n",
            "Iteration:  3025      Current loss:  66.55281066894531      Current accuracy:  0.0      Comp_time of one batch:  -0.32518458366394043\n",
            "Iteration:  3030      Current loss:  148.28321838378906      Current accuracy:  0.0      Comp_time of one batch:  -0.3517289161682129\n",
            "Iteration:  3035      Current loss:  79.93292236328125      Current accuracy:  0.0      Comp_time of one batch:  -0.3125474452972412\n",
            "Iteration:  3040      Current loss:  95.47306060791016      Current accuracy:  0.0      Comp_time of one batch:  -0.3150458335876465\n",
            "Iteration:  3045      Current loss:  90.56392669677734      Current accuracy:  0.0      Comp_time of one batch:  -0.3391225337982178\n",
            "Iteration:  3050      Current loss:  119.5003433227539      Current accuracy:  0.0      Comp_time of one batch:  -0.3388824462890625\n",
            "Iteration:  3055      Current loss:  144.8075714111328      Current accuracy:  0.0      Comp_time of one batch:  -0.31769585609436035\n",
            "Iteration:  3060      Current loss:  105.39456176757812      Current accuracy:  0.0      Comp_time of one batch:  -0.32116055488586426\n",
            "Iteration:  3065      Current loss:  107.18184661865234      Current accuracy:  0.0      Comp_time of one batch:  -0.31322574615478516\n",
            "Iteration:  3070      Current loss:  130.0028533935547      Current accuracy:  0.0      Comp_time of one batch:  -0.3086421489715576\n",
            "Iteration:  3075      Current loss:  105.2035903930664      Current accuracy:  0.0      Comp_time of one batch:  -0.31394243240356445\n",
            "Iteration:  3080      Current loss:  103.68368530273438      Current accuracy:  0.0      Comp_time of one batch:  -0.3287482261657715\n",
            "Iteration:  3085      Current loss:  90.23056030273438      Current accuracy:  0.0      Comp_time of one batch:  -0.3198521137237549\n",
            "Iteration:  3090      Current loss:  161.9478302001953      Current accuracy:  0.0      Comp_time of one batch:  -0.34905004501342773\n",
            "Iteration:  3095      Current loss:  110.92771911621094      Current accuracy:  0.0      Comp_time of one batch:  -0.3150756359100342\n",
            "Iteration:  3100      Current loss:  157.9534149169922      Current accuracy:  0.0      Comp_time of one batch:  -0.3118317127227783\n",
            "Iteration:  3105      Current loss:  108.44833374023438      Current accuracy:  0.0      Comp_time of one batch:  -0.32244324684143066\n",
            "Iteration:  3110      Current loss:  165.7942352294922      Current accuracy:  0.0      Comp_time of one batch:  -0.31464314460754395\n",
            "Iteration:  3115      Current loss:  95.14257049560547      Current accuracy:  0.0      Comp_time of one batch:  -0.31462597846984863\n",
            "Iteration:  3120      Current loss:  155.88926696777344      Current accuracy:  0.0      Comp_time of one batch:  -0.34662508964538574\n",
            "Iteration:  3125      Current loss:  115.21257781982422      Current accuracy:  0.0      Comp_time of one batch:  -0.3392775058746338\n",
            "Iteration:  3130      Current loss:  106.0928726196289      Current accuracy:  0.0      Comp_time of one batch:  -0.3313026428222656\n",
            "Iteration:  3135      Current loss:  159.0743865966797      Current accuracy:  0.0      Comp_time of one batch:  -0.34675121307373047\n",
            "Iteration:  3140      Current loss:  85.71358489990234      Current accuracy:  0.0      Comp_time of one batch:  -0.32457447052001953\n",
            "Iteration:  3145      Current loss:  121.9796142578125      Current accuracy:  0.0      Comp_time of one batch:  -0.32703447341918945\n",
            "Iteration:  3150      Current loss:  113.00394439697266      Current accuracy:  0.0      Comp_time of one batch:  -0.34603023529052734\n",
            "Iteration:  3155      Current loss:  104.71888732910156      Current accuracy:  0.0      Comp_time of one batch:  -0.3360726833343506\n",
            "Iteration:  3160      Current loss:  96.2071304321289      Current accuracy:  0.0      Comp_time of one batch:  -0.33689188957214355\n",
            "Iteration:  3165      Current loss:  110.04592895507812      Current accuracy:  0.0      Comp_time of one batch:  -0.3401310443878174\n",
            "Iteration:  3170      Current loss:  109.9163589477539      Current accuracy:  0.0      Comp_time of one batch:  -0.3138768672943115\n",
            "Iteration:  3175      Current loss:  93.6777572631836      Current accuracy:  0.0      Comp_time of one batch:  -0.31276655197143555\n",
            "Iteration:  3180      Current loss:  174.2283172607422      Current accuracy:  0.0      Comp_time of one batch:  -0.3200223445892334\n",
            "Iteration:  3185      Current loss:  101.07211303710938      Current accuracy:  0.0      Comp_time of one batch:  -0.3224048614501953\n",
            "Iteration:  3190      Current loss:  140.40875244140625      Current accuracy:  0.0      Comp_time of one batch:  -0.33432817459106445\n",
            "Iteration:  3195      Current loss:  106.1246109008789      Current accuracy:  0.0      Comp_time of one batch:  -0.35315370559692383\n",
            "Iteration:  3200      Current loss:  127.36859893798828      Current accuracy:  0.0      Comp_time of one batch:  -0.329301118850708\n",
            "Iteration:  3205      Current loss:  131.7338104248047      Current accuracy:  0.0      Comp_time of one batch:  -0.33535218238830566\n",
            "Iteration:  3210      Current loss:  140.9879150390625      Current accuracy:  0.0      Comp_time of one batch:  -0.34990596771240234\n",
            "Iteration:  3215      Current loss:  119.90172576904297      Current accuracy:  0.0      Comp_time of one batch:  -0.3145260810852051\n",
            "Iteration:  3220      Current loss:  97.53546142578125      Current accuracy:  0.0      Comp_time of one batch:  -0.30933141708374023\n",
            "Iteration:  3225      Current loss:  132.22434997558594      Current accuracy:  0.0      Comp_time of one batch:  -0.3412590026855469\n",
            "Iteration:  3230      Current loss:  103.87339782714844      Current accuracy:  0.0      Comp_time of one batch:  -0.31891822814941406\n",
            "Iteration:  3235      Current loss:  105.98573303222656      Current accuracy:  0.0      Comp_time of one batch:  -0.3181154727935791\n",
            "Iteration:  3240      Current loss:  92.89114379882812      Current accuracy:  0.0      Comp_time of one batch:  -0.3625829219818115\n",
            "Iteration:  3245      Current loss:  89.3197021484375      Current accuracy:  0.0      Comp_time of one batch:  -0.3392653465270996\n",
            "Iteration:  3250      Current loss:  89.05706024169922      Current accuracy:  0.0      Comp_time of one batch:  -0.3161623477935791\n",
            "Iteration:  3255      Current loss:  114.45771789550781      Current accuracy:  0.0      Comp_time of one batch:  -0.3206906318664551\n",
            "Iteration:  3260      Current loss:  147.65811157226562      Current accuracy:  0.0      Comp_time of one batch:  -0.32224535942077637\n",
            "Iteration:  3265      Current loss:  106.55363464355469      Current accuracy:  0.0      Comp_time of one batch:  -0.31499767303466797\n",
            "Iteration:  3270      Current loss:  131.83712768554688      Current accuracy:  0.0      Comp_time of one batch:  -0.34942197799682617\n",
            "Iteration:  3275      Current loss:  95.31009674072266      Current accuracy:  0.0      Comp_time of one batch:  -0.33547043800354004\n",
            "Iteration:  3280      Current loss:  107.99937438964844      Current accuracy:  0.0      Comp_time of one batch:  -0.3200385570526123\n",
            "Iteration:  3285      Current loss:  145.43154907226562      Current accuracy:  0.0      Comp_time of one batch:  -0.31882452964782715\n",
            "Iteration:  3290      Current loss:  116.15035247802734      Current accuracy:  0.0      Comp_time of one batch:  -0.32416796684265137\n",
            "Iteration:  3295      Current loss:  106.57032775878906      Current accuracy:  0.0      Comp_time of one batch:  -0.3130195140838623\n",
            "Iteration:  3300      Current loss:  149.91049194335938      Current accuracy:  0.0      Comp_time of one batch:  -0.3503611087799072\n",
            "Iteration:  3305      Current loss:  117.07646942138672      Current accuracy:  0.0      Comp_time of one batch:  -0.325427770614624\n",
            "Iteration:  3310      Current loss:  78.40904235839844      Current accuracy:  0.0      Comp_time of one batch:  -0.35178542137145996\n",
            "Iteration:  3315      Current loss:  91.093994140625      Current accuracy:  0.0      Comp_time of one batch:  -0.32361745834350586\n",
            "Iteration:  3320      Current loss:  134.2930908203125      Current accuracy:  0.0      Comp_time of one batch:  -0.32024621963500977\n",
            "Iteration:  3325      Current loss:  105.09911346435547      Current accuracy:  0.0      Comp_time of one batch:  -0.3545691967010498\n",
            "Iteration:  3330      Current loss:  116.91435241699219      Current accuracy:  0.0      Comp_time of one batch:  -0.34535932540893555\n",
            "Iteration:  3335      Current loss:  79.81693267822266      Current accuracy:  0.0      Comp_time of one batch:  -0.3296043872833252\n",
            "Iteration:  3340      Current loss:  143.4292449951172      Current accuracy:  0.0      Comp_time of one batch:  -0.33446335792541504\n",
            "Iteration:  3345      Current loss:  76.40258026123047      Current accuracy:  0.0      Comp_time of one batch:  -0.3151097297668457\n",
            "Iteration:  3350      Current loss:  115.96057891845703      Current accuracy:  0.0      Comp_time of one batch:  -0.31911301612854004\n",
            "Iteration:  3355      Current loss:  125.96041870117188      Current accuracy:  0.0      Comp_time of one batch:  -0.33732032775878906\n",
            "Iteration:  3360      Current loss:  90.56822967529297      Current accuracy:  0.0      Comp_time of one batch:  -0.35892796516418457\n",
            "Iteration:  3365      Current loss:  145.72547912597656      Current accuracy:  0.0      Comp_time of one batch:  -0.34580087661743164\n",
            "Iteration:  3370      Current loss:  139.1258087158203      Current accuracy:  0.0      Comp_time of one batch:  -0.3160066604614258\n",
            "Iteration:  3375      Current loss:  108.61475372314453      Current accuracy:  0.0      Comp_time of one batch:  -0.34055113792419434\n",
            "Iteration:  3380      Current loss:  125.63488006591797      Current accuracy:  0.0      Comp_time of one batch:  -0.32483768463134766\n",
            "Iteration:  3385      Current loss:  120.7162857055664      Current accuracy:  0.0      Comp_time of one batch:  -0.33574962615966797\n",
            "Iteration:  3390      Current loss:  133.92002868652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3407149314880371\n",
            "Iteration:  3395      Current loss:  129.14060974121094      Current accuracy:  0.0      Comp_time of one batch:  -0.3388700485229492\n",
            "Iteration:  3400      Current loss:  134.56640625      Current accuracy:  0.0      Comp_time of one batch:  -0.3152742385864258\n",
            "Iteration:  3405      Current loss:  131.85934448242188      Current accuracy:  0.0      Comp_time of one batch:  -0.33979344367980957\n",
            "Iteration:  3410      Current loss:  154.38035583496094      Current accuracy:  0.0      Comp_time of one batch:  -0.34537625312805176\n",
            "Iteration:  3415      Current loss:  188.4104461669922      Current accuracy:  0.0      Comp_time of one batch:  -0.31807708740234375\n",
            "Iteration:  3420      Current loss:  124.6181869506836      Current accuracy:  0.0      Comp_time of one batch:  -0.35448336601257324\n",
            "Iteration:  3425      Current loss:  116.2876968383789      Current accuracy:  0.0      Comp_time of one batch:  -0.3122599124908447\n",
            "Iteration:  3430      Current loss:  125.11406707763672      Current accuracy:  0.0      Comp_time of one batch:  -0.3201110363006592\n",
            "Iteration:  3435      Current loss:  108.3437728881836      Current accuracy:  0.0      Comp_time of one batch:  -0.323702335357666\n",
            "Iteration:  3440      Current loss:  116.78756713867188      Current accuracy:  0.0      Comp_time of one batch:  -0.3383212089538574\n",
            "Iteration:  3445      Current loss:  103.61721801757812      Current accuracy:  0.0      Comp_time of one batch:  -0.3163142204284668\n",
            "Iteration:  3450      Current loss:  140.79736328125      Current accuracy:  0.0      Comp_time of one batch:  -0.3393707275390625\n",
            "Iteration:  3455      Current loss:  112.58805847167969      Current accuracy:  0.0      Comp_time of one batch:  -0.3300638198852539\n",
            "Iteration:  3460      Current loss:  92.62958526611328      Current accuracy:  0.0      Comp_time of one batch:  -0.31671833992004395\n",
            "Iteration:  3465      Current loss:  151.63980102539062      Current accuracy:  0.0      Comp_time of one batch:  -0.34218287467956543\n",
            "Iteration:  3470      Current loss:  156.9421844482422      Current accuracy:  0.0      Comp_time of one batch:  -0.32628893852233887\n",
            "Iteration:  3475      Current loss:  143.28875732421875      Current accuracy:  0.0      Comp_time of one batch:  -0.33876848220825195\n",
            "Iteration:  3480      Current loss:  94.7567367553711      Current accuracy:  0.0      Comp_time of one batch:  -0.3168175220489502\n",
            "Iteration:  3485      Current loss:  75.16792297363281      Current accuracy:  0.0      Comp_time of one batch:  -0.3358769416809082\n",
            "Iteration:  3490      Current loss:  124.94093322753906      Current accuracy:  0.0      Comp_time of one batch:  -0.3288428783416748\n",
            "Iteration:  3495      Current loss:  91.5086441040039      Current accuracy:  0.0      Comp_time of one batch:  -0.32918310165405273\n",
            "Iteration:  3500      Current loss:  114.14578247070312      Current accuracy:  0.0      Comp_time of one batch:  -0.3196525573730469\n",
            "Iteration:  3505      Current loss:  132.8280792236328      Current accuracy:  0.0      Comp_time of one batch:  -0.3156168460845947\n",
            "Iteration:  3510      Current loss:  146.897705078125      Current accuracy:  0.0      Comp_time of one batch:  -0.3311493396759033\n",
            "Iteration:  3515      Current loss:  85.77839660644531      Current accuracy:  0.0      Comp_time of one batch:  -0.3257479667663574\n",
            "Iteration:  3520      Current loss:  120.1097412109375      Current accuracy:  0.0      Comp_time of one batch:  -0.33566975593566895\n",
            "Iteration:  3525      Current loss:  83.6779556274414      Current accuracy:  0.0      Comp_time of one batch:  -0.3347892761230469\n",
            "Iteration:  3530      Current loss:  103.24400329589844      Current accuracy:  0.0      Comp_time of one batch:  -0.3449127674102783\n",
            "Iteration:  3535      Current loss:  154.72320556640625      Current accuracy:  0.0      Comp_time of one batch:  -0.3487262725830078\n",
            "Iteration:  3540      Current loss:  134.45201110839844      Current accuracy:  0.0      Comp_time of one batch:  -0.31286168098449707\n",
            "Iteration:  3545      Current loss:  147.24827575683594      Current accuracy:  0.0      Comp_time of one batch:  -0.31548190116882324\n",
            "Iteration:  3550      Current loss:  117.62178039550781      Current accuracy:  0.0      Comp_time of one batch:  -0.34051966667175293\n",
            "Iteration:  3555      Current loss:  137.17494201660156      Current accuracy:  0.0      Comp_time of one batch:  -0.31850600242614746\n",
            "Iteration:  3560      Current loss:  95.06710052490234      Current accuracy:  0.0      Comp_time of one batch:  -0.314727783203125\n",
            "Iteration:  3565      Current loss:  76.61988067626953      Current accuracy:  0.0      Comp_time of one batch:  -0.33759188652038574\n",
            "Iteration:  3570      Current loss:  113.99959564208984      Current accuracy:  0.0      Comp_time of one batch:  -0.34148311614990234\n",
            "Iteration:  3575      Current loss:  82.73519134521484      Current accuracy:  0.0      Comp_time of one batch:  -0.31742286682128906\n",
            "Iteration:  3580      Current loss:  79.47740173339844      Current accuracy:  0.0      Comp_time of one batch:  -0.3192293643951416\n",
            "Iteration:  3585      Current loss:  121.51638793945312      Current accuracy:  0.0      Comp_time of one batch:  -0.34746336936950684\n",
            "Iteration:  3590      Current loss:  141.5029296875      Current accuracy:  0.0      Comp_time of one batch:  -0.33341193199157715\n",
            "Iteration:  3595      Current loss:  112.57563781738281      Current accuracy:  0.0      Comp_time of one batch:  -0.3122069835662842\n",
            "Iteration:  3600      Current loss:  169.10499572753906      Current accuracy:  0.0      Comp_time of one batch:  -0.32128381729125977\n",
            "Iteration:  3605      Current loss:  91.33634185791016      Current accuracy:  0.0      Comp_time of one batch:  -0.3438901901245117\n",
            "Iteration:  3610      Current loss:  127.9686050415039      Current accuracy:  0.0      Comp_time of one batch:  -0.3223271369934082\n",
            "Iteration:  3615      Current loss:  120.57344818115234      Current accuracy:  0.0      Comp_time of one batch:  -0.32116198539733887\n",
            "Iteration:  3620      Current loss:  104.0369873046875      Current accuracy:  0.0      Comp_time of one batch:  -0.32447218894958496\n",
            "Iteration:  3625      Current loss:  108.81292724609375      Current accuracy:  0.0      Comp_time of one batch:  -0.3354604244232178\n",
            "Iteration:  3630      Current loss:  147.23226928710938      Current accuracy:  0.0      Comp_time of one batch:  -0.3155982494354248\n",
            "Iteration:  3635      Current loss:  115.42665100097656      Current accuracy:  0.0      Comp_time of one batch:  -0.3415961265563965\n",
            "Iteration:  3640      Current loss:  140.67552185058594      Current accuracy:  0.0      Comp_time of one batch:  -0.336681604385376\n",
            "Iteration:  3645      Current loss:  112.2902603149414      Current accuracy:  0.0      Comp_time of one batch:  -0.3269684314727783\n",
            "Iteration:  3650      Current loss:  86.86707305908203      Current accuracy:  0.0      Comp_time of one batch:  -0.31929564476013184\n",
            "Iteration:  3655      Current loss:  139.6023712158203      Current accuracy:  0.0      Comp_time of one batch:  -0.3361198902130127\n",
            "Iteration:  3660      Current loss:  136.3994598388672      Current accuracy:  0.0      Comp_time of one batch:  -0.3425266742706299\n",
            "Iteration:  3665      Current loss:  99.89466094970703      Current accuracy:  0.0      Comp_time of one batch:  -0.33583617210388184\n",
            "Iteration:  3670      Current loss:  95.08164978027344      Current accuracy:  0.0      Comp_time of one batch:  -0.3379347324371338\n",
            "Iteration:  3675      Current loss:  112.97543334960938      Current accuracy:  0.0      Comp_time of one batch:  -0.33866310119628906\n",
            "Iteration:  3680      Current loss:  118.62435913085938      Current accuracy:  0.0      Comp_time of one batch:  -0.3241739273071289\n",
            "Iteration:  3685      Current loss:  138.1428985595703      Current accuracy:  0.0      Comp_time of one batch:  -0.3291945457458496\n",
            "Iteration:  3690      Current loss:  88.18573760986328      Current accuracy:  0.0      Comp_time of one batch:  -0.31473851203918457\n",
            "Iteration:  3695      Current loss:  80.96876525878906      Current accuracy:  0.0      Comp_time of one batch:  -0.32204627990722656\n",
            "Iteration:  3700      Current loss:  159.0184326171875      Current accuracy:  0.0      Comp_time of one batch:  -0.3242318630218506\n",
            "Iteration:  3705      Current loss:  108.74791717529297      Current accuracy:  0.0      Comp_time of one batch:  -0.3155999183654785\n",
            "Iteration:  3710      Current loss:  75.8287582397461      Current accuracy:  0.0      Comp_time of one batch:  -0.3168675899505615\n",
            "Iteration:  3715      Current loss:  83.15892791748047      Current accuracy:  0.0      Comp_time of one batch:  -0.3197755813598633\n",
            "Iteration:  3720      Current loss:  99.7970962524414      Current accuracy:  0.0      Comp_time of one batch:  -0.31522655487060547\n",
            "Iteration:  3725      Current loss:  124.49346160888672      Current accuracy:  0.0      Comp_time of one batch:  -0.3166210651397705\n",
            "Iteration:  3730      Current loss:  86.8591079711914      Current accuracy:  0.0      Comp_time of one batch:  -0.32198095321655273\n",
            "Iteration:  3735      Current loss:  83.34915924072266      Current accuracy:  0.0      Comp_time of one batch:  -0.3181741237640381\n",
            "Iteration:  3740      Current loss:  137.74937438964844      Current accuracy:  0.0      Comp_time of one batch:  -0.3170928955078125\n",
            "Iteration:  3745      Current loss:  162.7472686767578      Current accuracy:  0.0      Comp_time of one batch:  -0.31619977951049805\n",
            "Iteration:  3750      Current loss:  114.97505187988281      Current accuracy:  0.0      Comp_time of one batch:  -0.32648801803588867\n",
            "Iteration:  3755      Current loss:  106.27192687988281      Current accuracy:  0.0      Comp_time of one batch:  -0.3207674026489258\n",
            "Iteration:  3760      Current loss:  123.69713592529297      Current accuracy:  0.0      Comp_time of one batch:  -0.31318211555480957\n",
            "Iteration:  3765      Current loss:  128.4832305908203      Current accuracy:  0.0      Comp_time of one batch:  -0.3180069923400879\n",
            "Iteration:  3770      Current loss:  108.85617065429688      Current accuracy:  0.0      Comp_time of one batch:  -0.31133460998535156\n",
            "Iteration:  3775      Current loss:  115.70603942871094      Current accuracy:  0.0      Comp_time of one batch:  -0.31235218048095703\n",
            "Iteration:  3780      Current loss:  82.3673324584961      Current accuracy:  0.0      Comp_time of one batch:  -0.3118929862976074\n",
            "Iteration:  3785      Current loss:  106.80126190185547      Current accuracy:  0.0      Comp_time of one batch:  -0.3156883716583252\n",
            "Iteration:  3790      Current loss:  128.44577026367188      Current accuracy:  0.0      Comp_time of one batch:  -0.31845664978027344\n",
            "Iteration:  3795      Current loss:  166.05615234375      Current accuracy:  0.0      Comp_time of one batch:  -0.31915998458862305\n",
            "Iteration:  3800      Current loss:  126.02761840820312      Current accuracy:  0.0      Comp_time of one batch:  -0.31305932998657227\n",
            "Iteration:  3805      Current loss:  93.20386505126953      Current accuracy:  0.0      Comp_time of one batch:  -0.326937198638916\n",
            "Iteration:  3810      Current loss:  89.04336547851562      Current accuracy:  0.0      Comp_time of one batch:  -0.316455602645874\n",
            "Iteration:  3815      Current loss:  102.024169921875      Current accuracy:  0.0      Comp_time of one batch:  -0.31569600105285645\n",
            "Iteration:  3820      Current loss:  79.7939224243164      Current accuracy:  0.0      Comp_time of one batch:  -0.31418371200561523\n",
            "Iteration:  3825      Current loss:  109.09322357177734      Current accuracy:  0.0      Comp_time of one batch:  -0.3186514377593994\n",
            "Iteration:  3830      Current loss:  90.39423370361328      Current accuracy:  0.0      Comp_time of one batch:  -0.3296821117401123\n",
            "Iteration:  3835      Current loss:  110.11038970947266      Current accuracy:  0.0      Comp_time of one batch:  -0.32248473167419434\n",
            "Iteration:  3840      Current loss:  88.41069030761719      Current accuracy:  0.0      Comp_time of one batch:  -0.34262537956237793\n",
            "Iteration:  3845      Current loss:  137.4846649169922      Current accuracy:  0.0      Comp_time of one batch:  -0.3124873638153076\n",
            "Iteration:  3850      Current loss:  143.71817016601562      Current accuracy:  0.0      Comp_time of one batch:  -0.3166823387145996\n",
            "Iteration:  3855      Current loss:  75.99138641357422      Current accuracy:  0.0      Comp_time of one batch:  -0.3387470245361328\n",
            "Iteration:  3860      Current loss:  89.03341674804688      Current accuracy:  0.0      Comp_time of one batch:  -0.3134033679962158\n",
            "Iteration:  3865      Current loss:  101.42805480957031      Current accuracy:  0.0      Comp_time of one batch:  -0.31206703186035156\n",
            "Iteration:  3870      Current loss:  87.82132720947266      Current accuracy:  0.0      Comp_time of one batch:  -0.31556248664855957\n",
            "Iteration:  3875      Current loss:  125.94312286376953      Current accuracy:  0.0      Comp_time of one batch:  -0.3260793685913086\n",
            "Iteration:  3880      Current loss:  145.34730529785156      Current accuracy:  0.0      Comp_time of one batch:  -0.33602404594421387\n",
            "Iteration:  3885      Current loss:  74.89910125732422      Current accuracy:  0.0      Comp_time of one batch:  -0.34453487396240234\n",
            "Iteration:  3890      Current loss:  154.76808166503906      Current accuracy:  0.0      Comp_time of one batch:  -0.31606316566467285\n",
            "Iteration:  3895      Current loss:  69.05027770996094      Current accuracy:  0.0      Comp_time of one batch:  -0.31397485733032227\n",
            "Iteration:  3900      Current loss:  102.48358917236328      Current accuracy:  0.0      Comp_time of one batch:  -0.333834171295166\n",
            "Iteration:  3905      Current loss:  129.52081298828125      Current accuracy:  0.0      Comp_time of one batch:  -0.3144948482513428\n",
            "Iteration:  3910      Current loss:  150.81117248535156      Current accuracy:  0.0      Comp_time of one batch:  -0.31049036979675293\n",
            "Iteration:  3915      Current loss:  155.15090942382812      Current accuracy:  0.0      Comp_time of one batch:  -0.3130202293395996\n",
            "Iteration:  3920      Current loss:  130.8899688720703      Current accuracy:  0.0      Comp_time of one batch:  -0.32473015785217285\n",
            "Iteration:  3925      Current loss:  140.43907165527344      Current accuracy:  0.0      Comp_time of one batch:  -0.3157672882080078\n",
            "Iteration:  3930      Current loss:  98.1241226196289      Current accuracy:  0.0      Comp_time of one batch:  -0.3475821018218994\n",
            "Iteration:  3935      Current loss:  118.51202392578125      Current accuracy:  0.0      Comp_time of one batch:  -0.32088303565979004\n",
            "Iteration:  3940      Current loss:  120.83785247802734      Current accuracy:  0.0      Comp_time of one batch:  -0.3126094341278076\n",
            "Iteration:  3945      Current loss:  127.23560333251953      Current accuracy:  0.0      Comp_time of one batch:  -0.3373379707336426\n",
            "Iteration:  3950      Current loss:  146.33236694335938      Current accuracy:  0.0      Comp_time of one batch:  -0.34223294258117676\n",
            "Iteration:  3955      Current loss:  167.21165466308594      Current accuracy:  0.0      Comp_time of one batch:  -0.33079004287719727\n",
            "Iteration:  3960      Current loss:  96.33912658691406      Current accuracy:  0.0      Comp_time of one batch:  -0.3373301029205322\n",
            "Iteration:  3965      Current loss:  113.22648620605469      Current accuracy:  0.0      Comp_time of one batch:  -0.31705665588378906\n",
            "Iteration:  3970      Current loss:  93.50900268554688      Current accuracy:  0.0      Comp_time of one batch:  -0.3121519088745117\n",
            "Iteration:  3975      Current loss:  80.72306060791016      Current accuracy:  0.0      Comp_time of one batch:  -0.3439922332763672\n",
            "Iteration:  3980      Current loss:  131.69046020507812      Current accuracy:  0.0      Comp_time of one batch:  -0.3399195671081543\n",
            "Iteration:  3985      Current loss:  133.81680297851562      Current accuracy:  0.0      Comp_time of one batch:  -0.3234879970550537\n",
            "Iteration:  3990      Current loss:  119.02938079833984      Current accuracy:  0.0      Comp_time of one batch:  -0.33322644233703613\n",
            "Iteration:  3995      Current loss:  130.169677734375      Current accuracy:  0.0      Comp_time of one batch:  -0.3179745674133301\n",
            "Iteration:  4000      Current loss:  134.48361206054688      Current accuracy:  0.0      Comp_time of one batch:  -0.32287168502807617\n",
            "Iteration:  4005      Current loss:  88.10511016845703      Current accuracy:  0.0      Comp_time of one batch:  -0.3352515697479248\n",
            "Iteration:  4010      Current loss:  156.97463989257812      Current accuracy:  0.0      Comp_time of one batch:  -0.3255765438079834\n",
            "Iteration:  4015      Current loss:  120.38217163085938      Current accuracy:  0.0      Comp_time of one batch:  -0.37143778800964355\n",
            "Iteration:  4020      Current loss:  147.04185485839844      Current accuracy:  0.0      Comp_time of one batch:  -0.32473301887512207\n",
            "Iteration:  4025      Current loss:  141.47549438476562      Current accuracy:  0.0      Comp_time of one batch:  -0.32590222358703613\n",
            "Iteration:  4030      Current loss:  118.3293228149414      Current accuracy:  0.0      Comp_time of one batch:  -0.36199140548706055\n",
            "Iteration:  4035      Current loss:  112.7831802368164      Current accuracy:  0.0      Comp_time of one batch:  -0.3503146171569824\n",
            "Iteration:  4040      Current loss:  168.89251708984375      Current accuracy:  0.0      Comp_time of one batch:  -0.3237721920013428\n",
            "Iteration:  4045      Current loss:  119.27188110351562      Current accuracy:  0.0      Comp_time of one batch:  -0.3559129238128662\n",
            "Iteration:  4050      Current loss:  93.89315032958984      Current accuracy:  0.0      Comp_time of one batch:  -0.35403013229370117\n",
            "Iteration:  4055      Current loss:  154.6272430419922      Current accuracy:  0.0      Comp_time of one batch:  -0.3183104991912842\n",
            "Iteration:  4060      Current loss:  128.32907104492188      Current accuracy:  0.0      Comp_time of one batch:  -0.3342399597167969\n",
            "Iteration:  4065      Current loss:  94.0566177368164      Current accuracy:  0.0      Comp_time of one batch:  -0.33533191680908203\n",
            "Iteration:  4070      Current loss:  136.38027954101562      Current accuracy:  0.0      Comp_time of one batch:  -0.32605481147766113\n",
            "Iteration:  4075      Current loss:  91.42723083496094      Current accuracy:  0.0      Comp_time of one batch:  -0.3708193302154541\n",
            "Iteration:  4080      Current loss:  93.62234497070312      Current accuracy:  0.0      Comp_time of one batch:  -0.3148977756500244\n",
            "Iteration:  4085      Current loss:  156.74359130859375      Current accuracy:  0.0      Comp_time of one batch:  -0.32859349250793457\n",
            "Iteration:  4090      Current loss:  102.48062896728516      Current accuracy:  0.0      Comp_time of one batch:  -0.3482081890106201\n",
            "Iteration:  4095      Current loss:  125.03705596923828      Current accuracy:  0.0      Comp_time of one batch:  -0.33379554748535156\n",
            "Iteration:  4100      Current loss:  133.0440673828125      Current accuracy:  0.0      Comp_time of one batch:  -0.3114173412322998\n",
            "Iteration:  4105      Current loss:  117.28533935546875      Current accuracy:  0.0      Comp_time of one batch:  -0.3349459171295166\n",
            "Iteration:  4110      Current loss:  93.06681060791016      Current accuracy:  0.0      Comp_time of one batch:  -0.3315095901489258\n",
            "Iteration:  4115      Current loss:  97.29328918457031      Current accuracy:  0.0      Comp_time of one batch:  -0.36307358741760254\n",
            "Iteration:  4120      Current loss:  165.06619262695312      Current accuracy:  0.0      Comp_time of one batch:  -0.3277299404144287\n",
            "Iteration:  4125      Current loss:  104.70659637451172      Current accuracy:  0.0      Comp_time of one batch:  -0.3551182746887207\n",
            "Iteration:  4130      Current loss:  117.71688079833984      Current accuracy:  0.0      Comp_time of one batch:  -0.3161766529083252\n",
            "Iteration:  4135      Current loss:  85.81216430664062      Current accuracy:  0.0      Comp_time of one batch:  -0.3226039409637451\n",
            "Iteration:  4140      Current loss:  118.38048553466797      Current accuracy:  0.0      Comp_time of one batch:  -0.3150753974914551\n",
            "Iteration:  4145      Current loss:  72.63134765625      Current accuracy:  0.0      Comp_time of one batch:  -0.31653618812561035\n",
            "Iteration:  4150      Current loss:  92.55442810058594      Current accuracy:  0.0      Comp_time of one batch:  -0.3144352436065674\n",
            "Iteration:  4155      Current loss:  124.34354400634766      Current accuracy:  0.0      Comp_time of one batch:  -0.33298730850219727\n",
            "Iteration:  4160      Current loss:  80.64059448242188      Current accuracy:  0.0      Comp_time of one batch:  -0.3153529167175293\n",
            "Iteration:  4165      Current loss:  87.99893188476562      Current accuracy:  0.0      Comp_time of one batch:  -0.3166346549987793\n",
            "Iteration:  4170      Current loss:  138.2684326171875      Current accuracy:  0.0      Comp_time of one batch:  -0.3158681392669678\n",
            "Iteration:  4175      Current loss:  91.35337829589844      Current accuracy:  0.0      Comp_time of one batch:  -0.3140103816986084\n",
            "Iteration:  4180      Current loss:  120.75346374511719      Current accuracy:  0.0      Comp_time of one batch:  -0.31227588653564453\n",
            "Iteration:  4185      Current loss:  145.13088989257812      Current accuracy:  0.0      Comp_time of one batch:  -0.35741519927978516\n",
            "Iteration:  4190      Current loss:  123.17182922363281      Current accuracy:  0.0      Comp_time of one batch:  -0.32039427757263184\n",
            "Iteration:  4195      Current loss:  89.68560791015625      Current accuracy:  0.0      Comp_time of one batch:  -0.32221078872680664\n",
            "Iteration:  4200      Current loss:  82.31314849853516      Current accuracy:  0.0      Comp_time of one batch:  -0.3260989189147949\n",
            "Iteration:  4205      Current loss:  118.29997253417969      Current accuracy:  0.0      Comp_time of one batch:  -0.3198432922363281\n",
            "Iteration:  4210      Current loss:  146.2081298828125      Current accuracy:  0.0      Comp_time of one batch:  -0.32266783714294434\n",
            "Iteration:  4215      Current loss:  77.17450714111328      Current accuracy:  0.0      Comp_time of one batch:  -0.337522029876709\n",
            "Iteration:  4220      Current loss:  99.6896743774414      Current accuracy:  0.0      Comp_time of one batch:  -0.31328821182250977\n",
            "Iteration:  4225      Current loss:  92.08241271972656      Current accuracy:  0.0      Comp_time of one batch:  -0.31628990173339844\n",
            "Iteration:  4230      Current loss:  163.03944396972656      Current accuracy:  0.0      Comp_time of one batch:  -0.31531620025634766\n",
            "Iteration:  4235      Current loss:  69.09159088134766      Current accuracy:  0.0      Comp_time of one batch:  -0.3200356960296631\n",
            "Iteration:  4240      Current loss:  108.2166519165039      Current accuracy:  0.0      Comp_time of one batch:  -0.3202972412109375\n",
            "Iteration:  4245      Current loss:  120.2468032836914      Current accuracy:  0.0      Comp_time of one batch:  -0.3167233467102051\n",
            "Iteration:  4250      Current loss:  129.23622131347656      Current accuracy:  0.0      Comp_time of one batch:  -0.3096916675567627\n",
            "Iteration:  4255      Current loss:  103.75548553466797      Current accuracy:  0.0      Comp_time of one batch:  -0.3114035129547119\n",
            "Iteration:  4260      Current loss:  97.3445053100586      Current accuracy:  0.0      Comp_time of one batch:  -0.3351430892944336\n",
            "Iteration:  4265      Current loss:  117.57902526855469      Current accuracy:  0.0      Comp_time of one batch:  -0.3409407138824463\n",
            "Iteration:  4270      Current loss:  98.7894058227539      Current accuracy:  0.0      Comp_time of one batch:  -0.31501102447509766\n",
            "Iteration:  4275      Current loss:  117.45690155029297      Current accuracy:  0.0      Comp_time of one batch:  -0.33580732345581055\n",
            "Iteration:  4280      Current loss:  97.52597045898438      Current accuracy:  0.0      Comp_time of one batch:  -0.31480908393859863\n",
            "Iteration:  4285      Current loss:  123.15446472167969      Current accuracy:  0.0      Comp_time of one batch:  -0.34572529792785645\n",
            "Iteration:  4290      Current loss:  147.2252960205078      Current accuracy:  0.0      Comp_time of one batch:  -0.3363008499145508\n",
            "Iteration:  4295      Current loss:  128.39938354492188      Current accuracy:  0.0      Comp_time of one batch:  -0.31933164596557617\n",
            "Iteration:  4300      Current loss:  103.16883087158203      Current accuracy:  0.0      Comp_time of one batch:  -0.3215606212615967\n",
            "Iteration:  4305      Current loss:  79.8672866821289      Current accuracy:  0.0      Comp_time of one batch:  -0.3315467834472656\n",
            "Iteration:  4310      Current loss:  127.22171020507812      Current accuracy:  0.0      Comp_time of one batch:  -0.3359994888305664\n",
            "Iteration:  4315      Current loss:  68.4132308959961      Current accuracy:  0.0      Comp_time of one batch:  -0.3197333812713623\n",
            "Iteration:  4320      Current loss:  119.22613525390625      Current accuracy:  0.0      Comp_time of one batch:  -0.3491957187652588\n",
            "Iteration:  4325      Current loss:  124.26934051513672      Current accuracy:  0.0      Comp_time of one batch:  -0.3303868770599365\n",
            "Iteration:  4330      Current loss:  111.46463775634766      Current accuracy:  0.0      Comp_time of one batch:  -0.31223058700561523\n",
            "Iteration:  4335      Current loss:  111.90423583984375      Current accuracy:  0.0      Comp_time of one batch:  -0.32777953147888184\n",
            "Iteration:  4340      Current loss:  107.26679992675781      Current accuracy:  0.0      Comp_time of one batch:  -0.3114771842956543\n",
            "Iteration:  4345      Current loss:  117.93949890136719      Current accuracy:  0.0      Comp_time of one batch:  -0.33455657958984375\n",
            "Iteration:  4350      Current loss:  123.83929443359375      Current accuracy:  0.0      Comp_time of one batch:  -0.35445356369018555\n",
            "Iteration:  4355      Current loss:  130.70068359375      Current accuracy:  0.0      Comp_time of one batch:  -0.3289029598236084\n",
            "Iteration:  4360      Current loss:  125.13739013671875      Current accuracy:  0.0      Comp_time of one batch:  -0.3102841377258301\n",
            "Iteration:  4365      Current loss:  88.10160827636719      Current accuracy:  0.0      Comp_time of one batch:  -0.31519317626953125\n",
            "Iteration:  4370      Current loss:  132.71755981445312      Current accuracy:  0.0      Comp_time of one batch:  -0.319305419921875\n",
            "Iteration:  4375      Current loss:  187.77023315429688      Current accuracy:  0.0      Comp_time of one batch:  -0.31130480766296387\n",
            "Iteration:  4380      Current loss:  125.0845947265625      Current accuracy:  0.0      Comp_time of one batch:  -0.3159151077270508\n",
            "Iteration:  4385      Current loss:  144.566650390625      Current accuracy:  0.0      Comp_time of one batch:  -0.3227994441986084\n",
            "Iteration:  4390      Current loss:  177.38482666015625      Current accuracy:  0.0      Comp_time of one batch:  -0.31647276878356934\n",
            "Iteration:  4395      Current loss:  137.32992553710938      Current accuracy:  0.0      Comp_time of one batch:  -0.31725549697875977\n",
            "Iteration:  4400      Current loss:  86.17475128173828      Current accuracy:  0.0      Comp_time of one batch:  -0.3194460868835449\n",
            "Iteration:  4405      Current loss:  139.9237823486328      Current accuracy:  0.0      Comp_time of one batch:  -0.32140159606933594\n",
            "Iteration:  4410      Current loss:  135.01783752441406      Current accuracy:  0.0      Comp_time of one batch:  -0.31789422035217285\n",
            "Iteration:  4415      Current loss:  169.64549255371094      Current accuracy:  0.0      Comp_time of one batch:  -0.32372546195983887\n",
            "Iteration:  4420      Current loss:  68.01071166992188      Current accuracy:  0.0      Comp_time of one batch:  -0.3231546878814697\n",
            "Iteration:  4425      Current loss:  122.15534210205078      Current accuracy:  0.0      Comp_time of one batch:  -0.3333311080932617\n",
            "Iteration:  4430      Current loss:  149.33547973632812      Current accuracy:  0.0      Comp_time of one batch:  -0.3245711326599121\n",
            "Iteration:  4435      Current loss:  92.73291015625      Current accuracy:  0.0      Comp_time of one batch:  -0.32156968116760254\n",
            "Iteration:  4440      Current loss:  99.80501556396484      Current accuracy:  0.0      Comp_time of one batch:  -0.31816649436950684\n",
            "Iteration:  4445      Current loss:  93.85780334472656      Current accuracy:  0.0      Comp_time of one batch:  -0.3153963088989258\n",
            "Iteration:  4450      Current loss:  83.61981201171875      Current accuracy:  0.0      Comp_time of one batch:  -0.32030725479125977\n",
            "Iteration:  4455      Current loss:  80.96234130859375      Current accuracy:  0.0      Comp_time of one batch:  -0.3198418617248535\n",
            "Iteration:  4460      Current loss:  138.11373901367188      Current accuracy:  0.0      Comp_time of one batch:  -0.32137370109558105\n",
            "Iteration:  4465      Current loss:  113.48017120361328      Current accuracy:  0.0      Comp_time of one batch:  -0.31413936614990234\n",
            "Iteration:  4470      Current loss:  133.232666015625      Current accuracy:  0.0      Comp_time of one batch:  -0.3114147186279297\n",
            "Iteration:  4475      Current loss:  117.22408294677734      Current accuracy:  0.0      Comp_time of one batch:  -0.3335103988647461\n",
            "Iteration:  4480      Current loss:  119.16226196289062      Current accuracy:  0.0      Comp_time of one batch:  -0.3392164707183838\n",
            "Iteration:  4485      Current loss:  85.62809753417969      Current accuracy:  0.0      Comp_time of one batch:  -0.3244283199310303\n",
            "Iteration:  4490      Current loss:  125.55593872070312      Current accuracy:  0.0      Comp_time of one batch:  -0.33150339126586914\n",
            "Iteration:  4495      Current loss:  104.5051498413086      Current accuracy:  0.0      Comp_time of one batch:  -0.3103055953979492\n",
            "Iteration:  4500      Current loss:  167.468505859375      Current accuracy:  0.0      Comp_time of one batch:  -0.3141627311706543\n",
            "Iteration:  4505      Current loss:  79.26946258544922      Current accuracy:  0.0      Comp_time of one batch:  -0.32251405715942383\n",
            "Iteration:  4510      Current loss:  89.32697296142578      Current accuracy:  0.0      Comp_time of one batch:  -0.32181334495544434\n",
            "Iteration:  4515      Current loss:  108.0115966796875      Current accuracy:  0.0      Comp_time of one batch:  -0.3130970001220703\n",
            "Iteration:  4520      Current loss:  104.7752685546875      Current accuracy:  0.0      Comp_time of one batch:  -0.34554052352905273\n",
            "Iteration:  4525      Current loss:  143.64988708496094      Current accuracy:  0.0      Comp_time of one batch:  -0.3204660415649414\n",
            "Iteration:  4530      Current loss:  144.29396057128906      Current accuracy:  0.0      Comp_time of one batch:  -0.3370351791381836\n",
            "Iteration:  4535      Current loss:  110.83290100097656      Current accuracy:  0.0      Comp_time of one batch:  -0.3432462215423584\n",
            "Iteration:  4540      Current loss:  125.93376922607422      Current accuracy:  0.0      Comp_time of one batch:  -0.3341696262359619\n",
            "Iteration:  4545      Current loss:  121.14883422851562      Current accuracy:  0.0      Comp_time of one batch:  -0.31923937797546387\n",
            "Iteration:  4550      Current loss:  98.69715118408203      Current accuracy:  0.0      Comp_time of one batch:  -0.32183194160461426\n",
            "Iteration:  4555      Current loss:  86.93269348144531      Current accuracy:  0.0      Comp_time of one batch:  -0.3329651355743408\n",
            "Iteration:  4560      Current loss:  68.2344970703125      Current accuracy:  0.0      Comp_time of one batch:  -0.31278419494628906\n",
            "Iteration:  4565      Current loss:  79.99595642089844      Current accuracy:  0.0      Comp_time of one batch:  -0.31409263610839844\n",
            "Iteration:  4570      Current loss:  127.05838012695312      Current accuracy:  0.0      Comp_time of one batch:  -0.3346292972564697\n",
            "Iteration:  4575      Current loss:  84.26224517822266      Current accuracy:  0.0      Comp_time of one batch:  -0.3143033981323242\n",
            "Iteration:  4580      Current loss:  102.58367919921875      Current accuracy:  0.0      Comp_time of one batch:  -0.3195044994354248\n",
            "Iteration:  4585      Current loss:  144.19857788085938      Current accuracy:  0.0      Comp_time of one batch:  -0.31641602516174316\n",
            "Iteration:  4590      Current loss:  140.9456024169922      Current accuracy:  0.0      Comp_time of one batch:  -0.31578540802001953\n",
            "Iteration:  4595      Current loss:  89.03618621826172      Current accuracy:  0.0      Comp_time of one batch:  -0.3225233554840088\n",
            "Iteration:  4600      Current loss:  95.85933685302734      Current accuracy:  0.0      Comp_time of one batch:  -0.32500219345092773\n",
            "Iteration:  4605      Current loss:  113.14366149902344      Current accuracy:  0.0      Comp_time of one batch:  -0.3243997097015381\n",
            "Iteration:  4610      Current loss:  127.43175506591797      Current accuracy:  0.0      Comp_time of one batch:  -0.32214784622192383\n",
            "Iteration:  4615      Current loss:  185.5127716064453      Current accuracy:  0.0      Comp_time of one batch:  -0.32561230659484863\n",
            "Iteration:  4620      Current loss:  100.45410919189453      Current accuracy:  0.0      Comp_time of one batch:  -0.3316764831542969\n",
            "Iteration:  4625      Current loss:  124.0653305053711      Current accuracy:  0.0      Comp_time of one batch:  -0.3354766368865967\n",
            "Iteration:  4630      Current loss:  125.02198791503906      Current accuracy:  0.0      Comp_time of one batch:  -0.3322420120239258\n",
            "Iteration:  4635      Current loss:  126.623779296875      Current accuracy:  0.0      Comp_time of one batch:  -0.3635578155517578\n",
            "Iteration:  4640      Current loss:  121.54457092285156      Current accuracy:  0.0      Comp_time of one batch:  -0.3350858688354492\n",
            "Iteration:  4645      Current loss:  163.49949645996094      Current accuracy:  0.0      Comp_time of one batch:  -0.316802978515625\n",
            "Iteration:  4650      Current loss:  121.6385498046875      Current accuracy:  0.0      Comp_time of one batch:  -0.35338568687438965\n",
            "Iteration:  4655      Current loss:  99.90642547607422      Current accuracy:  0.0      Comp_time of one batch:  -0.3400156497955322\n",
            "Iteration:  4660      Current loss:  139.5503387451172      Current accuracy:  0.0      Comp_time of one batch:  -0.3160269260406494\n",
            "Iteration:  4665      Current loss:  161.06991577148438      Current accuracy:  0.0      Comp_time of one batch:  -0.33382225036621094\n",
            "Iteration:  4670      Current loss:  129.36236572265625      Current accuracy:  0.0      Comp_time of one batch:  -0.32990002632141113\n",
            "Iteration:  4675      Current loss:  112.65743255615234      Current accuracy:  0.0      Comp_time of one batch:  -0.3205387592315674\n",
            "Iteration:  4680      Current loss:  101.22112274169922      Current accuracy:  0.0      Comp_time of one batch:  -0.346637487411499\n",
            "Iteration:  4685      Current loss:  117.65010070800781      Current accuracy:  0.0      Comp_time of one batch:  -0.3372988700866699\n",
            "Iteration:  4690      Current loss:  129.2068328857422      Current accuracy:  0.0      Comp_time of one batch:  -0.314558744430542\n",
            "Iteration:  4695      Current loss:  125.77903747558594      Current accuracy:  0.0      Comp_time of one batch:  -0.3145029544830322\n",
            "Iteration:  4700      Current loss:  143.59999084472656      Current accuracy:  0.0      Comp_time of one batch:  -0.3197977542877197\n",
            "Iteration:  4705      Current loss:  159.1306610107422      Current accuracy:  0.0      Comp_time of one batch:  -0.31378984451293945\n",
            "Iteration:  4710      Current loss:  83.65763854980469      Current accuracy:  0.0      Comp_time of one batch:  -0.34497737884521484\n",
            "Iteration:  4715      Current loss:  100.8830795288086      Current accuracy:  0.0      Comp_time of one batch:  -0.363405704498291\n",
            "Iteration:  4720      Current loss:  151.8210906982422      Current accuracy:  0.0      Comp_time of one batch:  -0.3102242946624756\n",
            "Iteration:  4725      Current loss:  121.04842376708984      Current accuracy:  0.0      Comp_time of one batch:  -0.3114016056060791\n",
            "Iteration:  4730      Current loss:  97.1861343383789      Current accuracy:  0.0      Comp_time of one batch:  -0.3410928249359131\n",
            "Iteration:  4735      Current loss:  108.81281280517578      Current accuracy:  0.0      Comp_time of one batch:  -0.33513307571411133\n",
            "Iteration:  4740      Current loss:  115.48139953613281      Current accuracy:  0.0      Comp_time of one batch:  -0.34136295318603516\n",
            "Iteration:  4745      Current loss:  87.6501235961914      Current accuracy:  0.0      Comp_time of one batch:  -0.3368985652923584\n",
            "Iteration:  4750      Current loss:  143.5140838623047      Current accuracy:  0.0      Comp_time of one batch:  -0.3450508117675781\n",
            "Iteration:  4755      Current loss:  114.05838012695312      Current accuracy:  0.0      Comp_time of one batch:  -0.31328916549682617\n",
            "Iteration:  4760      Current loss:  121.47576904296875      Current accuracy:  0.0      Comp_time of one batch:  -0.34013915061950684\n",
            "Iteration:  4765      Current loss:  131.88201904296875      Current accuracy:  0.0      Comp_time of one batch:  -0.31373143196105957\n",
            "Iteration:  4770      Current loss:  113.36235046386719      Current accuracy:  0.0      Comp_time of one batch:  -0.3122541904449463\n",
            "Iteration:  4775      Current loss:  79.31114196777344      Current accuracy:  0.0      Comp_time of one batch:  -0.32416200637817383\n",
            "Iteration:  4780      Current loss:  123.83840942382812      Current accuracy:  0.0      Comp_time of one batch:  -0.30893635749816895\n",
            "Iteration:  4785      Current loss:  156.61044311523438      Current accuracy:  0.0      Comp_time of one batch:  -0.3173959255218506\n",
            "Iteration:  4790      Current loss:  133.86614990234375      Current accuracy:  0.0      Comp_time of one batch:  -0.3176848888397217\n",
            "Iteration:  4795      Current loss:  92.49986267089844      Current accuracy:  0.0      Comp_time of one batch:  -0.3130824565887451\n",
            "Iteration:  4800      Current loss:  182.8628387451172      Current accuracy:  0.0      Comp_time of one batch:  -0.32526612281799316\n",
            "Iteration:  4805      Current loss:  130.55838012695312      Current accuracy:  0.0      Comp_time of one batch:  -0.3257615566253662\n",
            "Iteration:  4810      Current loss:  120.92438507080078      Current accuracy:  0.0      Comp_time of one batch:  -0.3307948112487793\n",
            "Iteration:  4815      Current loss:  87.16193389892578      Current accuracy:  0.0      Comp_time of one batch:  -0.3235011100769043\n",
            "Iteration:  4820      Current loss:  131.28533935546875      Current accuracy:  0.0      Comp_time of one batch:  -0.31402039527893066\n",
            "Iteration:  4825      Current loss:  164.39743041992188      Current accuracy:  0.0      Comp_time of one batch:  -0.31705665588378906\n",
            "Iteration:  4830      Current loss:  95.8982925415039      Current accuracy:  0.0      Comp_time of one batch:  -0.3237931728363037\n",
            "Iteration:  4835      Current loss:  87.25111389160156      Current accuracy:  0.0      Comp_time of one batch:  -0.33855414390563965\n",
            "Iteration:  4840      Current loss:  104.80061340332031      Current accuracy:  0.0      Comp_time of one batch:  -0.34355974197387695\n",
            "Iteration:  4845      Current loss:  105.50680541992188      Current accuracy:  0.0      Comp_time of one batch:  -0.3277883529663086\n",
            "Iteration:  4850      Current loss:  112.79175567626953      Current accuracy:  0.0      Comp_time of one batch:  -0.33111095428466797\n",
            "Iteration:  4855      Current loss:  77.44269561767578      Current accuracy:  0.0      Comp_time of one batch:  -0.3238232135772705\n",
            "Iteration:  4860      Current loss:  111.94976806640625      Current accuracy:  0.0      Comp_time of one batch:  -0.32480621337890625\n",
            "Iteration:  4865      Current loss:  128.35598754882812      Current accuracy:  0.0      Comp_time of one batch:  -0.3205113410949707\n",
            "Iteration:  4870      Current loss:  124.77977752685547      Current accuracy:  0.0      Comp_time of one batch:  -0.3123812675476074\n",
            "Iteration:  4875      Current loss:  136.525146484375      Current accuracy:  0.0      Comp_time of one batch:  -0.31685590744018555\n",
            "Iteration:  4880      Current loss:  84.44529724121094      Current accuracy:  0.0      Comp_time of one batch:  -0.3371717929840088\n",
            "Iteration:  4885      Current loss:  101.91072845458984      Current accuracy:  0.0      Comp_time of one batch:  -0.31514644622802734\n",
            "Iteration:  4890      Current loss:  79.28544616699219      Current accuracy:  0.0      Comp_time of one batch:  -0.31921911239624023\n",
            "Iteration:  4895      Current loss:  119.03013610839844      Current accuracy:  0.0      Comp_time of one batch:  -0.3492271900177002\n",
            "Iteration:  4900      Current loss:  86.35938262939453      Current accuracy:  0.0      Comp_time of one batch:  -0.31255340576171875\n",
            "Iteration:  4905      Current loss:  73.179931640625      Current accuracy:  0.0      Comp_time of one batch:  -0.31511735916137695\n",
            "Iteration:  4910      Current loss:  125.49246978759766      Current accuracy:  0.0      Comp_time of one batch:  -0.33910512924194336\n",
            "Iteration:  4915      Current loss:  97.66871643066406      Current accuracy:  0.0      Comp_time of one batch:  -0.3173990249633789\n",
            "Iteration:  4920      Current loss:  143.67633056640625      Current accuracy:  0.0      Comp_time of one batch:  -0.32027220726013184\n",
            "Iteration:  4925      Current loss:  109.98682403564453      Current accuracy:  0.0      Comp_time of one batch:  -0.3376016616821289\n",
            "Iteration:  4930      Current loss:  81.15556335449219      Current accuracy:  0.0      Comp_time of one batch:  -0.3192424774169922\n",
            "Iteration:  4935      Current loss:  95.44664764404297      Current accuracy:  0.0      Comp_time of one batch:  -0.318070650100708\n",
            "Iteration:  4940      Current loss:  113.43875122070312      Current accuracy:  0.0      Comp_time of one batch:  -0.34064745903015137\n",
            "Iteration:  4945      Current loss:  140.49002075195312      Current accuracy:  0.0      Comp_time of one batch:  -0.3164100646972656\n",
            "Iteration:  4950      Current loss:  67.85834503173828      Current accuracy:  0.0      Comp_time of one batch:  -0.31434178352355957\n",
            "Iteration:  4955      Current loss:  106.90162658691406      Current accuracy:  0.0      Comp_time of one batch:  -0.3205840587615967\n",
            "Iteration:  4960      Current loss:  99.266357421875      Current accuracy:  0.0      Comp_time of one batch:  -0.3142681121826172\n",
            "Iteration:  4965      Current loss:  100.04005432128906      Current accuracy:  0.0      Comp_time of one batch:  -0.31533002853393555\n",
            "Iteration:  4970      Current loss:  116.1352310180664      Current accuracy:  0.0      Comp_time of one batch:  -0.3232765197753906\n",
            "Iteration:  4975      Current loss:  112.86442565917969      Current accuracy:  0.0      Comp_time of one batch:  -0.32158780097961426\n",
            "Iteration:  4980      Current loss:  94.06704711914062      Current accuracy:  0.0      Comp_time of one batch:  -0.31771397590637207\n",
            "Iteration:  4985      Current loss:  80.05818176269531      Current accuracy:  0.0      Comp_time of one batch:  -0.32398319244384766\n",
            "Iteration:  4990      Current loss:  156.72335815429688      Current accuracy:  0.0      Comp_time of one batch:  -0.31632494926452637\n",
            "Iteration:  4995      Current loss:  67.87165832519531      Current accuracy:  0.0      Comp_time of one batch:  -0.3156733512878418\n",
            "Iteration:  5000      Current loss:  141.2836456298828      Current accuracy:  0.0      Comp_time of one batch:  -0.3262357711791992\n",
            "Iteration:  5005      Current loss:  111.2110824584961      Current accuracy:  0.0      Comp_time of one batch:  -0.3159928321838379\n",
            "Iteration:  5010      Current loss:  115.79544067382812      Current accuracy:  0.0      Comp_time of one batch:  -0.31641244888305664\n",
            "Iteration:  5015      Current loss:  113.82762145996094      Current accuracy:  0.0      Comp_time of one batch:  -0.3172578811645508\n",
            "Iteration:  5020      Current loss:  107.48341369628906      Current accuracy:  0.0      Comp_time of one batch:  -0.31658935546875\n",
            "Iteration:  5025      Current loss:  91.22631072998047      Current accuracy:  0.0      Comp_time of one batch:  -0.31540536880493164\n",
            "Iteration:  5030      Current loss:  133.06163024902344      Current accuracy:  0.0      Comp_time of one batch:  -0.31275248527526855\n",
            "Iteration:  5035      Current loss:  90.16753387451172      Current accuracy:  0.0      Comp_time of one batch:  -0.3119783401489258\n",
            "Iteration:  5040      Current loss:  122.26055145263672      Current accuracy:  0.0      Comp_time of one batch:  -0.3106675148010254\n",
            "Iteration:  5045      Current loss:  75.53331756591797      Current accuracy:  0.0      Comp_time of one batch:  -0.31345510482788086\n",
            "Iteration:  5050      Current loss:  88.30652618408203      Current accuracy:  0.0      Comp_time of one batch:  -0.3388497829437256\n",
            "Iteration:  5055      Current loss:  102.26953125      Current accuracy:  0.0      Comp_time of one batch:  -0.34319448471069336\n",
            "Iteration:  5060      Current loss:  146.65338134765625      Current accuracy:  0.0      Comp_time of one batch:  -0.33841991424560547\n",
            "Iteration:  5065      Current loss:  81.2423324584961      Current accuracy:  0.0      Comp_time of one batch:  -0.3338181972503662\n",
            "Iteration:  5070      Current loss:  188.5409698486328      Current accuracy:  0.0      Comp_time of one batch:  -0.32230401039123535\n",
            "Iteration:  5075      Current loss:  128.7351837158203      Current accuracy:  0.0      Comp_time of one batch:  -0.33726954460144043\n",
            "Iteration:  5080      Current loss:  81.19644927978516      Current accuracy:  0.0      Comp_time of one batch:  -0.34487462043762207\n",
            "Iteration:  5085      Current loss:  141.24119567871094      Current accuracy:  0.0      Comp_time of one batch:  -0.3330411911010742\n",
            "Iteration:  5090      Current loss:  90.38090515136719      Current accuracy:  0.0      Comp_time of one batch:  -0.3377413749694824\n",
            "Iteration:  5095      Current loss:  86.94559478759766      Current accuracy:  0.0      Comp_time of one batch:  -0.3154416084289551\n",
            "Iteration:  5100      Current loss:  95.71170806884766      Current accuracy:  0.0      Comp_time of one batch:  -0.3580758571624756\n",
            "Iteration:  5105      Current loss:  172.88619995117188      Current accuracy:  0.0      Comp_time of one batch:  -0.3450350761413574\n",
            "Iteration:  5110      Current loss:  154.8246612548828      Current accuracy:  0.0      Comp_time of one batch:  -0.3551785945892334\n",
            "Iteration:  5115      Current loss:  118.73199462890625      Current accuracy:  0.0      Comp_time of one batch:  -0.32986021041870117\n",
            "Iteration:  5120      Current loss:  87.8990249633789      Current accuracy:  0.0      Comp_time of one batch:  -0.32604146003723145\n",
            "Iteration:  5125      Current loss:  115.18833923339844      Current accuracy:  0.0      Comp_time of one batch:  -0.31507444381713867\n",
            "Iteration:  5130      Current loss:  109.77647399902344      Current accuracy:  0.0      Comp_time of one batch:  -0.312732458114624\n",
            "Iteration:  5135      Current loss:  104.60671997070312      Current accuracy:  0.0      Comp_time of one batch:  -0.3227047920227051\n",
            "Iteration:  5140      Current loss:  117.36743927001953      Current accuracy:  0.0      Comp_time of one batch:  -0.33191442489624023\n",
            "Iteration:  5145      Current loss:  131.32308959960938      Current accuracy:  0.0      Comp_time of one batch:  -0.3311610221862793\n",
            "Iteration:  5150      Current loss:  87.06407928466797      Current accuracy:  0.0      Comp_time of one batch:  -0.318575382232666\n",
            "Iteration:  5155      Current loss:  94.68488311767578      Current accuracy:  0.0      Comp_time of one batch:  -0.3121969699859619\n",
            "Iteration:  5160      Current loss:  114.94239807128906      Current accuracy:  0.0      Comp_time of one batch:  -0.3148796558380127\n",
            "Iteration:  5165      Current loss:  91.7217788696289      Current accuracy:  0.0      Comp_time of one batch:  -0.31482648849487305\n",
            "Iteration:  5170      Current loss:  117.60856628417969      Current accuracy:  0.0      Comp_time of one batch:  -0.32553768157958984\n",
            "Iteration:  5175      Current loss:  141.16661071777344      Current accuracy:  0.0      Comp_time of one batch:  -0.31278038024902344\n",
            "Iteration:  5180      Current loss:  106.16802978515625      Current accuracy:  0.0      Comp_time of one batch:  -0.31212806701660156\n",
            "Iteration:  5185      Current loss:  118.45226287841797      Current accuracy:  0.0      Comp_time of one batch:  -0.3206050395965576\n",
            "Iteration:  5190      Current loss:  84.83815002441406      Current accuracy:  0.0      Comp_time of one batch:  -0.310610294342041\n",
            "Iteration:  5195      Current loss:  111.49668884277344      Current accuracy:  0.0      Comp_time of one batch:  -0.31014037132263184\n",
            "Iteration:  5200      Current loss:  153.42340087890625      Current accuracy:  0.0      Comp_time of one batch:  -0.3467240333557129\n",
            "Iteration:  5205      Current loss:  77.3293228149414      Current accuracy:  0.0      Comp_time of one batch:  -0.31998729705810547\n",
            "Iteration:  5210      Current loss:  96.74427032470703      Current accuracy:  0.0      Comp_time of one batch:  -0.3162858486175537\n",
            "Iteration:  5215      Current loss:  73.55744171142578      Current accuracy:  0.0      Comp_time of one batch:  -0.33603501319885254\n",
            "Iteration:  5220      Current loss:  97.00768280029297      Current accuracy:  0.0      Comp_time of one batch:  -0.3332993984222412\n",
            "Iteration:  5225      Current loss:  112.24360656738281      Current accuracy:  0.0      Comp_time of one batch:  -0.3750019073486328\n",
            "Iteration:  5230      Current loss:  158.5247039794922      Current accuracy:  0.0      Comp_time of one batch:  -0.339080810546875\n",
            "Iteration:  5235      Current loss:  129.68215942382812      Current accuracy:  0.0      Comp_time of one batch:  -0.3082282543182373\n",
            "Iteration:  5240      Current loss:  131.04623413085938      Current accuracy:  0.0      Comp_time of one batch:  -0.33028197288513184\n",
            "Iteration:  5245      Current loss:  145.31735229492188      Current accuracy:  0.0      Comp_time of one batch:  -0.3301410675048828\n",
            "Iteration:  5250      Current loss:  82.11702728271484      Current accuracy:  0.0      Comp_time of one batch:  -0.30796146392822266\n",
            "Iteration:  5255      Current loss:  154.80484008789062      Current accuracy:  0.0      Comp_time of one batch:  -0.33369946479797363\n",
            "Iteration:  5260      Current loss:  84.99197387695312      Current accuracy:  0.0      Comp_time of one batch:  -0.356048583984375\n",
            "Iteration:  5265      Current loss:  81.05245971679688      Current accuracy:  0.0      Comp_time of one batch:  -0.3555488586425781\n",
            "Iteration:  5270      Current loss:  96.51021575927734      Current accuracy:  0.0      Comp_time of one batch:  -0.3121957778930664\n",
            "Iteration:  5275      Current loss:  125.08497619628906      Current accuracy:  0.0      Comp_time of one batch:  -0.3150289058685303\n",
            "Iteration:  5280      Current loss:  96.19351196289062      Current accuracy:  0.0      Comp_time of one batch:  -0.31656718254089355\n",
            "Iteration:  5285      Current loss:  101.36888885498047      Current accuracy:  0.0      Comp_time of one batch:  -0.3173971176147461\n",
            "Iteration:  5290      Current loss:  129.60447692871094      Current accuracy:  0.0      Comp_time of one batch:  -0.3375074863433838\n",
            "Iteration:  5295      Current loss:  116.42057037353516      Current accuracy:  0.0      Comp_time of one batch:  -0.3301980495452881\n",
            "Iteration:  5300      Current loss:  159.0460968017578      Current accuracy:  0.0      Comp_time of one batch:  -0.3142244815826416\n",
            "Iteration:  5305      Current loss:  86.63601684570312      Current accuracy:  0.0      Comp_time of one batch:  -0.32138895988464355\n",
            "Iteration:  5310      Current loss:  94.36223602294922      Current accuracy:  0.0      Comp_time of one batch:  -0.30968546867370605\n",
            "Iteration:  5315      Current loss:  95.67160034179688      Current accuracy:  0.0      Comp_time of one batch:  -0.31121277809143066\n",
            "Iteration:  5320      Current loss:  100.51763153076172      Current accuracy:  0.0      Comp_time of one batch:  -0.332791805267334\n",
            "Iteration:  5325      Current loss:  89.18431854248047      Current accuracy:  0.0      Comp_time of one batch:  -0.31979918479919434\n",
            "Iteration:  5330      Current loss:  90.91156768798828      Current accuracy:  0.0      Comp_time of one batch:  -0.3120412826538086\n",
            "Iteration:  5335      Current loss:  152.02395629882812      Current accuracy:  0.0      Comp_time of one batch:  -0.31409358978271484\n",
            "Iteration:  5340      Current loss:  103.1723403930664      Current accuracy:  0.0      Comp_time of one batch:  -0.31316256523132324\n",
            "Iteration:  5345      Current loss:  118.05528259277344      Current accuracy:  0.0      Comp_time of one batch:  -0.32158398628234863\n",
            "Iteration:  5350      Current loss:  85.21051025390625      Current accuracy:  0.0      Comp_time of one batch:  -0.3284761905670166\n",
            "Iteration:  5355      Current loss:  93.1715316772461      Current accuracy:  0.0      Comp_time of one batch:  -0.31366920471191406\n",
            "Iteration:  5360      Current loss:  136.05116271972656      Current accuracy:  0.0      Comp_time of one batch:  -0.3355221748352051\n",
            "Iteration:  5365      Current loss:  140.0150909423828      Current accuracy:  0.0      Comp_time of one batch:  -0.32334160804748535\n",
            "Iteration:  5370      Current loss:  69.13433837890625      Current accuracy:  0.0      Comp_time of one batch:  -0.3191359043121338\n",
            "Iteration:  5375      Current loss:  142.3712158203125      Current accuracy:  0.0      Comp_time of one batch:  -0.31355929374694824\n",
            "Iteration:  5380      Current loss:  126.7279281616211      Current accuracy:  0.0      Comp_time of one batch:  -0.34346652030944824\n",
            "Iteration:  5385      Current loss:  167.48895263671875      Current accuracy:  0.0      Comp_time of one batch:  -0.3461127281188965\n",
            "Iteration:  5390      Current loss:  182.58396911621094      Current accuracy:  0.0      Comp_time of one batch:  -0.32712841033935547\n",
            "Iteration:  5395      Current loss:  104.46331787109375      Current accuracy:  0.0      Comp_time of one batch:  -0.316817045211792\n",
            "Iteration:  5400      Current loss:  101.7907943725586      Current accuracy:  0.0      Comp_time of one batch:  -0.30913686752319336\n",
            "Iteration:  5405      Current loss:  100.88284301757812      Current accuracy:  0.0      Comp_time of one batch:  -0.33176255226135254\n",
            "Iteration:  5410      Current loss:  146.86337280273438      Current accuracy:  0.0      Comp_time of one batch:  -0.31955599784851074\n",
            "Iteration:  5415      Current loss:  110.33566284179688      Current accuracy:  0.0      Comp_time of one batch:  -0.329700231552124\n",
            "Iteration:  5420      Current loss:  95.69488525390625      Current accuracy:  0.0      Comp_time of one batch:  -0.31503939628601074\n",
            "Iteration:  5425      Current loss:  79.2914810180664      Current accuracy:  0.0      Comp_time of one batch:  -0.31778717041015625\n",
            "Iteration:  5430      Current loss:  89.63629913330078      Current accuracy:  0.0      Comp_time of one batch:  -0.314985990524292\n",
            "Iteration:  5435      Current loss:  110.16145324707031      Current accuracy:  0.0      Comp_time of one batch:  -0.3163483142852783\n",
            "Iteration:  5440      Current loss:  71.65959167480469      Current accuracy:  0.0      Comp_time of one batch:  -0.3134734630584717\n",
            "Iteration:  5445      Current loss:  77.02210235595703      Current accuracy:  0.0      Comp_time of one batch:  -0.3364746570587158\n",
            "Iteration:  5450      Current loss:  117.02299499511719      Current accuracy:  0.0      Comp_time of one batch:  -0.3176612854003906\n",
            "Iteration:  5455      Current loss:  122.79784393310547      Current accuracy:  0.0      Comp_time of one batch:  -0.33427000045776367\n",
            "Iteration:  5460      Current loss:  124.71027374267578      Current accuracy:  0.0      Comp_time of one batch:  -0.31456518173217773\n",
            "Iteration:  5465      Current loss:  123.16283416748047      Current accuracy:  0.0      Comp_time of one batch:  -0.31200504302978516\n",
            "Iteration:  5470      Current loss:  83.3521728515625      Current accuracy:  0.0      Comp_time of one batch:  -0.31440019607543945\n",
            "Iteration:  5475      Current loss:  110.11686706542969      Current accuracy:  0.0      Comp_time of one batch:  -0.3331754207611084\n",
            "Iteration:  5480      Current loss:  104.81476593017578      Current accuracy:  0.0      Comp_time of one batch:  -0.321155309677124\n",
            "Iteration:  5485      Current loss:  91.43679809570312      Current accuracy:  0.0      Comp_time of one batch:  -0.31445741653442383\n",
            "Iteration:  5490      Current loss:  132.90145874023438      Current accuracy:  0.0      Comp_time of one batch:  -0.3176569938659668\n",
            "Iteration:  5495      Current loss:  94.24877166748047      Current accuracy:  0.0      Comp_time of one batch:  -0.3100707530975342\n",
            "Iteration:  5500      Current loss:  102.37174224853516      Current accuracy:  0.0      Comp_time of one batch:  -0.31624293327331543\n",
            "Iteration:  5505      Current loss:  92.1616439819336      Current accuracy:  0.0      Comp_time of one batch:  -0.3094608783721924\n",
            "Iteration:  5510      Current loss:  88.2567367553711      Current accuracy:  0.0      Comp_time of one batch:  -0.3199336528778076\n",
            "Iteration:  5515      Current loss:  133.7573699951172      Current accuracy:  0.0      Comp_time of one batch:  -0.3117356300354004\n",
            "Iteration:  5520      Current loss:  148.78370666503906      Current accuracy:  0.0      Comp_time of one batch:  -0.32543349266052246\n",
            "Iteration:  5525      Current loss:  105.6325454711914      Current accuracy:  0.0      Comp_time of one batch:  -0.3353302478790283\n",
            "Iteration:  5530      Current loss:  117.09061431884766      Current accuracy:  0.0      Comp_time of one batch:  -0.37313079833984375\n",
            "Iteration:  5535      Current loss:  111.74597930908203      Current accuracy:  0.0      Comp_time of one batch:  -0.34227728843688965\n",
            "Iteration:  5540      Current loss:  118.82625579833984      Current accuracy:  0.0      Comp_time of one batch:  -0.35584378242492676\n",
            "Iteration:  5545      Current loss:  142.82635498046875      Current accuracy:  0.0      Comp_time of one batch:  -0.3699758052825928\n",
            "Iteration:  5550      Current loss:  135.55108642578125      Current accuracy:  0.0      Comp_time of one batch:  -0.336658239364624\n",
            "Iteration:  5555      Current loss:  109.22547912597656      Current accuracy:  0.0      Comp_time of one batch:  -0.31401681900024414\n",
            "Iteration:  5560      Current loss:  105.7299575805664      Current accuracy:  0.0      Comp_time of one batch:  -0.33646130561828613\n",
            "Iteration:  5565      Current loss:  114.95960235595703      Current accuracy:  0.0      Comp_time of one batch:  -0.335690975189209\n",
            "Iteration:  5570      Current loss:  119.09514617919922      Current accuracy:  0.0      Comp_time of one batch:  -0.3325083255767822\n",
            "Iteration:  5575      Current loss:  126.36212158203125      Current accuracy:  0.0      Comp_time of one batch:  -0.33588504791259766\n",
            "Iteration:  5580      Current loss:  95.00440979003906      Current accuracy:  0.0      Comp_time of one batch:  -0.33193421363830566\n",
            "Iteration:  5585      Current loss:  129.860595703125      Current accuracy:  0.0      Comp_time of one batch:  -0.3333606719970703\n",
            "Iteration:  5590      Current loss:  156.69822692871094      Current accuracy:  0.0      Comp_time of one batch:  -0.325467586517334\n",
            "Iteration:  5595      Current loss:  115.82044982910156      Current accuracy:  0.0      Comp_time of one batch:  -0.3341383934020996\n",
            "Iteration:  5600      Current loss:  134.2821044921875      Current accuracy:  0.0      Comp_time of one batch:  -0.3172171115875244\n",
            "Iteration:  5605      Current loss:  124.71815490722656      Current accuracy:  0.0      Comp_time of one batch:  -0.3116793632507324\n",
            "Iteration:  5610      Current loss:  148.03187561035156      Current accuracy:  0.0      Comp_time of one batch:  -0.32543420791625977\n",
            "Iteration:  5615      Current loss:  150.4188232421875      Current accuracy:  0.0      Comp_time of one batch:  -0.3343932628631592\n",
            "Iteration:  5620      Current loss:  122.00045776367188      Current accuracy:  0.0      Comp_time of one batch:  -0.3127903938293457\n",
            "Iteration:  5625      Current loss:  161.08963012695312      Current accuracy:  0.0      Comp_time of one batch:  -0.33272218704223633\n",
            "Iteration:  5630      Current loss:  84.96544647216797      Current accuracy:  0.0      Comp_time of one batch:  -0.322110652923584\n",
            "Iteration:  5635      Current loss:  112.79753112792969      Current accuracy:  0.0      Comp_time of one batch:  -0.344118595123291\n",
            "Iteration:  5640      Current loss:  105.57737731933594      Current accuracy:  0.0      Comp_time of one batch:  -0.32109832763671875\n",
            "Iteration:  5645      Current loss:  147.0943603515625      Current accuracy:  0.0      Comp_time of one batch:  -0.32041239738464355\n",
            "Iteration:  5650      Current loss:  109.52478790283203      Current accuracy:  0.0      Comp_time of one batch:  -0.34519028663635254\n",
            "Iteration:  5655      Current loss:  101.28218078613281      Current accuracy:  0.0      Comp_time of one batch:  -0.3143458366394043\n",
            "Iteration:  5660      Current loss:  128.9010009765625      Current accuracy:  0.0      Comp_time of one batch:  -0.3156449794769287\n",
            "Iteration:  5665      Current loss:  97.49811553955078      Current accuracy:  0.0      Comp_time of one batch:  -0.3414127826690674\n",
            "Iteration:  5670      Current loss:  169.05123901367188      Current accuracy:  0.0      Comp_time of one batch:  -0.3697471618652344\n",
            "Iteration:  5675      Current loss:  75.66563415527344      Current accuracy:  0.0      Comp_time of one batch:  -0.34687376022338867\n",
            "Iteration:  5680      Current loss:  112.03601837158203      Current accuracy:  0.0      Comp_time of one batch:  -0.34580397605895996\n",
            "Iteration:  5685      Current loss:  96.60631561279297      Current accuracy:  0.0      Comp_time of one batch:  -0.3506009578704834\n",
            "Iteration:  5690      Current loss:  98.30388641357422      Current accuracy:  0.0      Comp_time of one batch:  -0.33510875701904297\n",
            "Iteration:  5695      Current loss:  136.7401123046875      Current accuracy:  0.0      Comp_time of one batch:  -0.316119909286499\n",
            "Iteration:  5700      Current loss:  120.38301086425781      Current accuracy:  0.0      Comp_time of one batch:  -0.346027135848999\n",
            "Iteration:  5705      Current loss:  113.74444580078125      Current accuracy:  0.0      Comp_time of one batch:  -0.3213953971862793\n",
            "Iteration:  5710      Current loss:  109.72807312011719      Current accuracy:  0.0      Comp_time of one batch:  -0.32280445098876953\n",
            "Iteration:  5715      Current loss:  141.8140106201172      Current accuracy:  0.0      Comp_time of one batch:  -0.3338899612426758\n",
            "Iteration:  5720      Current loss:  107.10863494873047      Current accuracy:  0.0      Comp_time of one batch:  -0.34394311904907227\n",
            "Iteration:  5725      Current loss:  133.82504272460938      Current accuracy:  0.0      Comp_time of one batch:  -0.32024264335632324\n",
            "Iteration:  5730      Current loss:  124.00743865966797      Current accuracy:  0.0      Comp_time of one batch:  -0.3234117031097412\n",
            "Iteration:  5735      Current loss:  128.01242065429688      Current accuracy:  0.0      Comp_time of one batch:  -0.3287680149078369\n",
            "Iteration:  5740      Current loss:  87.70079803466797      Current accuracy:  0.0      Comp_time of one batch:  -0.3206138610839844\n",
            "Iteration:  5745      Current loss:  128.25079345703125      Current accuracy:  0.0      Comp_time of one batch:  -0.3130302429199219\n",
            "Iteration:  5750      Current loss:  82.67584991455078      Current accuracy:  0.0      Comp_time of one batch:  -0.3204317092895508\n",
            "Iteration:  5755      Current loss:  93.4351577758789      Current accuracy:  0.0      Comp_time of one batch:  -0.31936168670654297\n",
            "Iteration:  5760      Current loss:  86.01528930664062      Current accuracy:  0.0      Comp_time of one batch:  -0.3342876434326172\n",
            "Iteration:  5765      Current loss:  97.78563690185547      Current accuracy:  0.0      Comp_time of one batch:  -0.3347160816192627\n",
            "Iteration:  5770      Current loss:  145.6977996826172      Current accuracy:  0.0      Comp_time of one batch:  -0.31006908416748047\n",
            "Iteration:  5775      Current loss:  140.52584838867188      Current accuracy:  0.0      Comp_time of one batch:  -0.34283924102783203\n",
            "Iteration:  5780      Current loss:  125.55012512207031      Current accuracy:  0.0      Comp_time of one batch:  -0.32115864753723145\n",
            "Iteration:  5785      Current loss:  83.97083282470703      Current accuracy:  0.0      Comp_time of one batch:  -0.33693361282348633\n",
            "Iteration:  5790      Current loss:  103.84204864501953      Current accuracy:  0.0      Comp_time of one batch:  -0.3311305046081543\n",
            "Iteration:  5795      Current loss:  117.79084014892578      Current accuracy:  0.0      Comp_time of one batch:  -0.3342416286468506\n",
            "Iteration:  5800      Current loss:  168.0864715576172      Current accuracy:  0.0      Comp_time of one batch:  -0.34639620780944824\n",
            "Iteration:  5805      Current loss:  93.80376434326172      Current accuracy:  0.0      Comp_time of one batch:  -0.31356334686279297\n",
            "Iteration:  5810      Current loss:  131.42002868652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3395347595214844\n",
            "Iteration:  5815      Current loss:  103.22889709472656      Current accuracy:  0.0      Comp_time of one batch:  -0.33528923988342285\n",
            "Iteration:  5820      Current loss:  95.34038543701172      Current accuracy:  0.0      Comp_time of one batch:  -0.31374287605285645\n",
            "Iteration:  5825      Current loss:  66.72139739990234      Current accuracy:  0.0      Comp_time of one batch:  -0.335097074508667\n",
            "Iteration:  5830      Current loss:  95.41302490234375      Current accuracy:  0.0      Comp_time of one batch:  -0.33690309524536133\n",
            "Iteration:  5835      Current loss:  127.3016586303711      Current accuracy:  0.0      Comp_time of one batch:  -0.33878183364868164\n",
            "Iteration:  5840      Current loss:  102.25598907470703      Current accuracy:  0.0      Comp_time of one batch:  -0.319749116897583\n",
            "Iteration:  5845      Current loss:  85.91218566894531      Current accuracy:  0.0      Comp_time of one batch:  -0.3365771770477295\n",
            "Iteration:  5850      Current loss:  180.6168212890625      Current accuracy:  0.0      Comp_time of one batch:  -0.3503406047821045\n",
            "Iteration:  5855      Current loss:  137.40869140625      Current accuracy:  0.0      Comp_time of one batch:  -0.339524507522583\n",
            "Iteration:  5860      Current loss:  115.68058013916016      Current accuracy:  0.0      Comp_time of one batch:  -0.33309412002563477\n",
            "Iteration:  5865      Current loss:  109.85035705566406      Current accuracy:  0.0      Comp_time of one batch:  -0.35930562019348145\n",
            "Iteration:  5870      Current loss:  88.4697036743164      Current accuracy:  0.0      Comp_time of one batch:  -0.3402669429779053\n",
            "Iteration:  5875      Current loss:  88.45429229736328      Current accuracy:  0.0      Comp_time of one batch:  -0.33705806732177734\n",
            "Iteration:  5880      Current loss:  85.72053527832031      Current accuracy:  0.0      Comp_time of one batch:  -0.33112049102783203\n",
            "Iteration:  5885      Current loss:  66.27823638916016      Current accuracy:  0.0      Comp_time of one batch:  -0.34493589401245117\n",
            "Iteration:  5890      Current loss:  139.8268280029297      Current accuracy:  0.0      Comp_time of one batch:  -0.32411909103393555\n",
            "Iteration:  5895      Current loss:  120.57717895507812      Current accuracy:  0.0      Comp_time of one batch:  -0.3166193962097168\n",
            "Iteration:  5900      Current loss:  100.20956420898438      Current accuracy:  0.0      Comp_time of one batch:  -0.3223435878753662\n",
            "Iteration:  5905      Current loss:  84.32832336425781      Current accuracy:  0.0      Comp_time of one batch:  -0.31647372245788574\n",
            "Iteration:  5910      Current loss:  132.6425323486328      Current accuracy:  0.0      Comp_time of one batch:  -0.3100473880767822\n",
            "Iteration:  5915      Current loss:  131.29180908203125      Current accuracy:  0.0      Comp_time of one batch:  -0.3195056915283203\n",
            "Iteration:  5920      Current loss:  124.82752227783203      Current accuracy:  0.0      Comp_time of one batch:  -0.3343689441680908\n",
            "Iteration:  5925      Current loss:  121.01477813720703      Current accuracy:  0.0      Comp_time of one batch:  -0.3117649555206299\n",
            "Iteration:  5930      Current loss:  123.34870910644531      Current accuracy:  0.0      Comp_time of one batch:  -0.3106265068054199\n",
            "Iteration:  5935      Current loss:  120.16401672363281      Current accuracy:  0.0      Comp_time of one batch:  -0.3443613052368164\n",
            "Iteration:  5940      Current loss:  114.03789520263672      Current accuracy:  0.0      Comp_time of one batch:  -0.32888293266296387\n",
            "Iteration:  5945      Current loss:  90.03888702392578      Current accuracy:  0.0      Comp_time of one batch:  -0.3564941883087158\n",
            "Iteration:  5950      Current loss:  130.9990234375      Current accuracy:  0.0      Comp_time of one batch:  -0.33237433433532715\n",
            "Iteration:  5955      Current loss:  84.30120086669922      Current accuracy:  0.0      Comp_time of one batch:  -0.31243443489074707\n",
            "Iteration:  5960      Current loss:  89.09760284423828      Current accuracy:  0.0      Comp_time of one batch:  -0.35051631927490234\n",
            "Iteration:  5965      Current loss:  134.1820068359375      Current accuracy:  0.0      Comp_time of one batch:  -0.3224451541900635\n",
            "Iteration:  5970      Current loss:  95.84430694580078      Current accuracy:  0.0      Comp_time of one batch:  -0.32023167610168457\n",
            "Iteration:  5975      Current loss:  110.02207946777344      Current accuracy:  0.0      Comp_time of one batch:  -0.32468414306640625\n",
            "Iteration:  5980      Current loss:  131.21485900878906      Current accuracy:  0.0      Comp_time of one batch:  -0.31758952140808105\n",
            "Iteration:  5985      Current loss:  79.61210632324219      Current accuracy:  0.0      Comp_time of one batch:  -0.3215205669403076\n",
            "Iteration:  5990      Current loss:  104.06055450439453      Current accuracy:  0.0      Comp_time of one batch:  -0.3156406879425049\n",
            "Iteration:  5995      Current loss:  121.36267852783203      Current accuracy:  0.0      Comp_time of one batch:  -0.33215904235839844\n",
            "Iteration:  6000      Current loss:  108.04647064208984      Current accuracy:  0.0      Comp_time of one batch:  -0.325376033782959\n",
            "Iteration:  6005      Current loss:  106.35590362548828      Current accuracy:  0.0      Comp_time of one batch:  -0.32763099670410156\n",
            "Iteration:  6010      Current loss:  88.44556427001953      Current accuracy:  0.0      Comp_time of one batch:  -0.31832289695739746\n",
            "Iteration:  6015      Current loss:  105.42239379882812      Current accuracy:  0.0      Comp_time of one batch:  -0.3425464630126953\n",
            "Iteration:  6020      Current loss:  89.27436065673828      Current accuracy:  0.0      Comp_time of one batch:  -0.3132457733154297\n",
            "Iteration:  6025      Current loss:  131.02252197265625      Current accuracy:  0.0      Comp_time of one batch:  -0.3321347236633301\n",
            "Iteration:  6030      Current loss:  166.17164611816406      Current accuracy:  0.0      Comp_time of one batch:  -0.33055973052978516\n",
            "Iteration:  6035      Current loss:  88.42850494384766      Current accuracy:  0.0      Comp_time of one batch:  -0.31259608268737793\n",
            "Iteration:  6040      Current loss:  117.02372741699219      Current accuracy:  0.0      Comp_time of one batch:  -0.31862783432006836\n",
            "Iteration:  6045      Current loss:  146.51953125      Current accuracy:  0.0      Comp_time of one batch:  -0.34284114837646484\n",
            "Iteration:  6050      Current loss:  120.68421173095703      Current accuracy:  0.0      Comp_time of one batch:  -0.3111002445220947\n",
            "Iteration:  6055      Current loss:  115.90642547607422      Current accuracy:  0.0      Comp_time of one batch:  -0.31580400466918945\n",
            "Iteration:  6060      Current loss:  165.2076416015625      Current accuracy:  0.0      Comp_time of one batch:  -0.34088683128356934\n",
            "Iteration:  6065      Current loss:  117.7540512084961      Current accuracy:  0.0      Comp_time of one batch:  -0.3292725086212158\n",
            "Iteration:  6070      Current loss:  121.33720397949219      Current accuracy:  0.0      Comp_time of one batch:  -0.31630802154541016\n",
            "Iteration:  6075      Current loss:  137.288818359375      Current accuracy:  0.0      Comp_time of one batch:  -0.3137667179107666\n",
            "Iteration:  6080      Current loss:  138.14813232421875      Current accuracy:  0.0      Comp_time of one batch:  -0.31414270401000977\n",
            "Iteration:  6085      Current loss:  71.92219543457031      Current accuracy:  0.0      Comp_time of one batch:  -0.3176281452178955\n",
            "Iteration:  6090      Current loss:  57.99298858642578      Current accuracy:  0.0      Comp_time of one batch:  -0.3297562599182129\n",
            "Iteration:  6095      Current loss:  113.8573226928711      Current accuracy:  0.0      Comp_time of one batch:  -0.3110499382019043\n",
            "Iteration:  6100      Current loss:  113.5494155883789      Current accuracy:  0.0      Comp_time of one batch:  -0.31485629081726074\n",
            "Iteration:  6105      Current loss:  160.67236328125      Current accuracy:  0.0      Comp_time of one batch:  -0.33041977882385254\n",
            "Iteration:  6110      Current loss:  112.70443725585938      Current accuracy:  0.0      Comp_time of one batch:  -0.3076441287994385\n",
            "Iteration:  6115      Current loss:  118.29613494873047      Current accuracy:  0.0      Comp_time of one batch:  -0.3403956890106201\n",
            "Iteration:  6120      Current loss:  133.77708435058594      Current accuracy:  0.0      Comp_time of one batch:  -0.3374361991882324\n",
            "Iteration:  6125      Current loss:  155.33226013183594      Current accuracy:  0.0      Comp_time of one batch:  -0.31606388092041016\n",
            "Iteration:  6130      Current loss:  132.99913024902344      Current accuracy:  0.0      Comp_time of one batch:  -0.31908392906188965\n",
            "Iteration:  6135      Current loss:  115.0449447631836      Current accuracy:  0.0      Comp_time of one batch:  -0.3220984935760498\n",
            "Iteration:  6140      Current loss:  132.5753936767578      Current accuracy:  0.0      Comp_time of one batch:  -0.3161661624908447\n",
            "Iteration:  6145      Current loss:  108.6693115234375      Current accuracy:  0.0      Comp_time of one batch:  -0.3196084499359131\n",
            "Iteration:  6150      Current loss:  131.19346618652344      Current accuracy:  0.0      Comp_time of one batch:  -0.3070504665374756\n",
            "Iteration:  6155      Current loss:  116.71542358398438      Current accuracy:  0.0      Comp_time of one batch:  -0.3035719394683838\n",
            "Iteration:  6160      Current loss:  78.93677520751953      Current accuracy:  0.0      Comp_time of one batch:  -0.3111135959625244\n",
            "Iteration:  6165      Current loss:  94.80614471435547      Current accuracy:  0.0      Comp_time of one batch:  -0.3128647804260254\n",
            "Iteration:  6170      Current loss:  117.54588317871094      Current accuracy:  0.0      Comp_time of one batch:  -0.3085014820098877\n",
            "Iteration:  6174      Current loss:  124.12517547607422      Current accuracy:  0.0      Comp_time of one batch:  -0.26445889472961426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for e in range(len(depth_maps)):\n",
        "    os.mkdir(\"/home/epoch%1d\" % e)\n",
        "    for i in range(len(depth_maps[0])):\n",
        "        for j in range(len(depth_maps[0][0])):\n",
        "            est_map = depth_maps[0][i][0][j]\n",
        "            gt_map = depth_maps[0][i][1][j]\n",
        "            ref = depth_maps[0][i][2][j]\n",
        "            ref = np.swapaxes(ref, 0, 1)\n",
        "            ref = np.swapaxes(ref,1, 2)\n",
        "            #ref = np.uint8(ref)\n",
        "            visualize.show_depth_img(est_map, \"epoch%1d/\" % e + \"est%1d\" % i)\n",
        "            visualize.show_depth_img(gt_map,\"epoch%1d/\" % e + \"gt%1d\" % i)\n",
        "            plt.imshow(ref)\n",
        "            plt.savefig(\"/home/epoch%1d/\" % e + \"ref%1d\" % i)\n",
        "            plt.show(block=True)\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "Q0lRmkHCMwK2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "475942fa-c23e-4873-905b-714e2e0e7697"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9b5450fc6ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/epoch%1d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/epoch0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#come up with something better at some point\n",
        "expanded_val_loss = []\n",
        "expanded_val_acc = []\n",
        "for i in range(len(loss_val)):\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    if i % 2 == 0:\n",
        "        expanded_val_loss.append(loss_val[i])\n",
        "        expanded_val_acc.append(acc_val[i])\n",
        "\n",
        "plot_history(loss_train, acc_train, expanded_val_loss, expanded_val_acc)"
      ],
      "metadata": {
        "id": "ZTiLP86iM-2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "9136ebcf-4681-4fb2-eaa4-7dd560150ba8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/home/best results.txt\", \"w+\")\n",
        "file.write(\"train_acc: \" + str(max(acc_train)) + \"    index: \" + str(np.argmax(acc_train)*300) + \"\\n\")\n",
        "file.write(\"val_acc: \" + str(max(acc_val)) + \"    index: \" + str(np.argmax(acc_val)*300) + \"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "YVNkAXvkNBbt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(7, 3))\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.plot(acc_train[0], label=\"train\")\n",
        "#plt.ylim()    \n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n"
      ],
      "metadata": {
        "id": "2nI48NzKNDjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5f96cb99-becf-4817-a484-60a87f11bde6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAADQCAYAAABr00SDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATd0lEQVR4nO3df5BddXnH8fdDkiZZN4ZNgFXBNrEq/kiHUBYaRdrdgoqolYIFRlCr0thqC9KijdUZ1GonitWWserEltZWyKoJVDtFRZiuVYFgEqNuBAk/FAKIiCZmIaEEnv5xD7qJ98bNZs/e3e99v2bu7L3ne865z3mSySfnx54TmYkkSaU4qN0FSJI0kQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlFmtruAsTjkkENy0aJF7S5jQj344IM84QlPaHcZU5K9ac3etGZvWiuxNxs2bPhxZh7abGxaBNuiRYtYv359u8uYUENDQ/T397e7jCnJ3rRmb1qzN62V2JuI+EGrMQ9FSpKKYrBJkopisEmSijItzrFJkvb0yCOPsHXrVnbt2vUr550/fz433XTTJFQ18ebMmcMRRxzBrFmzxryMwSZJ09DWrVuZN28eixYtIiL2Oe+OHTuYN2/eJFU2cTKTBx54gK1bt7J48eIxL+ehSEmahnbt2sXChQt/ZahNZxHBwoULx7RXOprBJknTVMmh9rjxbKPBJknab9u2beOjH/3ofi93yimnsG3bthoq+gWDTZK031oF2+7du/e53FVXXcXBBx9cV1mAF49IksZhxYoV3HbbbSxdupRZs2YxZ84cenp6uPnmm7nllls49dRTueuuu9i1axfnn38+y5cvB35xJ6mRkRFe8pKX8IIXvIDrrruOww8/nM997nPMnTv3gGsz2CRpmnv3f23mu/f8rOX4o48+yowZM/Zrnc95yhO56OXPbTm+cuVKhoeH2bRpE0NDQ7z0pS9leHj451cvXnrppSxYsICdO3dy7LHHcvrpp7Nw4cI91rFlyxZWr17NJz7xCc444wzWrl3LOeecs191NmOwSZIO2HHHHbfHJfmXXHIJV155JQB33XUXW7Zs+aVgW7x4MUuXLgXgmGOO4fvf//6E1GKwSdI0t689K5ic32Mb/fSAoaEhrrnmGq6//nq6urro7+9vesn+7Nmzf/5+xowZ7Ny5c0Jq8eIRSdJ+mzdvHjt27Gg6tn37dnp6eujq6uLmm2/mhhtumNTa3GOTJO23hQsXcvzxx7NkyRLmzp1Lb2/vz8dOPvlkPv7xj/PsZz+bI488kmXLlk1qbQabJGlcLr/88qbTZ8+ezRe+8IWmY4+fRzvkkEMYHh7++fQLL7xwwuqq9VBkRJwfEcMRsTki3lJNWxARX46ILdXPnjprkCR1ltqCLSKWAH8CHAccBbwsIp4OrACuzcxnANdWnyVJmhB17rE9G1iXmQ9l5m7gK8BpwCuAT1bzfBI4tcYaJEkdps5gGwZOiIiFEdEFnAI8FejNzHureX4I9LZagSSptcxsdwm1G882Rp2NiYg3AG8CHgQ2Aw8Df5yZB4+a56eZ+Uvn2SJiObAcoLe395jBwcHa6myHkZERuru7213GlGRvWrM3rXVab7q7u+nt7WX+/Pm/8g7447nzyFSQmWzfvp377ruPkZGRPcYGBgY2ZGZfs+VqDbY9viji74CtwPlAf2beGxFPBoYy88h9LdvX15fr16+fjDInzdDQEP39/e0uY0qyN63Zm9Y6rTf78wTtXbt2MWfOnEmoauK1eoJ2RLQMtlov94+IwzLzRxHx6zTOry0DFgOvBVZWPz9XZw2SVKJZs2aN+anSQ0NDHH300TVXNHXU/XtsayNiIfAI8ObM3BYRK4HPVIcpfwCcUXMNkqQOUmuwZeYJTaY9AJxY5/dKkjqX94qUJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBXFYJMkFcVgkyQVxWCTJBWl1mCLiAsiYnNEDEfE6oiYExEnRsTGiNgUEV+LiKfXWYMkqbPUFmwRcThwHtCXmUuAGcBZwMeAszNzKXA58M66apAkdZ66D0XOBOZGxEygC7gHSOCJ1fj8apokSRNiZl0rzsy7I+KDwJ3ATuDqzLw6Is4FroqIncDPgGV11SBJ6jyRmfWsOKIHWAucCWwDPgusAU4D3p+Z6yLircCRmXluk+WXA8sBent7jxkcHKylznYZGRmhu7u73WVMSfamNXvTmr1prcTeDAwMbMjMvmZjte2xAScBd2Tm/QARcQVwPHBUZq6r5vk08MVmC2fmKmAVQF9fX/b399dY6uQbGhqitG2aKPamNXvTmr1prdN6U+c5tjuBZRHRFREBnAh8F5gfEc+s5nkhcFONNUiSOkyd59jWRcQaYCOwG/gmjT2wrcDaiHgM+Cnw+rpqkCR1njoPRZKZFwEX7TX5yuolSdKE884jkqSiGGySpKIYbJKkohhskqSiGGySpKIYbJKkohhskqSiGGySpKIYbJKkohhskqSiGGySpKIYbJKkohhskqSijCnYIuL8iHhiNPxLRGyMiBfVXZwkSftrrHtsr8/MnwEvAnqAVwMra6tKkqRxGmuwRfXzFOA/MnPzqGmSJE0ZYw22DRFxNY1g+1JEzAMeq68sSZLGZ6xP0H4DsBS4PTMfiogFwOvqK0uSpPEZ6x7b84DvZea2iDgHeCewvb6yJEkan7EG28eAhyLiKOCvgNuAf6+tKkmSxmmswbY7MxN4BfCRzPwnYF59ZUmSND5jPce2IyLeTuMy/xMi4iBgVn1lSZI0PmPdYzsTeJjG77P9EDgCuLi2qiRJGqcxBVsVZpcB8yPiZcCuzPQcmyRpyhnrLbXOAG4E/gg4A1gXEa+sszBJksZjrOfY3gEcm5k/AoiIQ4FrgDV1FSZJ0niM9RzbQY+HWuWBsSwbERdExOaIGI6I1RExp7qR8vsi4paIuCkizhtX5ZIkNTHWPbYvRsSXgNXV5zOBq/a1QEQcDpwHPCczd0bEZ4CzaNxj8qnAszLzsYg4bHylS5L0y8YUbJn51og4HTi+mrQqM68c4/rnRsQjQBdwD/Be4FWZ+Vi17h/tY3lJkvZLNH7vuqaVR5wPvA/YCVydmWdHxAPAh4A/BO4HzsvMLU2WXQ4sB+jt7T1mcHCwtjrbYWRkhO7u7naXMSXZm9bsTWv2prUSezMwMLAhM/uaje1zjy0idgDNki+AzMwn7mPZHhp3KlkMbAM+W91ncjaNXxfoi4jTgEuBE/ZePjNXAasA+vr6sr+/f1+lTjtDQ0OUtk0Txd60Zm9aszetdVpv9hlsmXkgt806CbgjM+8HiIgrgOcDW4ErqnmuBP71AL5DkqQ9jPXikfG4E1gWEV00DkWeCKwHfgYMAHcAvwfcUmMNkqQOU1uwZea6iFgDbAR2A9+kcWhxLnBZRFwAjADn1lWDJKnz1LnHRmZeBFy01+SHgZfW+b2SpM411l/QliRpWjDYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRTHYJElFMdgkSUUx2CRJRak12CLigojYHBHDEbE6IuaMGrskIkbq/H5JUuepLdgi4nDgPKAvM5cAM4CzqrE+oKeu75Ykda66D0XOBOZGxEygC7gnImYAFwNvq/m7JUkdKDKzvpVHnA+8D9gJXJ2ZZ1fTDsrMD0fESGZ2t1h2ObAcoLe395jBwcHa6myHkZERurubbnrHszet2ZvW7E1rJfZmYGBgQ2b2NRurLdgiogdYC5wJbAM+C1xBI6z6M3P3voJttL6+vly/fn0tdbbL0NAQ/f397S5jSrI3rdmb1uxNayX2JiJaBtvMGr/3JOCOzLy/KuIK4N3AXODWiADoiohbM/PpNdYhSeogdZ5juxNYFhFd0UixE4EPZeaTMnNRZi4CHjLUJEkTqbZgy8x1wBpgI/Cd6rtW1fV9kiRBvYciycyLgIv2MV7W2UxJUtt55xFJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlEMNklSUQw2SVJRDDZJUlFqDbaIuCAiNkfEcESsjog5EXFZRHyvmnZpRMyqswZJUmepLdgi4nDgPKAvM5cAM4CzgMuAZwG/BcwFzq2rBklS55k5CeufGxGPAF3APZl59eODEXEjcETNNUiSOkhte2yZeTfwQeBO4F5g+16hNgt4NfDFumqQJHWeyMx6VhzRA6wFzgS2AZ8F1mTmp6rxTwAPZuZbWiy/HFgO0Nvbe8zg4GAtdbbLyMgI3d3d7S5jSrI3rdmb1uxNayX2ZmBgYENm9jUbq/NQ5EnAHZl5P0BEXAE8H/hURFwEHAq8sdXCmbkKWAXQ19eX/f39NZY6+YaGhihtmyaKvWnN3rRmb1rrtN7UGWx3AssiogvYCZwIrI+Ic4EXAydm5mM1fr8kqQPVFmyZuS4i1gAbgd3AN2nsgT0I/AC4PiIArsjM99RVhySps9R2jm0iRcT9NMKwJIcAP253EVOUvWnN3rRmb1orsTe/kZmHNhuYFsFWoohY3+rEZ6ezN63Zm9bsTWud1htvqSVJKorBJkkqisHWPqvaXcAUZm9aszet2ZvWOqo3nmOTJBXFPTZJUlEMthpFxIKI+HJEbKl+9rSY77XVPFsi4rVNxj8fEcP1Vzx5DqQ3EdEVEf8dETdXj0VaObnV1yMiTq4e6XRrRKxoMj47Ij5dja+LiEWjxt5eTf9eRLx4MuueDOPtTUS8MCI2RMR3qp+/P9m11+1A/t5U478eESMRceFk1Vy7zPRV0wv4ALCier8CeH+TeRYAt1c/e6r3PaPGTwMuB4bbvT1TpTc0nhQxUM3za8BXgZe0e5sOsB8zgNuAp1Xb9C3gOXvN8ybg49X7s4BPV++fU80/G1hcrWdGu7dpivTmaOAp1fslwN3t3p6p0ptR42to3Mv3wnZvz0S93GOr1yuAT1bvPwmc2mSeFwNfzsyfZOZPgS8DJwNERDfwl8B7J6HWyTbu3mTmQ5n5PwCZ+X807m4z3R9/dBxwa2beXm3TII0ejTa6Z2uAE6Nx+55XAIOZ+XBm3gHcWq2vFOPuTWZ+MzPvqaZvpvEYrdmTUvXkOJC/N0TEqcAdNHpTDIOtXr2ZeW/1/odAb5N5DgfuGvV5azUN4G+Bvwceqq3C9jnQ3gAQEQcDLweuraPISfQrt3X0PJm5G9gOLBzjstPZgfRmtNOBjZn5cE11tsO4e1P9x/mvgXdPQp2Tqu4HjRYvIq4BntRk6B2jP2RmRsSYL0GNiKXAb2bmBXsfE58u6urNqPXPBFYDl2Tm7eOrUp0gIp4LvB94UbtrmULeBXw4M0eqHbhiGGwHKDNPajUWEfdFxJMz896IeDLwoyaz3Q30j/p8BDAEPA/oi4jv0/hzOiwihjKzn2mixt48bhWwJTP/YQLKbbe7gaeO+nxENa3ZPFurUJ8PPDDGZaezA+kNEXEEcCXwmsy8rf5yJ9WB9OZ3gFdGxAeAg4HHImJXZn6k/rJr1u6TfCW/gIvZ8wKJDzSZZwGNY9w91esOYMFe8yyivItHDqg3NM47rgUOave2TFA/ZtK4OGYxv7gI4Ll7zfNm9rwI4DPV++ey58Ujt1PWxSMH0puDq/lPa/d2TLXe7DXPuyjo4pG2F1Dyi8Yx/muBLcA1o/5R7gP+edR8r6dxwv9W4HVN1lNisI27NzT+V5rATcCm6nVuu7dpAnpyCnALjavc3lFNew/wB9X7OTSuXrsVuBF42qhl31Et9z2m+RWiE9kb4J00HpW1adTrsHZvz1TozV7rKCrYvPOIJKkoXhUpSSqKwSZJKorBJkkqisEmSSqKwSZJKorBJtUoIq6rfi6KiFdN8Lr/ptl3SZ3Oy/2lSRAR/TR+T+hl+7HMzGzc26/V+Ehmdk9EfVJJ3GOTahQRI9XblcAJEbEpIi6IiBkRcXFEfCMivh0Rb6zm74+Ir0bE54HvVtP+s3qW2OaIWF5NW0njTvWbIuKy0d8VDRdHxHD1HLIzR617KCLWVM+yu+zxu7xLJfFekdLkWMGoPbYqoLZn5rHVY1S+HhFXV/P+NrAkG4+gAXh9Zv4kIuYC34iItZm5IiL+PDOXNvmu04ClwFHAIdUy/1uNHU3jFlz3AF8Hjge+NvGbK7WPe2xSe7wIeE1EbALW0bjF2DOqsRtHhRrAeRHxLeAGGjezfQb79gJgdWY+mpn3AV8Bjh217q2Z+RiN20stmpCtkaYQ99ik9gjgLzLzS3tMbJyLe3CvzycBz8vMhyJiiMa9/8Zr9LPIHsV/A1Qg99ikybEDmDfq85eAP4uIWQAR8cyIeEKT5eYDP61C7VnAslFjjzy+/F6+CpxZncc7FPhdGje/lTqC/1uTJse3gUerQ4r/BvwjjcOAG6sLOO4HTm2y3BeBP42Im2jcuf+GUWOrgG9HxMbMPHvU9CtpPM/vWzSegvC2zPxhFYxS8bzcX5JUFA9FSpKKYrBJkopisEmSimKwSZKKYrBJkopisEmSimKwSZKKYrBJkory/9zU2uTjY4lqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:pass"
      ],
      "metadata": {
        "id": "bhg7QacOP15h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}