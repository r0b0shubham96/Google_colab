{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mvsnet_Run7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP4OSPcjDzjreqpGlN5/ehX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0b0shubham96/Google_colab/blob/main/Mvsnet_Run7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript(''' \n",
        "function Clickconnect() {\n",
        "  btn = document.querySelector(\"colab-connect-button\")\n",
        "  if (btn != null){\n",
        "    console.log(\"Click colab-connect-button\");\n",
        "    btn.click()\n",
        "  }\n",
        "\n",
        "  btn = document.getElementbyID('ok')\n",
        "  if (btn != null){\n",
        "    console.log(\"Click reconnect\");\n",
        "    btn.click()\n",
        "  }\n",
        "}\n",
        "setInterval(Clickconnect,50000)\n",
        "'''))"
      ],
      "metadata": {
        "id": "LkXJ6D3CXGYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9BcI3Ko0R0i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/Dataset/dtu_training.rar\" \"/media/\""
      ],
      "metadata": {
        "id": "cNr3i7Z2R2WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/home/mvsnet/')"
      ],
      "metadata": {
        "id": "klOvC602SySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhI6_lLMMXoD"
      },
      "outputs": [],
      "source": [
        "import preprocess\n",
        "import visualize\n",
        "import torch\n",
        "import model\n",
        "import loss as l\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import constants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper parameters \n",
        "BATCH_SIZE = 5\n",
        "WEIGHT_DECAY = 1e-3\n",
        "LEARNING_RATE = 1e-2\n",
        "TRANSFORMS = transforms.Compose([preprocess.SampleWiseStandardization(), preprocess.ToTensor()])      #dataset_transforms = transforms.Compose([preprocess.SampleWiseStandardization(), preprocess.ToTensor()])\n",
        "#Dataloader\n",
        "NUM_WORKERS = 3\n",
        "\n",
        "#Training\n",
        "MAX_EPOCHS = 6\n"
      ],
      "metadata": {
        "id": "hgjnvkBPMd-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list_train = preprocess.gen_dtu_path(\"/media/mvs_training/dtu/\")  #Change path to your own directory of the DTU dataset\n",
        "sample_list_val = preprocess.gen_dtu_path(\"/media/mvs_training/dtu/\", mode=\"validation\")\n",
        "\n",
        "#dataset_transforms = transforms.Compose([preprocess.SampleWiseStandardization(), preprocess.ToTensor()])\n",
        "data_train = preprocess.DTUDatset(sample_list_train, transform=TRANSFORMS)\n",
        "data_val = preprocess.DTUDatset(sample_list_val, transform=TRANSFORMS)"
      ],
      "metadata": {
        "id": "vTJTJd6IMh_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_train = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "loader_val = torch.utils.data.DataLoader(data_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "_c_nnGLKMkS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MVSModel = model.NeuralNet()\n",
        "MVSModel = MVSModel.float()\n",
        "\n",
        "\n",
        "def one_epoch(model, data_loader, optimizer=None, best_val_acc = 0):\n",
        "    \"\"\"\n",
        "    Outputs the mean loss and acc for every 1000 iterations / ref images.\n",
        "    The returned output is a vector of length 27,\n",
        "    that is 27 mean loss and acc values. \n",
        "    The last index of the list contains a mean over 1097 iterations due to the \n",
        "    total lenght of the data in the dataloader is 27097 \n",
        "    \"\"\"\n",
        "    device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    train = False if optimizer is None else True\n",
        "    model.train() if train else model.eval()    #When model is set to train it knows to enclude dropout, batchnorm, etc in the training\n",
        "    mean_losses, mean_acc, accuracies, losses, acc, correct, total = [], [], 0, [], 0, 0, 0\n",
        "    log_iter = 100 * BATCH_SIZE #Number of iterations before we log loss and acc\n",
        "    max_iter = 27097 \n",
        "    iteration = 0\n",
        "    depth_maps = []\n",
        "    for ref, s1, s2, gt_depth, r_cam, s1_cam, s2_cam in data_loader:\n",
        "        start_time = time.time()\n",
        "        iteration += len(ref)\n",
        "        ref, s1, s2, gt_depth = ref.to(device), s1.to(device), s2.to(device), gt_depth.to(device)\n",
        "        with torch.set_grad_enabled(train): #Enable gradient calculation if train = True\n",
        "            logits = model(ref.float(), s1.float(), s2.float(), r_cam, s1_cam, s2_cam)\n",
        "        #ATM the loss can not be computed since the model is not finished, therefore the loop stops when trying to implement the backward pass\n",
        "        loss, acc = l.mean_abs_diff_loss(logits, gt_depth)\n",
        "        if train: \n",
        "            optimizer.zero_grad()   #Removing accumulated gradients \n",
        "            loss.backward()         #Calculating new gradients\n",
        "            optimizer.step()        #Updating weights\n",
        "        losses.append(loss.item())\n",
        "        accuracies += acc\n",
        "        total += len(ref)   \n",
        "        if iteration % log_iter == 0 and iteration < 27000:\n",
        "            mean_losses.append(np.mean(losses))\n",
        "            mean_acc.append(accuracies/log_iter)\n",
        "            if not train:\n",
        "                #Saving est_depth_map, gt_depth_map and ref image in the validation loop\n",
        "                est_depth_map = logits\n",
        "                est_depth_map = est_depth_map.detach().cpu().numpy()\n",
        "                gt = gt_depth\n",
        "                gt = gt.detach().cpu().numpy()\n",
        "                ref_img = ref\n",
        "                ref_img = ref_img.detach().cpu().numpy()\n",
        "                est_and_gt = [est_depth_map, gt, ref_img]\n",
        "                depth_maps.append(est_and_gt)\n",
        "                #Checking if current acc is better than current best acc\n",
        "                if acc > best_val_acc:\n",
        "                    best_val_acc = acc\n",
        "                    print(\"New best validation acc: \", best_val_acc)\n",
        "                    torch.save(model.state_dict(), \"best_model_param.pt\") #saving best model parameters\n",
        "            losses = []\n",
        "            accuracies = 0\n",
        "        elif iteration == max_iter:   #End of the epoch\n",
        "            print(\"THIS IS THE END OF THE EPOCH\")\n",
        "            mean_losses.append(np.mean(losses))\n",
        "            mean_acc.append(accuracies/log_iter)\n",
        "            if not train:\n",
        "                if acc > best_val_acc:\n",
        "                    best_val_acc = acc\n",
        "                    print(\"New best validation acc: \", best_val_acc)\n",
        "                    torch.save(model.state_dict(), \"best_model_param.pt\") #saving best model parameters\n",
        "        print(\"Iteration: \", iteration, \"     Current loss: \", loss.item(), \"     Current accuracy: \", np.mean(accuracies),  \"     Comp_time of one batch: \", start_time - time.time())\n",
        " \n",
        "    if train:\n",
        "        return mean_losses, mean_acc\n",
        "    else:\n",
        "        return mean_losses, mean_acc, best_val_acc, depth_maps\n",
        "\n",
        "\n",
        "\n",
        "def train(model, loader_train, loader_val, lr=1e-3, max_epochs = 6, weight_decay=1e-2, patience=10):\n",
        "    train_losses, train_accuracies = [], []\n",
        "    val_losses, val_accuracies = [], []\n",
        "    depth_maps_val = []\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_val_epochs = []\n",
        "    best_val_acc = 0\n",
        "    t = tqdm(range(max_epochs)) #Using tqdm to show progress of epochs\n",
        "    for epoch in t:\n",
        "        train_loss, train_acc = one_epoch(model, loader_train, opt) #One epoch returns 27 mean values. \n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc, best_val_acc, depth_map_val = one_epoch(model, loader_val, best_val_acc=best_val_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        best_val_epochs.append(best_val_acc)\n",
        "        depth_maps_val.append(depth_map_val)\n",
        "        \n",
        "\n",
        "    train_losses_flat = [item for sublist in train_losses for item in sublist]\n",
        "    val_losses_flat = [item for sublist in val_losses for item in sublist]\n",
        "    train_accuracies = [item for sublist in train_accuracies for item in sublist]\n",
        "    val_accuracies = [item for sublist in val_accuracies for item in sublist]\n",
        "\n",
        "        #t.set_description(f'train_acc: {train_acc:.2f}, valid_acc: {val_acc:.2f}, best_val_acc: {best_val_acc:.2f}')\n",
        "    return train_losses_flat, train_accuracies, val_losses_flat, val_accuracies, depth_maps_val\n",
        "\n",
        "\n",
        "def plot_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
        "    fig = plt.figure(figsize=(7, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    plt.plot(val_losses, label=\"valid\")\n",
        "    #plt.ylim()    \n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.plot(train_accuracies, label=\"train\")\n",
        "    plt.plot(val_accuracies, label=\"valid\")\n",
        "    #plt.ylim()\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"models/32D_12E/train_results.png\")\n"
      ],
      "metadata": {
        "id": "VsLjsg-kvhsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train, acc_train, loss_val, acc_val, depth_maps = train(MVSModel, loader_train, loader_val, lr=LEARNING_RATE, max_epochs=MAX_EPOCHS, weight_decay=WEIGHT_DECAY)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_P4j8arMsMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for e in range(len(depth_maps)):\n",
        "    os.mkdir(\"models/32D_12E/img/epoch%1d\" % e)\n",
        "    for i in range(len(depth_maps[0])):\n",
        "        for j in range(len(depth_maps[0][0])):\n",
        "            est_map = depth_maps[0][i][0][j]\n",
        "            gt_map = depth_maps[0][i][1][j]\n",
        "            ref = depth_maps[0][i][2][j]\n",
        "            ref = np.swapaxes(ref, 0, 1)\n",
        "            ref = np.swapaxes(ref,1, 2)\n",
        "            #ref = np.uint8(ref)\n",
        "            visualize.show_depth_img(est_map, \"epoch%1d/\" % e + \"est%1d\" % i)\n",
        "            visualize.show_depth_img(gt_map,\"epoch%1d/\" % e + \"gt%1d\" % i)\n",
        "            #plt.imshow(ref)\n",
        "            plt.savefig(\"models/32D_12E/img/epoch%1d/\" % e + \"ref%1d\" % i)\n",
        "            #plt.show(block=True)\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "Q0lRmkHCMwK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(acc_train))"
      ],
      "metadata": {
        "id": "XvYSekAKvy9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#come up with something better at some point\n",
        "expanded_val_loss = []\n",
        "expanded_val_acc = []\n",
        "for i in range(len(loss_val)):\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_loss.append(loss_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    expanded_val_acc.append(acc_val[i])\n",
        "    if i % 2 == 0:\n",
        "        expanded_val_loss.append(loss_val[i])\n",
        "        expanded_val_acc.append(acc_val[i])\n",
        "\n",
        "plot_history(loss_train, acc_train, expanded_val_loss, expanded_val_acc)"
      ],
      "metadata": {
        "id": "ZTiLP86iM-2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/home/best results.txt\", \"w+\")\n",
        "file.write(\"train_acc: \" + str(max(acc_train)) + \"    index: \" + str(np.argmax(acc_train)*300) + \"\\n\")\n",
        "file.write(\"val_acc: \" + str(max(acc_val)) + \"    index: \" + str(np.argmax(acc_val)*300) + \"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "YVNkAXvkNBbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(7, 3))\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.plot(acc_train[0], label=\"train\")\n",
        "#plt.ylim()    \n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n"
      ],
      "metadata": {
        "id": "2nI48NzKNDjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}